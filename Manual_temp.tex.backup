\documentclass[11pt]{article}
\usepackage{eurosym}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{setspace}
\usepackage[hmargin={.8in,.8in},vmargin={.9in,.7in}]{geometry}
\usepackage{graphicx}
\usepackage[labelsep=space,labelfont=bf,skip=1pt,sf,justification=centering]{caption}
\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage{rotating}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{moresize}
\usepackage{longtable}
\usepackage[comma]{natbib} 
\usepackage{bm}
\usepackage{framed}
\usepackage[section]{placeins}
\usepackage{chngcntr}
\usepackage[colorlinks=true,linkcolor=note,citecolor=note,urlcolor=note]{hyperref}
\usepackage{relsize}
\usepackage{soul}
\usepackage{tikz}
\usepackage[scaled]{helvet}
\usepackage[bordercolor=white,backgroundcolor=gray!30,linecolor=black]{todonotes}
\usepackage{sectsty}
\usepackage{courier}
\usepackage{fancyvrb}
\usepackage{newverbs}
\usepackage{mathpazo}
\usepackage[most]{tcolorbox}
\tcbuselibrary{listings,skins}
%\usepackage[sfdefault]{cabin}
\usepackage[scaled]{helvet}
\renewcommand\familydefault{\sfdefault} 
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=0pt}

% Colors
%-----------------------------------------------------
\definecolor{shadecolor}{rgb}{0.95,0.95,0.95}
\definecolor{title}{RGB}{0,0,90}
\definecolor{math}{RGB}{0,0,150}
\definecolor{note}{RGB}{49,79,179}
\definecolor{light}{RGB}{70,130,180}
\definecolor{red}{RGB}{200,0,0}
\definecolor{gold}{RGB}{218,165,32}
\definecolor{green}{RGB}{0,179,0}
\definecolor{purple}{RGB}{150,40,160}
\definecolor{brick}{RGB}{203, 92, 65}
\definecolor{maroon}{RGB}{128,0,0}
\definecolor{blue}{RGB}{0,0,115}
% Define your existing Matlab colors
\definecolor{matlabgreen}{RGB}{0,153,0}
\definecolor{matlabpurple}{RGB}{127,0,255}
\definecolor{matlabblue}{RGB}{0,42,252}
\definecolor{script}{RGB}{255,250,235}
\definecolor{matlabbg}{RGB}{252,252,245}  % Light background

% Comments
%-----------------------------------------------------
\definecolor{brick}{RGB}{203, 92, 65}
\definecolor{tomato}{RGB}{217, 83, 79}
\newcommand{\ACB}[1]{\noindent \textcolor{tomato}{{\hand \small \underline{ACB}: #1}}} %show 
%\newcommand{\ACB}[1]{} %hide comments

% Table set up
%-----------------------------------------------------
\makeatletter
\newcommand*\notesize{\@setfontsize\notesize{8.5}{9.0}}
\makeatother
\renewcommand{\arraystretch}{1.15}
\newcolumntype{Y}{>{\centering\arraybackslash\leavevmode}X}

% Appendix set up
%-----------------------------------------------------
\newcommand*\backmatter{\setcounter{section}{0}\renewcommand\theHsection{back.\Roman{section}}}

% Define the Matlab language style
%-----------------------------------------------------
\lstdefinestyle{matlab}{
    language=Matlab,
    basicstyle=\small\ttfamily,           % Monospaced font
    keywordstyle=\color{matlabblue},      % Keywords in blue
    commentstyle=\color{matlabgreen},     % Comments in green
    stringstyle=\color{matlabpurple},     % Strings in purple
    numberstyle=\tiny\color{gray},
    numbers=none,                          % No line numbers (set to 'left' if wanted)
    breaklines=true,                       % Automatic line breaking
    breakatwhitespace=true,
    showstringspaces=false,               % Don't show spaces in strings
    tabsize=4,
    keepspaces=true,
    columns=flexible,
    frame=none,
    backgroundcolor=\color{matlabbg},
    xleftmargin=3pt,
    xrightmargin=3pt,
    aboveskip=0pt,
    belowskip=0pt,
}
% Set Matlab as default
\lstset{style=matlab}
% OPTION 2: tcolorbox with listings (colored box)
\newtcblisting{matlabcode}{
    listing only,
    listing options={style=matlab},
    colback=script,              % Your existing script color
    colframe=black!50,
    boxrule=0.6pt,
    arc=2pt,
    left=3pt,
    right=3pt,
    top=3pt,
    bottom=3pt,
    breakable,                   % Allow page breaks
}

% Environment for MATLAB output
\newenvironment{matlaboutput}
{\singlespacing\small\Verbatim}
{\endVerbatim\setstretch{1.25}\normalsize}

% Environment for note boxes with normal paragraph indentation
\newtcolorbox{notebox}{
    breakable,
    colback=note!10,
    colframe=note!10,
    parskip=0.9cm,
    before upper={\vspace{-.2cm}\onehalfspacing{\hand \textbf{------------------------------------------------ NOTE ---------------------------------------------}}\par\medskip},
    after upper={\vspace{-.25cm}\begin{center}{\hand \textbf{------------------------------------------------------------------------------------------------------}}\end{center}},
}

% Tikz
%-----------------------------------------------------
\usetikzlibrary{calc,decorations,patterns,arrows,decorations.pathmorphing}
\makeatletter
\tikzset{nstyle/.style={inner sep=0pt,anchor=base}}
\tikzset{tstyle/.style={remember picture,baseline}}
\tikzset{tpstyle/.style={overlay, remember picture}}
\makeatother
%\pgfmathsetseed{1}

% Hand writing font
%-----------------------------------------------------
\DeclareRobustCommand{\hand}{%
\fontfamily{augie}\fontseries{b}\fontshape{n}\selectfont}
\DeclareTextFontCommand{\textaugie}{\hand}

% Theorems
%-----------------------------------------------------
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.}}{\ \rule{0.5em}{0.5em}}

% Star
%-----------------------------------------------------
\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ifcase#1 \or \FiveStarOpen \or
$\dagger$\or $\ddagger$\or $\mathSection$\or \else \@ctrerr
\fi}\setlength{\@fptop}{0pt}
\makeatother

\graphicspath{{./v3dot1/Primer/graphics/}}

% New
%-----------------------------------------------------
\definecolor{cmd}{gray}{0.99}
\definecolor{script}{RGB}{255,248,225}
\definecolor{subsection}{RGB}{0,0,50}
\definecolor{section}{RGB}{0,0,50}
\definecolor{blue}{RGB}{0,0,100}
\definecolor{shadecolor}{rgb}{0.95,0.95,0.95}
\definecolor{title}{RGB}{0,0,90}
\definecolor{note}{RGB}{49,79,179}
\definecolor{light}{RGB}{70,130,180}
\definecolor{red}{RGB}{200,0,0}
\definecolor{gold}{RGB}{218,165,32}
\definecolor{green}{RGB}{0,179,0}
\definecolor{purple}{RGB}{150,40,160}
\definecolor{maroon}{RGB}{128,0,0}
\definecolor{red}{RGB}{140,0,0}
\definecolor{blue}{RGB}{0,0,255}
\definecolor{maroon}{RGB}{128,0,0}
\definecolor{subsection}{RGB}{0,0,90}
\definecolor{section}{RGB}{0,0,90}
\definecolor{matlabgreen}{RGB}{0,153,0}
\definecolor{matlabpurple}{RGB}{127,0,255}
\definecolor{matlabblue}{RGB}{0,42,252}
\sectionfont{\color{section}}
\subsectionfont{\color{subsection}}
\newcommand{\note}[1]{\todo[color=gray!10,inline,caption={}]{#1}}

% Paragraphs
%-----------------------------------------------------
\usepackage[indent]{parskip}
\setstretch{1.25}
\setlength{\parskip}{2cm}
%\setlength{\parindent}{15pt}
\setcounter{secnumdepth}{3}

\begin{document}


\title{\textbf{\color{section} The VAR Toolbox Handbook}\thanks{{The views expressed in this paper are those of the author and do not necessarily represent the views of the Bank of England.\medskip}}}

\author{Ambrogio Cesa-Bianchi\thanks{Bank of England, CEPR, and CFM. \newline Email: \href{ambrogio.cesa-bianchi@bankofengland.co.uk}{\texttt{ambrogio.cesa-bianchi@bankofengland.co.uk}}.\newline Web: \href{https://sites.google.com/site/ambropo}{\texttt{https://sites.google.com/site/ambropo}}}}
\maketitle

\abstract{\noindent This paper describes a collection of Matlab routines to perform VAR analysis. The VAR toolbox allows to estimate reduced-form VARs and to identify structural shocks under a number of different identification techniques. Impulse responses, forecast error variance decompositions, and historical decompositions are 	computed according to the chosen identification option. The toolbox includes practical examples and replications of well-known studies in the VAR literature.}

\newpage

\tableofcontents

\newpage

\normalsize

\section{VAR Toolbox:\ High level description}

The VAR Toolbox is a collection of Matlab routines to perform Vector autoregressive (VAR) analysis. Initially developed by \cite{Sims1980} over four decades ago, as of today VAR models are still one of the most important tools in a macroeconometrician's toolkit. As \cite{StockWatson2000} famously put it, VARs provide a coherent and credible approach for macroeconometricians to do their job, which consists of four things: (i) describing the dynamic relations in macroeconomic and financial data, (ii) making forecasts, (iii) making inference about the true (and unobserved) structure of the macroeconomy, and (iv) advising policymakers. 

The VAR Toolbox provides an intelligible and accessible way into the workings of VARs step by step. In this regard, it differs from similar (and more efficient) toolboxes, such as the BEAR toolbox of \cite{DieppeVanRoyeLegrand2016} and the hitchhiker's guide to empirical macro models of \cite{CanovaFerroni2020}.\footnote{Other toolboxes covering similar topics are the Econometric toolbox of James LaSage, the Dynare project of Adjemian, Bastani, Juillard, Karam e, Maih, Mihoubi, Perendia, Pfeifer, Ratto and Villemot (2011), the Global VAR toolbox of Vanessa Smith, the mixed frequency VAR toolbox of Brave, Butters and Kelley (2020), the Bayesian Local Projection of Miranda-Agrippino and Ricco.} Estimation is performed with Ordinary Least Squares (OLS) and confidence intervals are obtained with bootstrapping methods. The toolbox includes the most commonly used identification options, and allows the computation of impulse responses, forecast error variance decompositions, and historical decompositions. 

The objective of this paper is to explain the functioning of the VAR Toolbox and the workings of VARs by means of simple examples -- the idea being that it is much easier to learn by doing rather than reading a technical manual first, and then go to the computer. As a result, many of the functions included in the VAR Toolbox are not covered here, nor is a full description of the output of each function. Moreover, to simplify the intuition and exposition, most of the material covered here is treated in a pretty informal way. In other words, the VAR Toolbox handbook should not be thought of as a substitute for more formal time series econometrics textbooks, but rather as a complement to them.

\textit{Acknowledgements}: the origin of the VAR Toolbox traces back to the early days of my PhD, when I was trying to understand the mechanics of VARs by replicating existing papers in the literature. In my personal experience, replication is key: it is only when coding up something that you really get the grips with it. %When I went to the job market, I thought that uploading the codes I had put together would make my profile stronger and help me find a job. The first version of the VAR Toolbox -- not too dissimilar from a ZIP file with a bunch of replication codes -- was born that way. 
The VAR Toolbox would not exist if it were not for James LeSage's \href{https://www.spatial-econometrics.com/}{Econometrics Toolbox}, which helped greatly with my understanding of VARs and econometrics more in general. The main function for the estimation of reduced form VARs in the VAR Toolbox is still a slightly modified version of LeSage's original function. Relative to Le Sage's Toolbox, the VAR Toolbox is much narrower in scope, its main focus being the estimation and structural identification of VAR models. The VAR Toolbox has then developed over the years, with the help of many users, colleagues, and co-authors who made useful suggestions and spotted typos or bugs with the codes. I'm grateful to all of you.

\textit{Disclaimer}:  All files available in the VAR toolbox are for education and/or research  purposes only. I take no responsibility and/or liability for how you choose to use any of the source code available here. While the codes in the VAR Toolbox have been tested extensively, and despite every effort has been made to ensure that they are error free, some of them may still have bugs or errors. If you find any, please email me at \ \url{ambrogio.cesabianchi@gmail.com} \ or open an issue in Github \url{https://github.com/ambropo/VAR-Toolbox}. Whenever the software is used, I would appreciate acknowledgment by citation of this working paper. 

\paragraph{Getting started}

No installation is required. Simply fork/download the latest version of the toolbox from \  \url{https://github.com/ambropo/VAR-Toolbox} \ to a specific folder on the hard drive, e.g. \path{/VARToolbox/v3dot1}. The only required step to get the VAR Toolbox working is to add the folder \path{v3dot1} (including all subfolders) to the Matlab path. To avoid clashes with other functions it is recommendable to add and remove the Toolbox with the following commands at both the beginning and the end of your scripts, e.g.:\vspace{.25cm}

\begin{matlabcode}
addpath(genpath('/VARToolbox/v3dot1'))

...

rmpath(genpath('/VARToolbox/v3dot1'))
\end{matlabcode}

\vspace{.15cm}\noindent The codes are grouped in five categories (and respective subfolders within \path{v3dot1}). Throughout this paper, all folder references are relative to this root folder:

\begin{itemize}
	\item \path{VAR}:\ codes for VAR analysis, e.g. estimation, identification, computation of the impulse response functions, forecast error variance decompositions, historical decompositions, etc.
	\item \path{Stats}:\ codes for the calculation of commonly used descriptive statistics, e.g. moving-window averages or sums, pairwise correlations, etc.
	\item \path{Utils}:\ codes that allow the smooth functioning of the Toolbox, e.g. functions to vectorize matrices or compute the number of rows of a matrix.
	\item \path{Auxiliary}:\ codes that I borrowed from other public sources.
	Each m-file has a reference to the original source.
	\item \path{Figure}:\ codes for plotting high quality figures, particularly thought for time series data, e.g. functions to control dates on the horizontal axis, appearance of the legends, plot charts with shaded error bands, etc.
\end{itemize}

\noindent The next sections describe how to load the data, estimate a VAR, identify the shocks of interest, and analyze how these shocks transmit through the system, as well as how important they are in driving variation in the endogenous variables. In the spirit of making the exposition more practical than the usual textbook, all of the material presented in this paper closely follows the code \ \path{VARToolbox_Primer.m}, which can be found in the folder \ \path{Primer/}.

The VAR Toolbox also includes replications of a few well-known VAR studies. These replications, which are discussed in the Appendix, are stored in the folder \ \path{Replic/}. 

At the time of writing, the latest version of the toolbox is 3.1. The VAR Toolbox 3.1 has been tested with \ACB{Matlab R2021B} on a Macbook Pro machine.

\section{A Reduced-form VAR Model}

This section introduces the reduced-form VAR model, which serves as the foundation for all subsequent analysis. We begin by describing how to load and prepare the data, then move to the estimation of the VAR coefficients and the variance-covariance matrix of the residuals. The reduced-form VAR is the starting point for any structural analysis: it summarizes the dynamic relationships among the endogenous variables without imposing any economic restrictions on the system.

\subsection{Prelims:\ Loading and Preparing the Data}\label{sec:basics}

The data for the examples in this paper is stored in the spreadsheet \path{Simple_Data.xlsx}, located in \path{Primer/data/}. The Data Appendix reports the source of each series used.

The file \path{Simple_Data.xlsx} contains US macro and financial data at quarterly frequency, namely the CPI Index, real GDP, the unemployment rate, the VIX Index, and the yield on the 1-year Treasury Bill. The sample period is 1989:Q1 to 2019:Q4, so that the number of observations for each time series is $T=124$. 

The code below shows a general way of loading the data and managing it in a way that is consistent with the functioning of the VAR Toolbox. Specifically, the code reads from the spreadsheet \ \path{Simple_Data.xlsx} \ and stores each time series into the structure \colorbox{script!80}{\small \texttt{DATA}} as a separate variable. The convention of the VAR Toolbox is that time series are stored in column-vectors, so that the number of rows corresponds to the numbers of observations (denoted by $T$) for a given time series.

The VAR Toolbox can handle dates in two formats:
\begin{itemize}
	\item \textit{String format}: Quarters (months) are denoted with $Q$ ($M$). For example, \colorbox{script!80}{\small \texttt{\color{matlabpurple}'2000Q1'}} (\colorbox{script!80}{\small \texttt{\color{matlabpurple}'2000M1'}}) corresponds to the first quarter (month) of the year 2000.
	\item \textit{Numeric format}: Following the convention in \cite{CanovaFerroni2020}, the first quarter (month) of the year corresponds to the integer of that year. For example, 2000Q1 (or 2000M1 for monthly data) corresponds to \colorbox{script!80}{\small \texttt{2000.00}}.
\end{itemize}
\noindent In the code below, dates are read in string format from the spreadsheet and stored in the cell array \colorbox{script!80}{\small\texttt{dates}}. The function \colorbox{script!80}{\small\texttt{Date2Num}} converts dates from string to numeric format, while \colorbox{script!80}{\small\texttt{Date2Cell}} performs the reverse conversion.\vspace{.45cm}

\begin{matlabcode}
% Load data from US macro data set
[xlsdata, xlstext] = xlsread('data/Simple_Data.xlsx','Sheet1');
dates = xlstext(3:end,1);        % vector of dates in string format
datesnum = Date2Num(dates);      % vector of dates in numeric format
vnames_long = xlstext(1,2:end);  % full variable names
vnames = xlstext(2,2:end);       % variable mnemonic
nvar = length(vnames);           % number of variables in spreadsheet
data = Num2NaN(xlsdata);         % matrix of data in spreadsheet
% Store variables in the structure DATA
for ii=1:length(vnames)
    DATA.(vnames{ii}) = data(:,ii);
end
% Observations
nobs = size(data,1);
\end{matlabcode}

As is often the case in empirical applications, the raw data needs to be transformed before it can be used in VAR analysis. The VAR Toolbox has some built-in functions to perform the most commonly used data treatments. The example below shows how to compute the growth rate of real GDP and the CPI Index, and the first difference of the 1-year Treasury Bill yield. The new variables are then added to the structure \colorbox{script!80}{\small\texttt{DATA}}.\vspace{.45cm}

\begin{matlabcode}
% Select variables to treat
tempnames = {'gdp','cpi','i1yr'};         % variable mnemonics
temptreat = {'logdiff','logdiff','diff'}; % type of transformation
tempscale = [100 100 1];                  % rescaling (if needed)
% Treat and add to DATA structure
for ii=1:length(tempnames)
    aux = {['d' tempnames{ii}]};
    DATA.(aux{1}) = tempscale(ii)*...
        XoX(DATA.(tempnames{ii}),1,temptreat{ii});
end
delete temp*
\end{matlabcode}

The example used throughout the paper is based on an over-simplistic VAR where the only $k=2$ endogenous variables are the quarterly growth rate of US GDP (as computed above and denoted by $y_{t}$) and the 1-year yield on the US Treasury bill (denoted by $r_{t}$). While such a simple VAR\ cannot realistically describe the complex interactions of the US\ economy, it is a useful device to understand the functioning of the VAR Toolbox and, importantly, the mechanics of VAR models more generally.\footnote{The VAR Toolbox also includes more realistic examples based on replications of existing papers (see the appendix). The replication codes can be found in the folder  \path{Replic/}.} 

The VAR Toolbox follows the convention that each endogenous variable is stored in a column vector of a $T \times k$ matrix \colorbox{script!80}{\small \texttt{X}}, as follows:
\begin{equation}\label{eq:convention}
	\text{\colorbox{script!80}{\small\texttt{X}}}=\left[ 
	\begin{array}{cc}
		y_{1} & r_{1} \\ 
		y_{2} & r_{2} \\ 
		... & ... \\ 
		y_{T} & r_{T}%
	\end{array}%
	\right]
\end{equation}
so that the matrix \colorbox{script!80}{\small \texttt{X}} has $T=124$ rows (i.e. the number of observations) and $k=2$ columns (i.e. the number of variables). The code below shows a general way to construct such \colorbox{script!80}{\small \texttt{X}} matrix in Matlab.\vspace{.45cm}

\begin{matlabcode}
% Select the list of endogenous variables...
Xvnames = {'dgdp','i1yr'};
% ... and corresponding labels to be used in plots
Xvnames_long = {'Real GDP Growth','1-year Int. Rate'};
% Number of endo variables
Xnvar = length(Xvnames);
% Create matrix X of variables to be used in the VAR
X = nan(nobs,Xnvar);
for ii=1:Xnvar
    X(:,ii) = DATA.(Xvnames{ii});
end
\end{matlabcode}

The VAR Toolbox includes functions to plot time series quickly and export them as high-quality PDFs, so that they can be used directly in research papers. The code shows how to plot the two time series in \colorbox{script!80}{\small \texttt{X}}.\vspace{.45cm}

\begin{matlabcode}
% Open a figure of the desired size and plot the selected variables
FigSize(26,8)
for ii=1:Xnvar
     subplot(1,2,ii)
     H(ii) = plot(X(:,ii),'LineWidth',3,'Color',cmap(1));
     title(Xvnames_long(ii));
     DatesPlot(datesnum(1),nobs,6,'q') % Set the x-axis labels
     grid on;
end
% Save figure
SaveFigure('graphics/BIV_DATA',1)
clf('reset')
\end{matlabcode}

\noindent Some useful functions used in the code above are:

\begin{itemize}
	\item \colorbox{script!80}{\small \texttt{FigSize.m}}: allows the user to choose the size and the proportions of the window for plotting the figure. This is particularly useful when creating figures with many panels.
	
	\item \colorbox{script!80}{\small \texttt{DatesPlot.m}}: Adds the dates (in numeric format) to the horizontal axis of a chart (at monthly, quarterly, or annual frequency) using a specified number of ticks (6 in the above example).
	
	\item \colorbox{script!80}{\small \texttt{SaveFigure.m}}: saves the chart to a specified location. 
\end{itemize}

\noindent  Figure \ref{fig:BIV_DATA} shows the behavior of the growth rate of real GDP and the interest rate on the US 1-year Treasury bill over the 1989:Q1 to 2019:Q4 sample period.\vspace{0.45cm}

\begin{figure}[th]
	\centering%
	\begin{minipage}[b]{.9\textwidth}
		\caption{\scshape{Endogenous Variables in the Simple VAR}}\vspace{0.1cm}
		\begin{center}
			\includegraphics[width=\textwidth]{BIV_DATA}
		\end{center}%
		\footnotesize{{\scshape Note.} Growth rate of US real GDP and the 1-year Treasury bill yield. Percentage points. Sample period: 1989:Q1 to 2019:Q4.}
		\label{fig:BIV_DATA}
	\end{minipage}
\end{figure}

\subsection{Estimation}\label{sec:reducedform}

The main idea of this paper is to employ a simple example -- containing the minimal possible number of ingredients -- to describe the workings of VAR models. A minimal example makes the exposition easier to follow, the algebra trivial, and allows to double check in a straightforward way the calculations performed by Matlab. These advantages, however, come at the cost of some unrealistic assumptions, which affect the economic interpretation of the results. In most cases, the examples that follow are too simple and parsimonious to credibly identify structural shocks (such as monetary policy shocks, or demand and supply shocks) and approximate the true structure of the economy. The VAR Toolbox also includes more realistic examples based on replications of existing papers. The replication codes, which are not discussed in this paper, can be found in the folder  \path{Replic/}.

Consider a simple bivariate VAR(1), i.e. a VAR where the number of endogenous variables is $k=2$ and the number of lags is $p=1$, with a constant term. Let the two endogenous variables be output growth ($y_{t}$) and the interest rate on the 1-year Treasury Bill ($r_{t}$). The reduced-form representation of the bivariate VAR(1) can be written as:%
\begin{equation}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t}%
	\end{array}%
	\right] =%
	\begin{bmatrix}
		c_{y} \\
		c_{r} %
	\end{bmatrix}%
	+
	\begin{bmatrix}
		\phi _{11} & \phi _{12} \\
		\phi _{21} & \phi _{22}%
	\end{bmatrix}%
	\left[
	\begin{array}{c}
		y_{t-1} \\
		r_{t-1}%
	\end{array}%
	\right] +\left[
	\begin{array}{c}
		u_{yt} \\
		u_{rt}%
	\end{array}%
	\right]   \label{eq:red_var_1}
\end{equation}%
or:%
\begin{equation}
	\begin{array}{c}
		y_{t}=c_y+ \phi _{11}y_{t-1}+\phi _{12}r_{t-1}+u_{yt}, \\
		r_{t}=c_r+ \phi _{21}y_{t-1}+\phi _{22}r_{t-1}+u_{rt},%
	\end{array}
	\label{eq:struct_var_2}
\end{equation}
In matrix form, the VAR in \eqref{eq:red_var_1} can be written more compactly as
\begin{equation}
	x_{t}= c + \Phi_1 x_{t-1}+u_{t}, \label{eq:red_var_0}
\end{equation}%
where $x_{t}$ is $2 \times T$ matrix collecting the two endogenous variables;\footnote{Note that, following the notation of the previous section, $x_t$ is the transpose of \colorbox{script!80}{\small \texttt{X}}.} $c$ is a $2 \times 1$ vector of constants; $\Phi_1$ is a $2 \times 2$ matrix of autoregressive coefficients; and $u_{t}$ is a $2 \times T$ vector of serially uncorrelated innovations, generally referred to as \textbf{reduced-form residuals}. Typically, the reduced-form residuals are correlated among themselves. Their covariance matrix can be written as:
\begin{equation}
	\mathbb{V}(u_{t})\equiv \Sigma_{u}=\left[
	\begin{array}{cc}
		\sigma _{y}^{2} & \sigma _{yr}^{2} \\
		\sigma _{yr}^{2} & \sigma _{r}^{2}%
	\end{array}%
	\right].  \label{eq:red_cov_1}
\end{equation}%
The covariance matrix $\Sigma_{u}$ is $2 \times 2$ symmetric matrix. Its diagonal elements are the variances of the estimated reduced-form innovations, $\sigma _{y}^{2}$ and $\sigma _{r}^{2}$; and the identical off-diagonal elements are instead equal to the covariance between the estimated reduced-form residuals, $\sigma _{yr}^{2}$. The covariance between the estimated reduced-form residuals plays an important role in VARs because it collects the information on the contemporaneous relation among the variables in the system (after controlling for their persistence) -- which, as we shall see below, is a key element of the identification problem in VARs.

The estimation of a VAR\ model can be done with a simple line of code, using the \colorbox{script!80}{\small \texttt{VARmodel.m}} function. There are three crucial inputs to this function: (i) a matrix including the endogenous variables, \colorbox{script!80}{\small \texttt{X}}; (ii) an integer specifying the number of lags to use, \colorbox{script!80}{\small \texttt{nlag}}; and an integer specifying whether a constant or a trend should be included, \colorbox{script!80}{\small \texttt{det}}. The code below shows how to estimate the simple VAR model in \eqref{eq:red_var_1}.\vspace{.45cm}

\begin{matlabcode}
% Make a common sample by removing NaNs
[X, fo, lo] = CommonSample(X);
% Set the deterministic variable in the VAR (1=constant, 2=trend)
det = 1;
% Set number of lags
nlags = 1;
% Estimate VAR by OLS
[VAR, VARopt] = VARmodel(X,nlags,det);
\end{matlabcode}

The results of the VAR estimation are stored in the structures \colorbox{script!80}{\texttt{VAR}} and \colorbox{script!80}{\texttt{VARopt}}. These are two crucial objects in the VAR Toolbox. Not only they collect all the relevant information of the estimated VAR, but they are also contain various options for identification (as shown below) and are updated with new information after each step of the VAR analysis.\footnote{For those familiar with Dynare, the structures \colorbox{script!80}{\texttt{VAR}} and \colorbox{script!80}{\texttt{VARopt}} are similar in spirit to the \colorbox{script!80}{\texttt{\_m}} and \colorbox{script!80}{\texttt{\_oo}} structures.}

The structure \colorbox{script!80}{\texttt{VAR}}\ includes all the inputs to the \colorbox{script!80}{\small \texttt{VARmodel.m}} function -- such as the matrix of endogenous variables (\colorbox{script!80}{\texttt{VAR.ENDO}}), the chosen number of lags (\colorbox{script!80}{\texttt{VAR.nlags}}), the number of endogenous
variables\ (\colorbox{script!80}{\texttt{VAR.nvar}}), etc). Crucially, the structure \colorbox{script!80}{\texttt{VAR}}\ also includes the estimation output. These can be seen at screen by double-clicking on the structure \colorbox{script!80}{\texttt{VAR}}\ in the Matlab workspace or by printing its output in the command window with the command \colorbox{script!80}{\texttt{disp(VAR)}}:

\begin{matlaboutput}
>> disp(VAR)

	ENDO: [123x2 double]
	nlag: 1
	const: 1
	EXOG: []
	nobs: 122
	nvar: 2
	nvar_ex: 0
	nlag_ex: 0
	ncoeff: 2
	ntotcoeff: 3
	eq1: [1x1 struct]
	eq2: [1x1 struct]
	Ft: [3x2 double]
	F: [2x3 double]
	sigma: [2x2 double]
	resid: [122x2 double]
	X: [122x3 double]
	Y: [122x2 double]
	Fcomp: [2x2 double]
	maxEig: 0.9559
	B: []
	b: []
	PSI: []
	Fp: []
	IV: []
\end{matlaboutput}

\noindent A few of the elements of the structure \colorbox{script!80}{\texttt{VAR}}\ are worth describing in detail

\begin{itemize}
\item \textbf{Estimated coefficients}. \ The matrix \colorbox{script!80}{\texttt{VAR.F}} collects all estimated coefficients following the notation in \eqref{eq:red_var_1}, so that \colorbox{script!80}{\texttt{VAR.F}}$=[c \ \Phi]$. For a VAR with $1$ lag and $2$ endogenous variables plus a constant, this means that \colorbox{script!80}{\texttt{VAR.F}} is a $2\times (1 + 1\times 2)$ matrix, as shown by the following command:
\begin{matlaboutput}
>> disp(VAR.F)

	 0.3630    0.3788    0.0041
	-0.0729    0.2607    0.9541
\end{matlaboutput}
\medskip

\item \textbf{VAR residuals} \ The matrix \colorbox{script!80}{\texttt{VAR.resid}} collects the VAR reduced-form residuals, as defined by \eqref{eq:red_var_1}. In line with the convention in equation \eqref{eq:convention} the residuals are stored as column vectors so that \colorbox{script!80}{\texttt{VAR.resid}}$=u_t'$. That is, for a bivariate VAR with $1$ lag, \colorbox{script!80}{\texttt{VAR.resid}} is a $2 \times (T-1)$ matrix (as one observation gets lost when computing the lags of $x_t$).\medskip

\item \textbf{Reduced-form covariance matrix} \ The matrix \colorbox{script!80}{\texttt{VAR.sigma}} collects the covariance matrix of the VAR reduced-form residuals defined by \eqref{eq:red_cov_1}. The convention is such that \colorbox{script!80}{\texttt{VAR.sigma}}=$\Sigma_{u}$. For a VAR with $2$ endogenous variables, the covariance matrix \colorbox{script!80}{\texttt{VAR.sigma}} is a symmetric $2\times 2$ matrix:
\begin{matlaboutput}
>> disp(VAR.sigma)

	0.2891    0.0782
	0.0782    0.1473
\end{matlaboutput}
\medskip

\item \textbf{Companion matrix} \ The matrix \colorbox{script!80}{\texttt{VAR.Fcomp}} includes the \textit{companion matrix}. The companion matrix allows rewriting VARs with lags greater than $1$ as VAR(1) -- see section \ref{app:dynamic} for details. Trivially, in the case of a VAR(1) the companion matrix \colorbox{script!80}{\texttt{VAR.Fcomp}} is identical to \colorbox{script!80}{\texttt{VAR.F}} after dropping deterministic coefficients, such as the constant or trend. As it will become clear below, the companion matrix plays a crucial role for many aspects of VAR analysis, such as the computation of impulse responses, the identification of structural shocks, etc.

\item \textbf{Equation-by-equation estimation output}. The structures \colorbox{script!80}{\texttt{VAR.eq1}} and  \colorbox{script!80}{\texttt{VAR.eq2}} include the OLS equation-by-equation estimation results.
\end{itemize}

\noindent Note that the VAR structure includes a few empty objects (e.g. \colorbox{script!80}{\texttt{VAR.B}}). This is because, for the moment, the code has only estimated the reduced-form VAR, and has not yet identified the structural shocks or computed impulse response functions. This will be the object of the next section.

The structure \colorbox{script!80}{\texttt{VARopt}} includes a few auxiliary variables that are created automatically by the  \colorbox{script!80}{\texttt{VARmodel.m}} function and will be needed below for the calculation of impulse responses, variance decompositions, etc. As above, the variables stored in \colorbox{script!80}{\texttt{VARopt}} can be seen by executing:

\begin{matlaboutput}
>> disp(VARopt)

	vnames: []
	vnames_ex: []
	snames: []
	nsteps: 40
	impact: 0
	shut: 0
	ident: 'short'
	recurs: 'wold'
	ndraws: 1000
	mult: 10
	pctg: 95
	method: 'bs'
	sr_hor: 1
	sr_rot: 500
	sr_draw: 100000
	sr_mod: 1
	pick: 0
	quality: 1
	suptitle: 0
	datesnum: []
	datestxt: []
	datestype: 1
	firstdate: []
	frequency: 'q'
	figname: []
	FigSize: [26 24]
\end{matlaboutput}

\noindent These variables include the number of steps for impulse response functions and variance decompositions (\colorbox{script!80}{\texttt{nsteps}}), the labels of the endogenous variables for plots (\colorbox{script!80}{\texttt{vnames}}), the confidence levels for the computation of confidence intervals (\colorbox{script!80}{\texttt{pctg}}), etc. While some variables are automatically created by the \colorbox{script!80}{\texttt{VARmodel}} function (i.e. where a default option is possible), some other variables need to be inputted by the user. For example, the code below manually adds to the \colorbox{script!80}{\texttt{VARopt}} structure the names of the VAR endogenous variables: \vspace{.45cm}

\begin{matlabcode}
% Print at screen the outputs of the VARmodel estimation
disp(VAR)
disp(VARopt)
% Update the VARopt structure with additional details
VARopt.vnames = Xvnames_long;
\end{matlabcode}

Finally, the VAR Toolbox has a built-in function to print at screen some of the most important estimation results in an easy and quick way:\vspace{.45cm}

\begin{matlabcode}
% Print at screen VAR coefficients and create table
[TABLE, beta] = VARprint(VAR,VARopt,2);
\end{matlabcode}

\noindent which produces the following output in the Matlab command window:

\begin{matlaboutput}
Reduced form VAR estimation:

                         Real GDP Growth    1-year Int. Rate 
constant                       0.3630          -0.0729 
Real GDP Growth(-1)            0.3788           0.2607 
1-year Int. Rate(-1)           0.0041           0.9541 

VAR eigenvalues:
0.3769
0.95592

Reduced-form covariance matrix:
0.28909       0.078151
0.078151      0.14726
\end{matlaboutput}

The cell array \colorbox{script!80}{\texttt{beta}} includes the estimated values of the coefficients in the VAR, while the cell array \colorbox{script!80}{\texttt{TABLE}} includes -- in addition to the estimated coefficients -- their standard errors and associated t-statistics. 

\section{Structural VARs \& The Identification Problem}\label{sec:struct_var}

This secction describes how to move from the reduced-form VAR estimated in the previous section to a structural VAR representation. The key element in this transformation is the so-called \textbf{identification problem}, which consists in finding a way to recover the structural shocks from the estimated reduced-form residuals.

\subsection{The structural VAR representation}

The reduced-form VAR(1) defined by \eqref{eq:red_var_1} and estimated in the previous section can be written in its structural form as:
\begin{equation}
	x_{t} = c + \Phi x_{t-1}+B\varepsilon_{t}
	\label{eq:struct_var_0}
\end{equation}%
where $x_{t}$, $c$, $\Phi$ have been already defined above; $B$ is a $k \times k$ matrix of coefficients, typically referred to as \textbf{structural impact matrix}; and $\varepsilon_{t}$ is a $2 \times T$ matrix of serially uncorrelated innovations, generally referred to as \textbf{structural shocks}, which are assumed to be mutually uncorrelated with zero mean and unit variance.\footnote{Note that the fact that the variance of the structural shocks is equal to one is just a harmless normalization which does not involve a loss of generality (as long as the diagonal elements of $B$ remain unrestricted. An alternative (and equivalently valid) normalization would be to leave unrestricted the variance of the structural innovations, namely $\varepsilon_{it}\sim \mathcal{N}(0,\sigma _{i}^{2})$ and assume that the diagonal elements of $B$ are equal to $1$.} The relation between the structural shocks and the reduced form innovations is therefore given by the following identity:
\begin{equation}\label{eq:struct_shocks}
	u_{t} = B\varepsilon_{t},
\end{equation}%

The system of equations defined by the structural VAR (\ref{eq:struct_var_0}) should be thought as approximating the true (and unobserved) structure of the economy (for example, the structure of a DSGE\ model); and the structural shocks as having a well-defined economic interpretation (for example, TFP\ shocks or monetary policy shocks). 

In our simple example of a bivariate VAR(1), let the only the two structural shocks be a demand shock $\left( \varepsilon_{t}^{Demand} \right)$ and a monetary policy shock\ $\left( \varepsilon_{t}^{MonPol} \right)$. This is, of course, another unrealistic simplifying assumption. In reality, there are many more shocks that drive movements in GDP growth and the short-term interest rate. But, as it will become clear below, this assumption is going to simplify the exposition of the identification problem described below, and to clarify the difference between various identification approaches.

The simple structural VAR(1) can be written as a system of linear equations:
\begin{equation}\label{eq:struct_var_1}
	\begin{bmatrix}
		y_{t} \\
		r_{t}%
	\end{bmatrix}%
	=%
	\begin{bmatrix}
		c_{1} \\
		c_{2} 
	\end{bmatrix}%
	+
	\begin{bmatrix}
		\phi _{11} & \phi _{12} \\
		\phi _{21} & \phi _{22}%
	\end{bmatrix}%
	\begin{bmatrix}
		y_{t-1} \\
		r_{t-1}%
	\end{bmatrix}%
	+%
	\begin{bmatrix}
		b_{11} & b_{12} \\
		b_{21} & b_{22}%
	\end{bmatrix}%
	\begin{bmatrix}
		\varepsilon_{t}^{Demand} \\
		\varepsilon_{t}^{MonPol}%
	\end{bmatrix}%
\end{equation}%
or:%
\begin{equation}\label{eq:struct_var_2}
	\begin{array}{c}
		y_{t}=\phi _{11}y_{t-1}+\phi _{12}r_{t-1}+b_{11}\varepsilon
		_{t}^{Demand}+b_{12}\varepsilon_{t}^{MonPol} \\
		r_{t}=\phi _{21}y_{t-1}+\phi _{22}r_{t-1}+b_{21}\varepsilon
		_{t}^{Demand}+b_{22}\varepsilon_{t}^{MonPol}%
	\end{array}
\end{equation}%
Moreover, as $\varepsilon_{t}=\left( \varepsilon_{t}^{Demand\prime },\varepsilon_{t}^{MonPol\prime }\right)'$ is assumed to be a $2\times T$ matrix of uncorrelated white noise processes, their covariance matrix can be written as:
\begin{equation} \label{eq:struct_cov_1}
	\mathbb{V}(\varepsilon_{t})\equiv \Sigma_{\varepsilon }=\left[
	\begin{array}{cc}
		1 & 0 \\
		0 & 1%
	\end{array}%
	\right] =I_{2}. 
\end{equation}%
The assumption that the elements of $\varepsilon_{t}$ are mutually uncorrelated is crucial. It implies that we can track the effects that a shock to, say, $\varepsilon_{t}^{Demand}$ has on all variables in the VAR keeping the other shock to zero (and vice versa). The $B$ matrix is also crucial. To see that, consider a unit surprise in $\varepsilon_{t}^{MonPol}$, i.e. a surprise tightening in monetary policy. What are the consequences for output growth $y_{t}$ and the short-term interest rate $r_{t}$? The answer to this question is given by the second column of the $B$ matrix: $y_{t}$ will move by $b_{12}$ and $r_{t}$ will move by $b_{22}$. This is why the $B$ matrix is also known as the structural impact matrix. The $\Phi $ matrix can then be used to track the dynamic effects of the shocks in $t+1$, $t+2$, etc.

The structural innovations $\varepsilon_{t}$ are unobservable, which means that we cannot directly estimate (\ref{eq:struct_var_2}). However, we can link the structural innovations and impact matrix to the reduced-form innovations using \eqref{eq:struct_shocks}:
\begin{equation}
	\begin{array}{c}
		u_{yt}=b_{11}\varepsilon_{t}^{Demand}+b_{12}\varepsilon_{t}^{MonPol},
		\\
		u_{rt}=b_{21}\varepsilon_{t}^{Demand}+b_{22}\varepsilon_{t}^{MonPol}.%
	\end{array}
	\label{eq:red_resid_1}
\end{equation}%
Equation \eqref{eq:red_resid_1} shows how the reduced-form innovations $u_{t}=\left( u_{yt}',u_{rt}'\right)'$ are a linear combination of the structural innovations. Thus, differently from structural VARs, reduced-form VAR cannot be informative about how shocks propagate through the system. An innovation to $u_{yt}$ could be driven by either $\varepsilon_{t}^{Demand}$ or $\varepsilon_{t}^{MonPol}$ (and vice versa). To be able to talk about the causal effects of a shock to the variables in the VAR\ we need to find a way to recover the $B$ matrix. This is the essence of identification in VARs, which is discussed next.

\subsection{Stability and the Wold Representation}\label{sec:wold}

Before turning to the identification problem, it is useful to introduce an alternative representation of the VAR that will prove instrumental for understanding the dynamic properties of the model and for conducting structural analysis. This representation, known as the Wold representation, expresses each observation as a function of past structural shocks and initial conditions.

The Wold representation can be obtained by recursively substituting the lagged values on the right-hand side of the structural VAR in equation (\ref{eq:struct_var_2}). Starting from:
\begin{equation}
	x_{t}=\Phi x_{t-1}+B\varepsilon_{t},
\end{equation}%
we can substitute $x_{t-1}=\Phi x_{t-2}+B\varepsilon_{t-1}$ into the equation above to get:
\begin{equation}
	x_{t}=\Phi(\Phi x_{t-2}+B\varepsilon_{t-1})+B\varepsilon_{t}=\Phi^{2}x_{t-2}+\Phi B\varepsilon_{t-1}+B\varepsilon_{t}.
\end{equation}%
Repeating this process recursively yields:
\begin{equation}\label{eq:wold}
	x_{t}=\Phi^{t}x_{0}+\sum_{j=0}^{t-1}\Phi^{j}B\varepsilon_{t-j},
\end{equation}%
where $x_{0}$ represents the initial condition. This is the Wold representation of the VAR. It shows that each observation ($x_{t}$) can be decomposed into two terms: (i) a component driven by the initial condition ($\Phi^{t}x_{0}$), and (ii) a weighted sum of all past and present structural shocks ($\sum_{j=0}^{t-1}\Phi^{j}B\varepsilon_{t-j}$).

A key property of well-behaved VARs is that the effect of shocks should progressively dissipate over time. This property is known as stability. For a VAR to be stable, we need $\Phi^{j}$ to converge to zero as $j\to\infty$. This happens if and only if all eigenvalues of $\Phi$ are less than one in modulus:
\begin{equation}
	\left|\text{eig}(\Phi)\right|<1.
\end{equation}%
When this condition is satisfied, the Wold representation converges as $t\to\infty$:
\begin{equation}\label{eq:wold_inf}
	x_{t}=\sum_{j=0}^{\infty}\Phi^{j}B\varepsilon_{t-j},
\end{equation}%
since $\Phi^{\infty}=0$.

The stability condition has important implications for the unconditional moments of the VAR. Under stability, the unconditional mean of $x_{t}$ is zero (assuming no constant term in the VAR):
\begin{equation}
	\mathbb{E}[x_{t}]=\mathbb{E}\left[\sum_{j=0}^{\infty}\Phi^{j}B\varepsilon_{t-j}\right]=\sum_{j=0}^{\infty}\Phi^{j}B\mathbb{E}[\varepsilon_{t-j}]=0,
\end{equation}%
where the last equality follows from the white noise assumption $\mathbb{E}[\varepsilon_{t}]=0$. If the VAR includes a constant term $\alpha$, the Wold representation becomes:
\begin{equation}
	x_{t}=\sum_{j=0}^{\infty}\Phi^{j}\alpha+\sum_{j=0}^{\infty}\Phi^{j}B\varepsilon_{t-j},
\end{equation}%
and the unconditional mean is given by:
\begin{equation}
	\mathbb{E}[x_{t}]=\sum_{j=0}^{\infty}\Phi^{j}\alpha=(I-\Phi)^{-1}\alpha,
\end{equation}%
where the last equality follows from the fact that $\sum_{j=0}^{\infty}\Phi^{j}=(I-\Phi)^{-1}$ when all eigenvalues of $\Phi$ are less than one in modulus.

In practice, stability can be easily verified by checking the maximum eigenvalue of the estimated $\Phi$ matrix. Most VAR software packages, including the VAR Toolbox, report this diagnostic automatically. If the stability condition is violated, the impulse responses and variance decompositions discussed in Section \ref{sec:dynamic} will not be well-defined, as the infinite sums in the Wold representation will not converge.

\subsection{The Identification\ Problem}

The key difference between the structural and reduced-form VARs lies in the covariance matrix of their innovations. While the covariance matrix of the structural VAR\ innovations is diagonal ($\Sigma_{\varepsilon }=I_{2}$), in general the reduced-form innovations are correlated among themselves, so that their covariance is given by a symmetric matrix non-diagonal matrix ($\Sigma_{u}$).

As hinted above, the covariance of the estimated reduced-form residuals plays an important role in VARs because it collects the information on the contemporaneous relation among the variables in the structural system, which (as we have just seen) is also captured by the $B$ matrix. Indeed, using (\ref{eq:struct_shocks}) the covariance matrix of the reduced for residuals can be re-written as:
\begin{equation}
	\Sigma_{u}=\mathbb{E}\left[ u_{t}u_{t}'\right] =B\mathbb{E}\left[
	\varepsilon_{t}\varepsilon_{t}'\right] B'=BB'=%
	\left[
	\begin{array}{cc}
		b_{11}^{2} & b_{11}b_{21}+b_{12}b_{22} \\
		b_{11}b_{21}+b_{12}b_{22} & b_{22}^{2}%
	\end{array}%
	\right]   \label{eq:red2struct_0}
\end{equation}%
This means that there is a mapping between the estimated covariance matrix of the reduced-form residuals ($\Sigma_{u}$)\ and the unobserved matrix of structural impact coefficients ($B$). The identification problem simply boils down to finding a $B$ matrix that satisfies $\Sigma_{u}=BB'$.

Unfortunately this is not as easy as it sounds. We can think of (\ref{eq:red2struct_1}) as a system of non-linear equations in the $4$ unknown coefficients of the $B$ matrix. The problem is that the $\Sigma_{u}$ matrix, given its symmetric nature, leads to only $3$ independent restrictions. In other words, we have:
\begin{equation}
	\left[
	\begin{array}{cc}
		\sigma _{y}^{2} & \sigma _{yr}^{2} \\
		\sigma _{yr}^{2} & \sigma _{r}^{2}%
	\end{array}%
	\right] =\underset{B}{\underbrace{\left[
			\begin{array}{cc}
				b_{11} & b_{12} \\
				b_{21} & b_{22}%
			\end{array}%
			\right] }}\underset{B'}{\underbrace{\left[
			\begin{array}{cc}
				b_{11} & b_{21} \\
				b_{12} & b_{22}%
			\end{array}%
			\right] }},  \label{eq:red2struct_2}
\end{equation}%
which can be rewritten as the following system of equations:%
\begin{equation}
	\left\{
	\begin{array}{l}
		\sigma _{y}^{2}=b_{11}^{2}+b_{12}^{2} \\
		\sigma _{yr}^{2}=b_{11}b_{21}+b_{12}b_{22} \\
		\sigma _{yr}^{2}=b_{11}b_{21}+b_{12}b_{22} \\
		\sigma _{r}^{2}=b_{21}^{2}+b_{22}^{2}%
	\end{array}%
	\right.   \label{eq:red2struct_3}
\end{equation}%
Because of the symmetry of the $\Sigma_{u}$ matrix, the second and the third equation are identical. This means that we are left with $4$ unknowns (the $b$'s) but only $3$ equations. The system is under-identified, meaning that there are infinite combination of the $b$'s that solve the system of equations (\ref{eq:red2struct_3}).

How to solve a system of $3$ equations in $4$ unknowns? The solution is (typically)\ to draw from economic theory an additional condition that allows us to recover a fourth equation -- and therefore, solve the system of equations (\ref{eq:red2struct_3}). There are many ways of solving the identification problem described above. The next section shows how to implement a few popular identification schemes in the VAR Toolbox.


\section{Identification in the VAR Toolbox}\label{sec:ident}

Many solutions have been developed in the literature to address the identification problem described in the previous section. This section describes how to implement some of the most popular ones by means of simple examples using the VAR Toolbox. Specifically, the next section covers identification by zero contemporaneous restrictions, identification by zero long-run restrictions, identification by sign restrictions, identification with external instruments, and identification with a combination of sign restrictions and external instruments.

\subsection{Identification by zero contemporaneous restrictions}\label{sec:short}

Identification using zero contemporaneous restrictions -- also improperly known as Cholesky or recursive identification -- was developed by \cite{Sims1980}, and is by far the most commonly used identification scheme used in the literature. The idea behind this approach to identification is that some structural shocks may take time to transmit through the economy, and therefore have no contemporaneous effects on (some of) the endogenous variables in the VAR. For example, it is widely believed that there are substantial lags in the transmission of monetary policy to the real economy, while this is not the case for other shocks (such as technology shocks, for example). Under this assumption, one could then impose that monetary policy shocks have zero contemporaneous effects on one (or a subset) of the endogenous variables in the VAR. 

This intuition can be formalized in the context of the simple bivariate VAR described in the previous section. As discussed above, the VAR in \eqref{eq:red_var_0} is not identified, as we have four unknowns (the elements of the $B$ matrix) but only three independent equations in those unknowns. Imposing zero contemporaneous restrictions amounts to assuming that some of the non-diagonal elements of $B$ are equal to zero, thus reducing the number of unknown coefficients. 

In the simple case considered here, it will therefore suffice to set to zero one element of the $B$ matrix to be able to solve for the remaining three elements. But which element of the $B$ matrix should be set to zero? One could maintain the assumption that monetary policy shock affects on impact the short-term interest rate (i.e. $r_{t}$) but take time to transmit to the real economy and affect output ($y_{t}$). If we wanted to implementn This identifying assumption implies that $b_{12}=0$, so that the structural VAR can be written as:
\begin{equation}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t}%
	\end{array}%
	\right] =%
	\begin{bmatrix}
		c_{y} \\
		c_{r} %
	\end{bmatrix}
	+
	\begin{bmatrix}
		\phi _{11} & \phi _{12} \\
		\phi _{21} & \phi _{22}%
	\end{bmatrix}%
	\left[
	\begin{array}{c}
		y_{t-1} \\
		r_{t-1}%
	\end{array}%
	\right] +\left[
	\begin{array}{cc}
		b_{11} & 0 \\
		b_{21} & b_{22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Demand} \\
		\varepsilon_{t}^{MonPol}%
	\end{bmatrix}%
	,  \label{eq:struct_var_ch}
\end{equation}
where note that $y_{t}$ is not contemporaneously affected by $\varepsilon_{t}^{MonPol}$, while $r_{t}$ is contemporaneously affected by both $\varepsilon_{t}^{Demand}$ and $\varepsilon_{t}^{MonPol}$, through the coefficient $b_{21}$ and $b_{22}$.\footnote{Typically, it is assumed that the first variable in the system is the only one that is contemporaneously affected by the first structural shock, the second variable is contemporaneously affected by the first and second structural shocks, and so on. The reason for this assumption depends on how this type of identification is commonly implemented, i.e. via a Cholesky decomposition of the reduced-from covariance matrix $\Sigma_u$. This assumption is also maintained in the VAR Toolbox. See Appendix \ref{app:ident} for details.} As we now have $3$ independent equations and $3$ unknowns, we can recover the elements of the $B$ matrix by solving the system of equations implied by $\Sigma_u=BB'$. The structural VAR\ is thus identified.

\paragraph{Math} 

The identification by zero short-run restrictions solves the identification problem by setting to zero some of the non-diagonal elements of the structural impact matrix ($B$), thus reducing the number of unknown coefficients in the $B$ matrix to the same number of equations implied by the condition:
\begin{equation}\label{eqapp:short0}
	\Sigma_u = BB' 
\end{equation}
The number of zeros that need to be imposed to achieve identification depend on the number of variables in the VAR -- and, crucially, they increase at a faster rate than the numbers of endogenous variables. As discussed in section \ref{sec:short}, in a simple bivariate VAR a single zero restriction is enough to achieve identification. To see that, consider the system of equations implied by $\Sigma_u = BB'$, namely:
\begin{equation}\label{eqapp:short1}
	\left[
	\begin{array}{cc}
		\sigma _{y}^{2} & \sigma _{yr}^{2} \\
		- & \sigma _{r}^{2}%
	\end{array}%
	\right] =\underset{B}{\underbrace{\left[
			\begin{array}{cc}
				b_{11} & b_{12} \\
				b_{21} & b_{22}%
			\end{array}%
			\right] }}\underset{B'}{\underbrace{\left[
			\begin{array}{cc}
				b_{11} & b_{21} \\
				b_{12} & b_{22}%
			\end{array}%
			\right] }}
		\Rightarrow
		\left\{
		\begin{array}{l}
			\sigma _{y}^{2}=b_{11}^{2}+b_{12}^{2} \\
			\sigma _{yr}^{2}=b_{11}b_{21}+b_{12}b_{22} \\
			\sigma _{yr}^{2}=b_{11}b_{21}+b_{12}b_{22} \\
			\sigma _{r}^{2}=b_{21}^{2}+b_{22}^{2}%
		\end{array}%
		\right. 
\end{equation}%
The above system has four unknowns but only three independent equations, as the second and the third equation are identical. When setting $b_{12}=0$, the system of equations (\ref{eqapp:short1}) becomes:%
\begin{equation}\label{eqapp:short2}
	\left\{
	\begin{array}{l}
		\sigma _{y}^{2}=b_{11}^{2}, \\
		\sigma _{yr}^{2}=b_{11}b_{21}, \\
		\sigma _{r}^{2}=b_{21}^{2}+b_{22}^{2}.%
	\end{array}%
	\right.
\end{equation}%
which can be easily solved to get:%
\begin{equation}\label{eqapp:short3}
	\left\{
	\begin{array}{l}
		b_{11}=\sigma _{y}, \\
		b_{21}=\sigma _{yr}^{2}/\sigma _{y}^{2}, \\
		b_{22}=\sqrt{\sigma _{r}^{2}-\frac{\sigma _{yr}^{2}}{\sigma _{y}^{2}}.}%
	\end{array}%
	\right.
\end{equation}%
which shows that the VAR\ is identified. 

\vspace{.5cm}
\begin{notebox}
\textbf{The Cholesky decomposition.} \ This identification scheme is often referred to as `Cholesky' identification. The reason is that the solution above can be obtained with a Cholesky decomposition of the reduced-form covariance matrix.

\indent Specifically, a symmetric and positive-definite matrix like $\Sigma_{u}$ always admits the following unique decomposition:
\begin{equation}\label{eqapp:short4}
	\Sigma_{u}=\left[
	\begin{array}{cc}
		\sigma _{y}^{2} & \sigma _{yr}^{2} \\
		\sigma _{yr}^{2} & \sigma _{r}^{2}%
	\end{array}%
	\right] =\left[
	\begin{array}{cc}
		p_{11} & 0 \\
		p_{12} & p_{22}%
	\end{array}%
	\right] \left[
	\begin{array}{cc}
		p_{11} & p_{12} \\
		0 & p_{22}%
	\end{array}%
	\right] =PP'
\end{equation}%
where the lower triangular matrix $P$ is known as the Cholesky factor of $\Sigma_{u}$. Recalling that (i) $\Sigma_{u}=BB'$ and (ii) we assumed that $B$ is also lower triangular (i.e. $b_{21}=0$), it follows that $P=B$. In other words, instead of solving by hand the system of equations \eqref{eqapp:short2}, we can ask Matlab to compute the Cholesky factor of $\Sigma_u$. 

This is particularly useful for large VARs, since the system of equations implied by $\Sigma_u = BB'$ becomes increasingly complex as the dimensionality of the VAR increases. With reference to the example in section \ref{sec:short}, the Cholesky decomposition of $\Sigma_u$ can be implemented by executing the following command in the Matlab command window:

\begin{matlaboutput}
>> chol(VAR.sigma,'lower')

ans= 
	0.5377         0
	0.1454    0.3552
\end{matlaboutput}

\noindent which can be shown being the solution to \eqref{eqapp:short3}.
\end{notebox}

\paragraph{In Matlab} Zero short-run restrictions can be implemented in the VAR Toolbox with a few lines of code. The structure \colorbox{script!80}{\small\texttt{VARopt}} includes a field that allows the user to choose what identification scheme to employ. The mnemonic for the identification by short-run restrictions is the string \colorbox{script!80}{\small\texttt{'short'}}. So, zero contemporaneous (or short-run) restrictions can be selected by simply executing the following line of code:\vspace{.45cm}

\begin{matlabcode}
% Update the VARopt structure to select zero short-run restrictions
VARopt.ident = 'short';
\end{matlabcode}

\noindent Note that the field \colorbox{script!80}{\small\texttt{VARopt.ident}} is automatically set to \colorbox{script!80}{\small\texttt{'short'}} when it is first created as an output of the \colorbox{script!80}{\small\texttt{VARmodel.m}} function -- so that, in the case of zero short-run restrictions, the above line of code is actually redundant. It is also useful (but not necessary) to update the \colorbox{script!80}{\small\texttt{VARopt}} structure with a few addition details that will be used when plotting the impulse responses and saving them: \vspace{.45cm}

\begin{matlabcode}
% Update the VARopt structure with additional details
VARopt.vnames = Xvnames_long;  		% variable names in plots
VARopt.nsteps = 12;  				% max horizon of IRF
VARopt.FigSize = [26,12];  			% size of window for figure
VARopt.firstdate = datesnum(1);  	% first date in plots
VARopt.frequency = 'q';  			% frequency of the data
VARopt.snames = ...					% shock names
{'$\epsilon\texttt{\^{Demand}'},'$\epsilon\texttt{\^{MonPol}'}};
\end{matlabcode}

\noindent Finally, the actual calculation of the elements of the $B$ matrix under zero short-run restrictions is implemented with the function\colorbox{script!80}{\small\texttt{VARir.m}}, which also computes the impulse responses to the identified shocks.\footnote{In a similar fashion, identification can be achieved with the functions for the calculations of forecast error variance decompositions \colorbox{script!80}{\small\texttt{VARvd.m}} and historical decompositions \colorbox{script!80}{\small\texttt{VARhd.m}}.} This function takes as an input the estimated \colorbox{script!80}{\small\texttt{VAR}} structure (as described in the previous section) as well as the updated \colorbox{script!80}{\small\texttt{VARopt}} structure, namely:\vspace{.45cm}

\begin{matlabcode}
% Compute impulse response
[IR, VAR] = VARir(VAR,VARopt);
\end{matlabcode}

\noindent The \colorbox{script!80}{\small\texttt{VARir.m}} function has two outputs. The first one (\colorbox{script!80}{\small\texttt{IR}}, which we are going to ignore for the moment as it will be the focus of section \ref{dynamic}) is a matrix with the impulse responses calculated according to the identification scheme chosen. The second output is, again, the structure \colorbox{script!80}{\small\texttt{VAR}}, which also served as an input to the function. This is because the \colorbox{script!80}{\small\texttt{VARir.m}} function updates the \colorbox{script!80}{\small \texttt{VAR.B}} field from an empty matrix to the $B$ matrix corresponding to the chosen identification scheme. The $B$ matrix can be printed at screen by executing the following command in the Matlab command window:

\begin{matlaboutput}
>> disp(VAR.B)

ans =
	0.5377         0
	0.1454    0.3552
\end{matlaboutput}

\noindent As discussed above in section \ref{sec:struct_var}, the $B$ matrix is crucial to be able to track the effects of a shock through the system. Consider a unit surprise in $\varepsilon_{t}^{MonPol}$, i.e. a surprise tightening in monetary policy, in the structural VAR in equation \eqref{eq:struct_var_ch}. What are the impact effects of such shock on output growth $y_{t}$ and the short-term safe rate $r_{t}$? The answer to this question is given by the second column of the $B$ matrix. To see that: 
\begin{equation*}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t}%
	\end{array}%
	\right] =%
	\left[
	\begin{array}{cc}
		b_{11} & 0 \\
		b_{21} & b_{22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Demand} \\
		\varepsilon_{t}^{MonPol}%
	\end{bmatrix} = 
	\left[
	\begin{array}{cc}
		\texttt{0.5377} & \texttt{0} \\
		\texttt{0.1454} & \texttt{0.3552}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\texttt{0} \\
		\texttt{1}%
	\end{bmatrix}=
	\begin{bmatrix}
		\texttt{0} \\
		\texttt{0.3552}
	\end{bmatrix}
\end{equation*}%
The response of output is equal to $0$ -- not surprisingly, as we assumed so. The response of the interest rate is instead equal to $0.3552$. These impact effects then propagate through the system over time according to the transition matrix $\Phi$. For example, the effect of the shock on the endogenous variables one quarter after the shock has hit is given by:
\begin{equation*}
	\left[
	\begin{array}{c}
		y_{t+1} \\
		r_{t+1}%
	\end{array}%
	\right] =%
	\left[
	\begin{array}{cc}
		\phi_{11} & \phi_{12} \\
		\phi_{21} & \phi_{22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		y_{t}\\
		r_{t}%
	\end{bmatrix} = 
	\left[
	\begin{array}{cc}
		\texttt{0.3788} & \texttt{0.0041} \\
		\texttt{0.2607} & \texttt{0.9541}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\texttt{0} \\
		\texttt{0.3552}%
	\end{bmatrix}=
	\begin{bmatrix}
		\texttt{0.0015} \\
		\texttt{0.3388}
	\end{bmatrix}
\end{equation*}%
To see that, execute the following command in the Matlab command window:

\noindent \texttt{>> disp(VAR.Fcomp*VAR.B(:,2))}
\begin{matlaboutput}
	0.0015
	0.3388
\end{matlaboutput}

\noindent This is called the impulse response function at horizon $h=2$. The effect of the shock in $h=3$, $h=4$, etc. can be computed in a similar fashion, i.e. using again the transition matrix $\Phi$ to iterate forward the values of the endogenous variables from horizon $h=2$ to horizon $h=3$, $h=3$ to $h=4$, etc.\footnote{A detailed description of the calculation of impulse responses is provided in Appendix \ref{app:dynamic}.} The resulting impulse response function is stored in the matrix \colorbox{script!80}{\small\texttt{IR}}. In this example, the \colorbox{script!80}{\small\texttt{IR}} matrix has dimension $12$ (as specified above in \colorbox{script!80}{\small\texttt{VARopt.nsteps}}) $\times 2$ (the number of endogenous variables, $y_t$ and $r_t$) $\times 2$ (the number of shocks, $\varepsilon_t^{Demand}$ and $\varepsilon_t^{MonPol}$). So, for example, the response of output growth and the interest rate to the monetary policy shock in the first year since the shock hit can be printed at screen by executing the following command:

\begin{matlaboutput}
>> disp(IR(1:4,:,2))

	0 	0.3552
	0.0015    0.3388
	0.0019    0.3237
	0.0021    0.3093
\end{matlaboutput}

\vspace{.5cm}
\begin{notebox}
	\textbf{Variable ordering matters!} As explained above, this identification is achieved with a Cholesky decomposition of the reduced form covariance matrix. This implies that the ordering of the variables in the matrix \colorbox{script!80}{\small \texttt{X}} matters. The \colorbox{script!80}{\small\texttt{'short'}} option implicitly assumes that the \colorbox{script!80}{\small\texttt{B}} matrix is lower triangular. In turn this means that the structural shock associated with the first equation affects all variables in the system (as captured by the first column of the \colorbox{script!80}{\small\texttt{B}} matrix); the the structural shocks associated with the second equation has zero effect on the first endogenous variable, and affects the variables in the system; etc.
\end{notebox}

Finally, note that a time series of the structural shocks can be obtained by inverting equation \eqref{eq:struct_shocks}, which gives $\varepsilon_t=B^{-1}u_t$. In Matlab, the structural shocks can be computed by executing a single line of code:\vspace{.45cm}

\begin{matlabcode}
% Compute structural shocks (Tx2)
eps_short = (VAR.B\VAR.resid')';
\end{matlabcode}

\noindent where \colorbox{script!80}{\small\texttt{eps\_short}} is the $T \times 2$ matrix of structural shocks. It is also possible to verify that the structural shocks are orthogonal to each other by typing in the command window:

\noindent \texttt{}
\begin{matlaboutput}
>> disp(corr(eps'))

	1.0000    0.0000
	0.0000    1.0000
\end{matlaboutput}

\noindent which implies that the covariance matrix of the structural shocks is diagonal.


\subsection{Identification by zero long-run restrictions}\label{sec:long}

\cite{BlanchardQuah1989} proposed an alternative identification method that builds on a similar intuition to the zero short-run restrictions described in the previous section, but imposes zero restrictions on the long-run effect of structural shocks. For example, some models imply that only technology shocks have a permanent effect on the level of output. On the contrary, demand shocks have a zero effect on the level of output in the long-run. Under this assumption, one could then impose that non-technology shocks have zero contemporaneous effects on the level of output in the long-run. 

But how to map the $B$ matrix, which captures the contemporaneous effects of structural shocks on the endogenous variables in the VAR, to the long-run effects of structural shocks? To give intuition in the context of the simple bivariate VAR described in the previous section, assume that the two shocks driving the model economy are a demand shock ($\varepsilon_t^{Demand}$) and a technology shock ($\varepsilon_t^{Supply}$):
\begin{equation}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t}%
	\end{array}%
	\right] =%
	\begin{bmatrix}
		c_{y} \\
		c_{r} %
	\end{bmatrix}
	+
	\begin{bmatrix}
		\phi _{11} & \phi _{12} \\
		\phi _{21} & \phi _{22}%
	\end{bmatrix}%
	\left[
	\begin{array}{c}
		y_{t-1} \\
		r_{t-1}%
	\end{array}%
	\right] +\left[
	\begin{array}{cc}
		b_{11} & b_{12} \\
		b_{21} & b_{22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Supply} \\
		\varepsilon_{t}^{Demand}%
	\end{bmatrix}%
	,  \label{eq:struct_var_bq}
\end{equation}
The effect of these structural shocks on the endogenous variables at different horizons $h$ can be computed easily, as we have seen in the previous section, by recursively iterating forward the structural (and yet unobserved) impact matrix, $B$. That is:
\begin{equation}
	\begin{array}{l}
		x_{t}=B\varepsilon_{t}, \\
		x_{t+1}=\Phi B\varepsilon_{t}, \\
		... \\
		x_{t+\infty }=\Phi ^{\infty }B\varepsilon_{t}.%
	\end{array}
	\label{eq:long1}
\end{equation}%
The long-run (i.e. for $h$ that goes to infinity) cumulative effect of the shock can be obtained by summing all the terms in \eqref{eq:long1}:
\begin{equation}
	x_{t,t+\infty }=B\varepsilon_{t}+\Phi B\varepsilon_{t}+\Phi
	^{2}B\varepsilon_{t}+...+\Phi ^{\infty }B\varepsilon
	_{t}=\sum\limits_{j=0}^{\infty }\Phi ^{j}B\varepsilon_{t},  \label{eq:long2}
\end{equation}%
where $x_{t,t+\infty }$ denotes the sum from $t$ to $t+\infty $ of the
elements in (\ref{eq:long1}). Finally note that, if the VAR\ is stable (i.e.
if the eigenvalues of $\Phi $ lie inside the unit circle), the infinite sum
in equation (\ref{eq:long2}) converges to:
\begin{equation}
	x_{t,t+\infty }=\left( I-\Phi \right) ^{-1}B\varepsilon_{t}=C\varepsilon
	_{t},  \label{eq:long3}
\end{equation}%
where
\begin{equation}\label{eq:long4}
	C\equiv \left( I-\Phi \right) ^{-1}B  
\end{equation}
is a $2 \times 2$  matrix which captures the cumulative effect of shocks $\varepsilon_{t}$ on $x_{t}$ from time $t$ to $t+\infty $.

The idea behind identification through zero long-run restrictions is to impose a zero restriction on the long-run impact matrix $C$. For example, one could maintain the assumption that demand shocks have zero effect on the level of output in the long run. Equation (\ref{eq:long3}) allows to exactly impose this restriction by setting $c_{12}=0$:
\begin{equation}\label{eq:long5}
	\left[
	\begin{array}{c}
		y_{t,t+\infty } \\
		r_{t,t+\infty }%
	\end{array}%
	\right] =\left[
	\begin{array}{cc}
		c_{11} & 0 \\
		c_{21} & c_{22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Supply} \\
		\varepsilon_{t}^{Demand}%
	\end{bmatrix}.  
\end{equation}%
In fact, the upper right element of $C$ captures the long-run \textit{cumulative} effect of $\varepsilon_{t}^{Demand}$ on the growth rate of GDP, i.e. its effect on the level of GDP. 

But how does this help with the identification of $B$?\ Clearly, $C\equiv \left( I-\Phi \right)^{-1}B$ is unknown as $B$ is unobserved. But Equation (\ref{eq:long4}) can be exploited to achieve identification. To see that, define $\Omega \equiv CC'$ and note that $\Omega$ is known:
\begin{equation}\label{eq:long6}
	\Omega \equiv CC' = \left( \left( I-\Phi \right) ^{-1}\right) BB'\left( \left(I-\Phi \right) ^{-1}\right)'=\left( \left( I-\Phi \right)^{-1}\right) \Sigma_{u}\left( \left( I-\Phi \right) ^{-1}\right)'.
\end{equation}%
Equation \eqref{eq:long6} therefore provides a mapping between the known $2 \times 2$ matrix $\Omega$ and the unobserved $C$ matrix -- and thus the $B$ matrix through equation \eqref{eq:long4} -- which could be used to solve for the unknown elements of $B$. 

The matrix $\Omega$, however, is a positive-definite symmetric matrix, which implies that it provides only three independent restrictions for four unknowns:
\begin{equation}\label{eq:long7}
	\left[
	\begin{array}{cc}
		\omega_{y} & \omega_{yr} \\
		\omega_{yr} & \omega_{r}%
	\end{array}%
	\right]
	=
	\left[
	\begin{array}{cc}
		c_{11}^2 + c_{12}^2 & c_{11}c_{21} + c_{12}c_{22}\\
		c_{11}c_{21}+ c_{12}c_{22} & c_{21}^2 + c_{22}^2%
	\end{array}%
	\right]
\end{equation}%
In a similar way to the the identification by short-run restrictions, assuming $c_{12}=0$ (i.e. assuming that the long-run \textit{cumulative} effect of $\varepsilon_{t}^{Demand}$ on the growth rate of GDP is equal to zero) allows to solve the system of equations implied by \eqref{eq:long7}. In fact, we have now three independent equations and three unknowns, and it is straightforward to compute the solution analytically. Finally, once $C$ is known, it is possible to recover the structural impact matrix $B$ using \eqref{eq:long4} and, thus, to identify the VAR.

\vspace{.5cm}
\begin{notebox}
\textbf{The Cholesky decomposition, again.} \ The identification by zero long-run restrictions solves the identification problem by setting to zero some of the non-diagonal elements of the structural long-run matrix	$C=(1-\Phi)^{-1}B$, thus reducing the number of unknown coefficients in the $C$ matrix to the same number of equations implied by the condition $CC'$. 

As discussed in section \ref{sec:long}, in a simple bivariate VAR a single zero restriction is enough to achieve identification, and the solution is easy to cmopute pencil and paper (3 equaltion and 3 unknonwns). But the number of zeros that need to be imposed to achieve identification depend on the number of variables in the VAR -- and, as for the case of zero-short run restrictions, they increase at a faster rate than the numbers of endogenous variables. 

As before the Cholesky decopm turns out to be useful to as the dimensionality of the VAR increases. Define $\Omega \equiv \left( \left( I-\Phi \right)^{-1}\right) \Sigma_{u}\left( \left( I-\Phi \right) ^{-1}\right)'$ and note that $\Omega $ is a known $2 \times 2$ positive-definite symmetric matrix. Thus, $\Omega$ admits a unique Cholesky decomposition, given by: 
\begin{equation} \label{eqapp:long2}
	\Omega =PP', 
\end{equation}%
where the lower triangular matrix $P$ is the Cholesky factor of\ $\Omega $. Because of the assumption that $C$ is lower triangular, it follows that $P=C$. Once $C$\ is known, we can recover $B$ from  \eqref{eqapp:long0}.
\end{notebox}

\paragraph{In Matlab} Zero long-run restrictions in The VAR Toolbox can be implemented with a few lines of code. As for zero short-run restrictions, the first step is to set the \colorbox{script!80}{\small\texttt{VARopt.ident}} field appropriately. The mnemonic for long-run restrictions is the string \colorbox{script!80}{\small\texttt{\color{matlabpurple}'long'}}. So, zero long-run restrictions can be selected by simply running the following line of code:\vspace{.45cm}

\begin{matlabcode}
% Update the VARopt structure to select zero long-run restrictions
VARopt.ident = 'long';
\end{matlabcode}

\noindent It is also useful (but not necessary) to update the \colorbox{script!80}{\small\texttt{VARopt}} structure with a few addition details that will be used when plotting the impulse responses, saving them, etc. As most setting have been set for the previous example, it will suffice to run the following lines of code:\vspace{.45cm}

\begin{matlabcode}
% Update the VARopt structure with additional details
VARopt.snames = {'$\epsilon\texttt{\^{Supply}'},'$\epsilon\texttt{\^{Demand}'}};
\end{matlabcode}

\noindent As for the case of other identification schemes, the actual implementation of the zero long-run restrictions is via the \colorbox{script!80}{\small\texttt{VARir.m}}, \colorbox{script!80}{\small\texttt{VARvd.m}}, or \colorbox{script!80}{\small\texttt{VARhd.m}} functions. For example:\vspace{.45cm}

\begin{matlabcode}
% Compute impulse responses
[IR, VAR] = VARir(VAR,VARopt);
\end{matlabcode}

\noindent As before, the \colorbox{script!80}{\small\texttt{VARir.m}} function generates a matrix of impulse responses (\colorbox{script!80}{\small\texttt{IR}}) and updates the \colorbox{script!80}{\small\texttt{VAR}} structure with a new \colorbox{script!80}{\small \texttt{VAR.B}} field consistent with the assumption of $c_{12}=0$. The $B$ matrix can be printed at screen by executing the following command in the Matlab command window:

\begin{matlaboutput}
>> disp(VAR.B)

	0.5368   -0.0309
	0.1655    0.3462
\end{matlaboutput}

\noindent Note that the $B$ matrix, which was left unrestricted, is not lower triangular anymore (as in the case of zero short-run restrictions), but has non-zero entries in both columns. As before, each column represents the impact impulse response to the supply and demand shocks, respectively. 

To check that the structural VAR estimated with the above commands is consistent with the assumptions made, it is necessary to compute the $C$ matrix -- the matrix that captures the cumulative long-run effect of shocks on the endogenous variables. Under the assumptions made above, the $C$ matrix should be lower triangular, so that the cumulative effect of $\varepsilon_{t}^{Demand}$ on the growth rate of GDP is zero. Recalling from equation \eqref{eq:long4} that $C\equiv \left( I-\Phi \right) ^{-1}B$, the $C$ matrix can be printed at screen with command in the Matlab command window:

\begin{matlaboutput}
>> disp((eye(2)-VAR.Fcomp)\textbackslash VAR.B)

	0.9224    0.0000
	8.8389    7.5367
\end{matlaboutput}

\noindent As assumed, the $c_{12}$ element is equal to zero. It is also possible to check that the assumed restriction holds by plotting the cumulative response of the endogenous variables to the demand shock at a long enough horizon, and checking that the cumulative impulse response of output growth, $y_t$, converges to zero.  Figure \ref{fig:longCum} plots the cumulative impulse response functions of the interest rate and output growth to the demand shocks for $150$ quarters:\vspace{.45cm}

\begin{figure}[ht!]
	\centering%
	\begin{minipage}[b]{.9\textwidth}
		\caption{\scshape{Cumulative Impulse Responses to $\varepsilon_{t}^{Demand}$}}\vspace{0.1cm}
		\begin{center}
			\includegraphics[width=\textwidth]{longCum_}
		\end{center}%
		\footnotesize{{\scshape Note.} Cumulative impulse responses of real GDP growth and the 1-year Treasury bill yield to a demand shock identified with zero long-run restrictions. Percentage points. }
		\label{fig:longCum}
	\end{minipage}
\end{figure}

\noindent The non-zero impact responses are consistent with the $B$ matrix reported above. In the long-run, the effect of the demand shock on the GDP \textit{level} (i.e. the cumulated GDP growth rates) tends to zero (left panel), while this is not the case for the interest rate (right panel).

\subsection{Identification by sign restrictions}\label{sec:sign}

While the zero restrictions discussed in the previous sections can be justified by economic theory, in many applications these restrictions are implausible or hard to justify. The identification by sign restrictions provides an alternative approach that exploits prior beliefs (typically informed by theroetical models) about the sign that certain shocks should have on certain endogenous variables.

The idea is to impose restrictions on a \textit{set} of orthogonalised impulse response functions. So, differently from the identification schemes described above (where there is a unique point estimate of the $B$ matrix), sign restricted VARs are only set identified. In other words, the data are potentially consistent with a wide range of $B$ matrices that are all admissible in that they satisfy the sign restrictions --  see \cite{Faust1998}, \cite{CanovaDeNicolo2002}, and \cite{Uhlig2005}.

To fix ideas, a demand shock ($\varepsilon_{t}^{Demand}$) should lead to an increase in output growth ($y_{t}$) and to an increase in the short term interest rate ($r_{t}$), as monetary policy responds to the shock by tightening its stance to contain the boom. Differently, a monetary policy shock ($\varepsilon_{t}^{MonPol}$) should lead to a fall in output growth for an unexpected increase in interest rates. That is:

\begin{table}[ht!]
	\centering\small
	\caption{Sign restrictions for demand and monetary policy shocks}
	\begin{tabular}{lcc}
		\hline \addlinespace & Demand ($\varepsilon_{t}^{Demand}$) & Monetary Policy ($\varepsilon_{t}^{MonPol}$) \\ \hline
		\addlinespace Output growth ($y_{t}$) & + & - \\
		Short-rate Int. Rate($r_{t}$) & + & + \\ \hline
	\end{tabular}%
	\label{tab:SR}
\end{table}

The signs in the above table represent restrictions on the elements of the structural impact matrix $B$. But how can such restrictions be imposed? The key intuition for the sign restrictions identification is based on the following three steps.

\noindent \textbf{1. Orthonormal matrices ($Q$)}. \ An orthonormal matrix $Q$ is a real square matrix whose columns and rows are orthogonal unit vectors.\footnote{More precisely, $Q$ is a matrix distributed according to the Haar measure over the group of orthogonal matrices.} What does it mean? Take for example two $2\times 1$ vectors $q_{1}$ and $q_{2}$, then the matrix $Q=(q_{1},q_{2})$ is orthonormal if (i) the vectors have unit norm ($\parallel q_{i}\parallel =1$), and (ii) the vectors are mutually orthogonal ($q_{1}^{T}q_{2}=0$). It follows that 
\begin{equation*}
	QQ'=I_2 \ \ \ \text{and} \ \ \ Q'=Q^{-1}
\end{equation*}
Note that it is possible to generate a large number of matrices that satisfy the above conditions by computing the orthogonal factor in the QR factorization of a random matrix with elements from the standard normal distribution -- see the \colorbox{script!80}{\small\texttt{getqr.m}} function and the examples therein.

\noindent \textbf{2. Candidate structural impact matrices ($B_j$)}. \ Consider the structural impact matrix $B$ corresponding to the Cholesky factor of the reduced form covariance matrix $\Sigma_{u}$ of our simple bivariate VAR, namely:
\begin{equation*}
	\Sigma_{u}=PP'.
\end{equation*}%
We know from Section \ref{sec:short} that $B=P$ is the unique structural impact matrix that would obtain under the zero contemporaneous restriction $b_{12}=0$. Now note that the following equality holds%
\begin{equation}\label{eq:Btilde}
	\Sigma_{u}=PP'=PQ_jQ_j'P'=\underset{B_j}{\underbrace{\left( PQ_j\right) }}\underset{B_j'}{\underbrace{\left( PQ_j\right)'}}
\end{equation}
where $Q_j$ denotes a randomly drawn orthonormal matrix such that $Q_jQ_j'=I_2$. The key property of $B_j$ is that, in addition to satisfying \eqref{eq:Btilde}, it is such that the associated structural shocks $\varepsilon_{jt} = B_j u_t$ are orthogonal and have unit variance. That is, $\Sigma_{j,\varepsilon}=I_2$. It follows that $B_j$ is a valid `candidate' structural impact matrix that solves the identification problem.

Also note that $B_j$ is not triangular any more. As there are infinite of these $B_j$ matrices, which one should we use? How do we know whether $B_j$ represents a plausible solution? 

\noindent \textbf{3. Checking the sign restrictions}. \ The idea of sign restrictions is to generate a large enough number of $B_j$ matrices and retain those that satisfy a set of \textit{a priori} signs for the response of the endogenous variables -- where recall that the $B_j$ matrix contains the impact response of all endogenous variables to all structural shocks. For example, for a given $Q_j$ matrix and associated structural impact matrix $B_j$, the structural
representation of our VAR can be written as:
\begin{equation}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t}%
	\end{array}%
	\right] =%
	\begin{bmatrix}
		\phi _{11} & \phi _{12} \\
		\phi _{21} & \phi _{22}%
	\end{bmatrix}%
	\left[
	\begin{array}{c}
		y_{t-1} \\
		r_{t-1}%
	\end{array}%
	\right] +\left[
	\begin{array}{cc}
		b_{j,11} & b_{j,12} \\
		b_{j,21} & b_{j,22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Demand} \\
		\varepsilon_{t}^{MonPol}%
	\end{bmatrix}%
	,  \label{eq:struct_var_sr}
\end{equation}%
where the elements of $B_j$ are known and equal to $PQ_j$. We can then check whether the impact response of the two structural shocks to output growth and the short-term interest rate satisfy the sign restrictions:

\begin{table*}[h!]
	\centering
	\begin{tabular}{lcc}
		\hline
		\addlinespace & Demand ($\varepsilon_{t}^{Demand}$) & Monetary Policy ($\varepsilon_{t}^{MonPol}$) \\ \hline
		\addlinespace Output growth ($y_{t}$) & $b_{j,11}>0$? & $b_{j,12}<0$? \\
		Short-rate Int. Rate($r_{t}$) & $b_{j,21}>0$? & $b_{j,22}>0$? \\ \hline
	\end{tabular}%
\end{table*}%

If all the elements of $B_j$ satisfy the sign restrictions, then we retain the draw and store $PQ_j$. If at least one element of $B_j$ does not satisfy the restrictions, we discard the draw, compute a new $Q_j$ matrix, and check again the signs of the associated new $B_j$. After drawing a large number of $Q_j$ matrices that satisfy the sign restrictions, we can then construct a distribution of the $B_j$ matrix -- as well as of the impulse responses variance decompositions, etc. Importantly, as noted above, VARs identified with sign restrictions are only \textit{set-identified}. In other words, the data are potentially consistent with a wide range of structural models that are all admissible in that they satisfy the identifying restrictions.

\paragraph{In Matlab} Sign restrictions in the VAR Toolbox can be implemented with a few lines of code. The key element is to specify the sign restrictions. This is done with a square matrix with size equal to the number of endogenous variables -- i.e., in the case of the examples in this section a $2 \times 2$ matrix -- containing $1$s, which stand for a positive response; $-1$s, which stand for a negative response; and $0$s which stand for an unrestricted response. Consistent with the notation in equation \eqref{eq:struct_var_sr}, each row contains restrictions for a given structural shock. Thus, the restrictions described in Table \ref{tab:SR} can therefore be implemented in Matlab with the following line of code:\vspace{.45cm}

\begin{matlabcode}
% Define sign restrictions : positive 1, negative -1, unrestricted 0
SIGN = [ 1, 1  ;  % Real GDP
-1, 1]; % 1-year rate
% Update the VARopt structure with inputs to the sign restriction routine
VARopt.ndraws = 500;
VARopt.sr_hor = 1;
VARopt.pctg = 68;
\end{matlabcode}

\noindent The matrix \colorbox{script!80}{\small\texttt{SIGN}} specifies the sign restrictions as in Table \ref{tab:SR}. It is also possible, though not necessary to update some of the fields in the VARopt structure. The field \colorbox{script!80}{\small\texttt{ndraws}} specifies the number of accepted draws the routine needs to find. The field \colorbox{script!80}{\small\texttt{sr\_hor}} specifies the number of periods the sign restrictions specified in are required to hold. In this specific example, we have set the restrictions to hold for one quarter only (namely, on impact), but it is possible to specify restrictions to hold for longer horizons. The field \colorbox{script!80}{\small\texttt{pctg}} specifies the confidence levels for the credible intervals.

As for other identification schemes, it is also useful (but not necessary) to update the \colorbox{script!80}{\small\texttt{VARopt}} structure with a few addition details that will be used when plotting the impulse responses, saving them, etc. As most setting have been set for the previous example, it will suffice to run the following lines of code:\vspace{.45cm}

\begin{matlabcode}
% Update the VARopt structure with additional details
VARopt.figname= 'graphics/sign_';  % folder and file prefix
VARopt.FigSize = [26 8];  % size of window for figure
VARopt.snames = {'$\backslash$epsilon\texttt{\^{Demand}'},... % shocks names
 '$\backslash$epsilon\texttt{\^{MonPol}'}};
\end{matlabcode}

\noindent Note that, differently from the other identification schemes, it is not required to change the \colorbox{script!80}{\small\texttt{VARopt.ident}} field  to the sign restrictions mnemonic. This is because, unlike the other identification schemes, the sign restrictions procedure is not implemented with the \colorbox{script!80}{\small\texttt{VARir.m}}, \colorbox{script!80}{\small\texttt{VARvd.m}}, or \colorbox{script!80}{\small\texttt{VARhd.m}} functions, but rather with the \colorbox{script!80}{\small\texttt{SR.m}} function as follows:\vspace{.45cm}

\begin{matlabcode}
% Implement sign restrictions identification with SR routine
SRout = SR(VAR,SIGN,VARopt);
\end{matlabcode}

\noindent The structure \colorbox{script!80}{\small\texttt{SRout}} contains all relevant output from the sign restriction procedure. Of particular interest for the discussion in this section are two matrices. The matrix \colorbox{script!80}{\small\texttt{SRout.Ball}} includes all the accepted draws of $B_j$, and thus has a dimension of $2 \times 2 \times 500$. Each of the accepted $B_j$, which by definition satisfies the sign restrictions in Table \ref{tab:SR}, is associated with an impulse response function, stored in the matrix \colorbox{script!80}{\small\texttt{SRout.IRall}}.
Figure \ref{fig:signAll} reports all the impulse responses of GDP growth and the 1-year rate to the monetary policy shock. \vspace{.45cm}

\begin{figure}[ht!]
	\centering%
	\begin{minipage}[b]{.9\textwidth}
		\caption{\scshape{Impulse Responses to a Monetary Policy Shock}}\vspace{0.1cm}
		\begin{center}
			\includegraphics[width=\textwidth]{signAll_}
		\end{center}%
		\footnotesize{{\scshape Note.} Full identified set of impulse responses of real GDP growth and the 1-year Treasury bill yield to a monetary policy shock identified with sign restrictions. Percentage points. }
		\label{fig:signAll}
	\end{minipage}
\end{figure}

While all accepted draws are associated with a structural representation of the VAR that satisfies the identifying restrictions, it is common to report a summary measure of the identified set. The matrix \colorbox{script!80}{\small\texttt{SRout.Bmed}} is computed as the median of \colorbox{script!80}{\small\texttt{SRout.Ball}} across all accepted draws, and thus has a dimension of $2 \times 2$. Similarly, it is possible to compute different quantiles of the distribution of \colorbox{script!80}{\small\texttt{SRout.Ball}} (and the associated \colorbox{script!80}{\small\texttt{SRout.IRall}}) to plot credible intervals for the impulse responses. The 16th and 84th quantiles of the distribution of \colorbox{script!80}{\small\texttt{SRout.IRall}} is stored in the matrices \colorbox{script!80}{\small\texttt{SRout.IRinf}} and \colorbox{script!80}{\small\texttt{SRout.IRsup}}, respectively. The VAR Toolbox has a built-in function (\colorbox{script!80}{\small\texttt{VARirplot.m}}) to plot the impulse responses from any of the identification schemes described in this section. The impulse responses identified with sign restrictions can be plotted with the following line of code:\vspace{.45cm}

\begin{matlabcode}
% Plot credible intervals
VARirplot(SRout.IRmed,VARopt,SRout.IRinf,SRout.IRsup)
\end{matlabcode}

\noindent Figure \ref{fig:sign} reports the median impulse response of GDP growth and the 1-year Treasury rate to the monetary policy shock, together with $68$ Percentage points. credible intervals, as specified above. \vspace{.45cm}

\begin{figure}[ht!]
	\centering%
	\begin{minipage}[b]{.9\textwidth}
		\caption{\scshape{Impulse Responses to a Monetary Policy Shock}}\vspace{0.1cm}
		\begin{center}
			\includegraphics[width=\textwidth]{sign_IR_2}
		\end{center}%
		\footnotesize{{\scshape Note.} Impulse responses of real GDP growth and the 1-year Treasury bill yield to a monetary policy shock identified with sign restrictions. Percentage points. }
		\label{fig:sign}
	\end{minipage}
\end{figure}

\subsection{Identification with external instruments (or proxies)}\label{sec:iv}

The external instruments identification approach has been proposed by \cite{StockWatson2012} and \cite{MertensRavn2013}. This identification strategy uses standard instrumental variable techniques to isolate the variation of the VAR\ reduced-form residuals that are due to the structural shock of
interest. The key element of this identification technique is, thus, the presence of an instrument that is correlated with a structural shock of interest and uncorrelated with all other structural shocks. 

For the example in this section, assume that the data are driven by a demand shock, as well as another shock (or a combination of shocks) that we leave un-identified. Also assume that a valid instrument ($z_{t}$) for the demand shock exists, namely that $z_{t}$ is correlated with $\varepsilon_{t}^{Demand}$ and uncorrelated with the other shock $\varepsilon_{t}^{Other}$. More formally, $z_t$ satisfies the following properties:
\begin{eqnarray}
	\mathbb{E}\left[ \varepsilon_{t}^{Demand}z_{t}'\right]  &=&c, \\
	\mathbb{E[}\varepsilon_{t}^{Other}z_{t}'] &=&0,
	\label{eq:instr_iv}
\end{eqnarray}%
If such an instrument exists, it is possible to identify the contemporaneous response of all endogenous variables to the demand shock. That is, we can identify one column (in this example, the first one) of the $B$ matrix:
\begin{equation}
	B=\left[
	\begin{array}{cc}
		b_{11} & - \\
		b_{21} & -%
	\end{array}%
	\right]
	\label{eq:column_iv}
\end{equation}

The intuition is as follows. Recall that the reduced-form residuals $u_{yt}$ and $u_{rt}$ are a linear combination of two orthogonal shocks $\varepsilon_{t}^{Demand}$ and $\varepsilon_{t}^{Other}$:
\begin{equation}
	\begin{array}{c}
		u_{yt}=b_{11}\varepsilon_{t}^{Demand}+b_{12}\varepsilon_{t}^{Other},
		\\
		u_{rt}=b_{21}\varepsilon_{t}^{Demand}+b_{22}\varepsilon_{t}^{Other}.%
	\end{array}
	\label{eq:red_resid_iv}
\end{equation}%
It is therefore possible to isolate the variation in one of the two reduced-form residuals (say, $u_{yt}$) that is due only to the shock of interest with a regression of $u_{yt}$ itself on the instrument $z_{t}$:
\begin{equation}
	u_{yt}=\beta z_{t}+\xi _{t},  
	\label{eq:first_stage}
\end{equation}%
The fitted values of this first stage regression $\hat{u}_{yt}=\hat{\beta}z_t$ capture the variation of $u_{yt}$ that is due to the demand shock. As $z_t$ is orthogonal to $\varepsilon_{t}^{Other}$, the variation of $u_{yt}$ that is due to the other shock (namely, the component $b_{12}\varepsilon_{t}^{Other}$) ends up in the residual $\xi_t$.

By projecting the residuals of the interest rate equation $u_{rt}$ on the fitted values of the previous regression $\hat{u}_{yt}$, it is possible to get a consistent estimate of the ratio $b_{21}/b_{11}$
\begin{equation}
	u_{rt}=\underset{b_{21}/b_{11}}{\underbrace{\gamma }}\hat{u}_{yt}+\zeta _{t},
	\label{eq:second_stage}
\end{equation}%
Under the assumption that $\mathbb{E}[\varepsilon_{t}^{Demand}z_{t}']=0$, the fitted values $\hat{u}_{yt}$ are orthogonal to $\varepsilon_{t}^{Other}$. Therefore, with a similar logic to the first stage regression, the variation in $u_{rt}$ that is due to $\varepsilon_{t}^{Other}$ ends up in the residual $\zeta_t$, while the fitted values $\hat{\gamma}\hat{u}_{yt}$ isolate the variation in $u_{rt}$ that is due to the demand shock.

If we consider the effect of a demand shock that increases GDP growth by $1$ percentage point (i.e. by normalizing $b_{11}=1$) we can easily recover $b_{21}=\gamma$.\footnote{In the VAR toolbox, the impulse response are further normalized to match the standard deviations of the shocks, as explained in \cite{GertlerKaradi2015}. See Appendix \ref{sec:ident} for details.} The procedure described in this section thus provides an estimate of the first column of $B$ up to a scaling factor:
\begin{equation}
	B=\left[
	\begin{array}{cc}
		1 & - \\
		\gamma & -%
	\end{array}%
	\right]
	\label{eq:column_iv}
\end{equation}
which can then be used to compute impulse response to the demand shock as described above.

\paragraph{IN Matlab} The first step to implement the identification with external instruments in VAR Toolbox is to add the time series of the instrument to the \colorbox{script!80}{\small\texttt{VAR}} structure. Typically, the instrument exploits `external' information that is independent of the VAR, such as narrative fiscal shocks as in \cite{MertensRavn2013} or high frequency monetary policy surprises as in \cite{GertlerKaradi2015}. For simplicity, the example in this section exploits an instrument for the demand shock (stored in the column vector \colorbox{script!80}{\small\texttt{iv}}) that is artificially constructed by adding some noise to the demand shock identified in the zero  short-run restrictions example. The code below shows how to update the \colorbox{script!80}{\small\texttt{VAR}} structure:\vspace{.45cm}

\begin{matlabcode}
% Update VAR structure with external instrument
VAR.IV = iv;
\end{matlabcode}

As for the short- and long-run zero restrictions identification schemes, the second step is to set the \colorbox{script!80}{\small\texttt{VARopt.ident}} structure appropriately. The mnemonic for the external instruments identification is the string \colorbox{script!80}{\small\texttt{\textcolor{matlabpurple}{'iv'}}}. It is also useful (but not necessary) to update the \colorbox{script!80}{\small\texttt{VARopt}} structure with a few additional details that will be used when plotting the impulse responses and saving them:\vspace{.45cm}

\begin{matlabcode}
% Update the options in VARopt
VARopt.ident = 'iv';
VARopt.snames = {'$\epsilon\texttt{\^{Demand}'},'$\epsilon\texttt{\^{Other}'}};
\end{matlabcode}

\noindent The actual implementation of the external instruments identification restrictions is via the \colorbox{script!80}{\small\texttt{VARir.m}} function:\vspace{.45cm}

\begin{matlabcode}
% Compute impulse responses
[IR, VAR] = VARir(VAR,VARopt);
\end{matlabcode}

\noindent As before, the \colorbox{script!80}{\small\texttt{VARir.m}} function updates the \colorbox{script!80}{\small\texttt{VAR}} structure with a new \colorbox{script!80}{\small \texttt{VAR.B}} field consistent with the chosen identification scheme. Differently from the previous examples though, it also updates the \colorbox{script!80}{\small\texttt{VAR}} structure with an additional structure including some information about the first stage regression (\colorbox{script!80}{\small\texttt{VAR.FirstStage}}). As in a standard instrumental variable approach, the F-statistic of the first stage regression is important to assess the relevance of the instrument.

As discussed above, only the first column of the $B$ matrix is identified, while the second column is set to zero by construction. The $B$ matrix can be printed at screen by executing the following command in the Matlab command window:

\begin{matlaboutput}
>> disp(VAR.B)

	0.5375   0
	0.1538   0
\end{matlaboutput}

\noindent Once the impact responses are obtained with the first stage and second stage regressions (reported in Figure \ref{fig:iv} with stars), the impulse responses at horizons $h>1$ to the demand shock are then computed as usual and stored in the matrix (\colorbox{script!80}{\small\texttt{IR}}). Figure \ref{fig:iv} reports the impulse response to the identified demand shock \vspace{.45cm}

\begin{figure}[ht!]
	\centering%
	\begin{minipage}[b]{.9\textwidth}
		\caption{\scshape{Impulse Responses to a Demand Shock}}\vspace{0.1cm}
		\begin{center}
			\includegraphics[width=\textwidth]{iv_}
		\end{center}%
		\footnotesize{{\scshape Note.} Impulse responses of real GDP growth and the 1-year Treasury bill yield to a demand shock identified with external instruments. Percentage points. }
		\label{fig:iv}
	\end{minipage}
\end{figure}

\subsection{Identification by a combination of sign restrictions and external instruments}\label{sec:iv_sign}

The external instruments and sign restriction identification approaches can be combined as proposed by \cite{CesaBianchiSokol2021}. The main idea of this approach is to employ external instruments to identify one (or more) shocks and the remaining shocks (or a subset of them) with sign restrictions.

This section builds on the example used in the previous section, where a demand shock is identified with external instruments and a second shock is left un-identified. Differently from the previous section, however, assume here that the second shock driving the data is a monetary policy shock. As the demand shock can be identified with external instruments, we only need to specify the sign restrictions for the monetary policy shock as discussed in section \ref{sec:sign}. Table \ref{tab:iv_SR} summarizes the identifying restrictions on the elements of the structural impact matrix $B$.

\begin{table}[ht!]
	\centering\small
	\caption{Restrictions for demand and monetary policy shocks}
	\begin{tabular}{lcc}
		\hline \addlinespace & Demand ($\varepsilon_{t}^{Demand}$) & Monetary Policy ($\varepsilon_{t}^{MonPol}$) \\ \hline
		\addlinespace Output growth ($y_{t}$) & Ext. Instrument & - \\
		Short-rate Int. Rate($r_{t}$) & Ext. Instrument & + \\ \hline
	\end{tabular}%
	\label{tab:iv_SR}
\end{table}

To see how to combine the external instrument and sign restrictions approaches, start by partitioning the matrix $B$ into a column vector $b$, which captures the impact of the demand shock, and a column vector $B^{SR}$, which captures the impact of the monetary policy shock:
\begin{equation}
	B=\left[
	\begin{array}{cc}
		B^{IV} & B^{SR}%
	\end{array}%
	\right] .  \label{eq:B_partitioned}
\end{equation}%
where $B^{IV} $ and $B^{SR}$ are $2\times 1$ vectors.\footnote{Note that both $B^{IV}$ and $B^{SR}$ can be matrices, i.e. can include more than one shock, as in the original paper by \cite{CesaBianchiSokol2021}.} Assuming that a valid instrument for the demand shock exists, the first column of the $B$ matrix ($B^{IV}$) can be easily identified as explained in the previous section.

We now show how to combine the external instruments identification approach with a standard sign restriction approach to identify the remaining structural shock ($\varepsilon_{t}^{MonPol}$) conditional on the shock identified with the external instrument ($\varepsilon_{t}^{Demand}$). To identify $B^{SR}$\ (i.e. the contemporaneous impact of the monetary policy shocks)\ we proceed as follows. First, using (\ref{eq:B_partitioned}), re-write the covariance matrix of the reduced-form residuals as:%
\begin{equation}
	\Sigma_{u}=BB'=\left[
	\begin{array}{cc}
		B^{IV} & B^{SR}%
	\end{array}%
	\right] \left[
	\begin{array}{cc}
		B^{IV} & B^{SR}%
	\end{array}%
	\right]'.  \label{eq:cov_partitioned}
\end{equation}%
As we seen above, this decomposition of the covariance matrix is not unique. Let $P$ be the Cholesky decomposition of the covariance matrix $\Sigma_{u}$, and let $Q_j$ be a randomly drawn orthonormal matrix (where, as before, $j$ denotes a random draw) such that $Q_jQ_j'=I_2$, then:
\begin{equation}
	\Sigma_{u}=PP'=PQ_jQ_j'P'=\left( PQ_j\right) \left(
	PQ_j\right)'
\end{equation}%
Similarly to the sign restriction procedure, the identification strategy described in this section consists in constructing a large number of orthonormal matrices $Q_j$ that satisfy the following condition:
\begin{equation}
	PQ_j=\left[
	\begin{array}{cc}
		B^{IV} & B_j^{SR}
	\end{array}%
	\right]
\end{equation}%
where $B^{IV}$ is point identified with the external instrument and $B_j^{SR}$ is set identified with the sign restrictions. Thus, the main difference with the standard sign restriction procedure lies in the construction of the $Q_j$ matrix. Instead of obtaining $Q_j$ from a QR factorization of a random matrix with elements from the standard normal distribution, here we construct the $Q_j$ matrix sequentially, with the following two steps:

\begin{enumerate}
	\item Find a normal vector $Q^{IV}$ of dimension $2 \times 1$ that rotates the first column of $P$ (the Cholesky decomposition of $\Sigma_{u}$) into the vector $B^{IV}$. That is, we find a $n\times 1$ normal vector $Q^{IV}$ such that the following equality holds:%
	\begin{equation}
		PQ^{IV}=B^{IV}
	\end{equation}
	
	\item Given $Q^{IV}$, build the remaining column of an orthonormal $2 \times 2$ matrix $Q_j$ following a standard Gram-Schmidt process.\footnote{Let $j$ index the columns of $Q.$ Let $Q_{j-1}$ denote the first $j-1$
		columns of $Q$, such that $Q_{2-1}=Q_{1}=q_{1}.$ Let $x_{j}$ be a draw from
		a Normal distribution on $\mathbb{R}^{N}.$ Then the $j-$th column of $Q$ can
		be constructed as:%
		\begin{equation*}
			q_{j}=\frac{\left( I_{N}-Q_{j-1}Q_{j-1}'\right) x_{j}}{\left\Vert
				\left( I_{N}-Q_{j-1}Q_{j-1}'\right) x_{j}\right\Vert }.
		\end{equation*}%
	} That is, find a $(2 \times 1)$ vector $Q^{SR}_j$ such that the following equality holds:%
	\begin{equation}
		\begin{bmatrix}
			Q^{IV} & Q^{SR}_j%
		\end{bmatrix}%
		\begin{bmatrix}
			Q^{IV} & Q^{SR}_j%
		\end{bmatrix}%
		'=Q_jQ_j'=I.
	\end{equation}%
\end{enumerate}

\noindent As in the standard sign restriction procedure, the matrix $B_j=PQ_j$ then represents a candidate structural representation because:
\begin{equation}
		\Sigma_u = 
		\begin{bmatrix}
			B^{IV} & B_j^{SR}%
		\end{bmatrix}%
		\begin{bmatrix}
			B^{IV} & B_j^{SR}%
		\end{bmatrix}'
		=
		\underset{B_j}%
		{\underbrace{%
				P 
				\begin{bmatrix}
					Q^{IV} & Q^{SR}_j%
				\end{bmatrix}}}%
		\underset{B_j'}%
		{\underbrace{
				\begin{bmatrix}
				Q^{IV} & Q^{SR}_j%
			\end{bmatrix}'
			P'}}
\end{equation}
and because $B_j=PQ_j$ is such that the associated structural shocks $\varepsilon_t = Bu_t$ are orthogonal and have unit variance. It is therefore possible to check whether the elements of $B_j$ associated with a given random matrix $Q_j$ are consistent with the restrictions in Table \ref{tab:iv_SR} -- and, if so, retain the draw.

The first step for the implementation of the identification with external instruments and sign restrictions in VAR Toolbox is to identify the first column of the $B$ matrix with the external instruments approach, as discussed in the previous section. Note that, in addition to the various outputs described above, the \colorbox{script!80}{\small\texttt{VARir.m}} function stores the $B^{IV}$ vector (i.e. the first column of the $B$ matrix) in \colorbox{script!80}{\small\texttt{VAR.Biv}}. The $B^{IV}$ vector can be printed at screen by executing the following command in the Matlab command window:

\begin{matlaboutput}
>> disp(VAR.Biv)

	0.5375 
	0.1538 
\end{matlaboutput}

\noindent The following step consist in defining the sign restrictions. Differently from the standard sign restrictions approach, here it is only necessary to specify the sign restrictions for the shocks that are not identified with external instruments -- in the case of the simple bivariate VAR considered in this section, the monetary policy shock. As before, this is done by specifying a matrix \colorbox{script!80}{\small\texttt{SIGN}} containing the restrictions. The size of the \colorbox{script!80}{\small\texttt{SIGN}} matrix has to be equal to the size of the $B^{SR}$ matrix, i.e. the number of rows has to be equal to the number of endogenous variables and the number of columns equal to the number of shocks identified with sign restrictions. In the case of the example in this section, \colorbox{script!80}{\small\texttt{SIGN}} is therefore a $2 \times 1$ matrix:\vspace{.45cm}

\begin{matlabcode}
% Define sign restrictions to identify monetary policy shock
% Positive 1, Negative -1, Unrestricted 0:
SIGN = [ 1;  % Real GDP
        -1]; % 1-year rate
\end{matlabcode}

\noindent The matrix \colorbox{script!80}{\small\texttt{SIGN}} specifies sign restrictions consistent with those in the second column of Table \ref{tab:iv_SR}. As for other identification schemes, it is also useful (but not necessary) to update the \colorbox{script!80}{\small\texttt{VARopt}} structure with a few addition details that will be used when plotting the impulse responses, saving them, etc. As most setting have been set for the previous example, it will suffice to run the following lines of code:\vspace{.45cm}

\begin{matlabcode}
% Update the VARopt structure with additional details
VARopt.figname= 'graphics/iv_sign_';
VARopt.snames = {'$\epsilon\texttt{\^{Demand}'},'$\epsilon\texttt{\^{MonPol}'}};
\end{matlabcode}

\noindent Finally, the identification with sign restrictions can be implemented as in the previous example, with the \colorbox{script!80}{\small\texttt{SR.m}} function as follows:\vspace{.45cm}

\begin{matlabcode}
% Implement sign restrictions identification with SR routine 
% conditional on VAR.Biv being already identified
SRIVout = SR(VAR,SIGN,VARopt);
\end{matlabcode}

\noindent The structure \colorbox{script!80}{\small\texttt{SRIVout}} contains all relevant output from the mix of external instruments and sign restriction procedure. As in teh sign restriction example of section XX, the matrix \colorbox{script!80}{\small\texttt{SRout.Ball}} includes all the accepted draws of $B_j$, and thus has a dimension of $2 \times 2 \times 500$. Importantly though, and differently from the stadard sign restrictions procedure, the first column of $B_j$ is always equal to $B^{IV}$, namely the estimates from the external instruments approach. One way to check this is by printing at screen the median of the $B_j$ matrices across the $j=500$ draws (stored in \colorbox{script!80}{\small\texttt{SRIVout.Bmed}} ), which should be equal to the vector \colorbox{script!80}{\small\texttt{VAR.Biv}}, as shown with the following line of code:

\begin{matlaboutput}
>> disp(SRIVout.Bmed)

	0.5375    0.0128
	0.1538   -0.3516
\end{matlaboutput}

Finally, as for the standard sign restriction approach, the impulse responses (and associated credible bands) are stored in the matrices  \colorbox{script!80}{\small\texttt{SRIVout.IRmed}}, \colorbox{script!80}{\small\texttt{SRIVout.IRinf}}, and \colorbox{script!80}{\small\texttt{SRIVout.IRupp}}. As the impulse responses to the demand shock are, by construction, equal to those in the external instrument example of section \ref{sec:iv}, Figure \ref{fig:signAll} reports the impulse responses of GDP growth and the 1-year rate to the monetary policy shock. \vspace{.45cm}

\begin{figure}[ht!]
	\centering%
	\begin{minipage}[b]{.9\textwidth}
		\caption{\scshape{Impulse Responses to a Monetary Policy Shock}}\vspace{0.1cm}
		\begin{center}
			\includegraphics[width=\textwidth]{iv_sign_IR_2}
		\end{center}%
		\footnotesize{{\scshape Note.} Full identified set of impulse responses of real GDP growth and the 1-year Treasury bill yield to a monetary policy shock identified with sign restrictions. Percentage points. }
		\label{fig:signAll}
	\end{minipage}
\end{figure}


\section{Structural Dynamic Analysis}\label{sec:dynamic}

In the previous section, we explored various methods to identify the $B$ matrix that maps structural shocks $\varepsilon_t$ to reduced-form residuals $u_t$. We have seen how this can be achieved using zero contemporaneous restrictions, zero long-run restrictions, sign restrictions, external instruments, or combinations thereof. With an identified $B$ matrix in hand, we can now formally define the kind of analysis that can be performed with an identified structural VAR.

Specifically, structural identification allows us to address three fundamental questions about the role of shocks in driving economic fluctuations:

\begin{enumerate}
    \item \textbf{How do shocks propagate over time?} When a structural shock hits the economy, how do the endogenous variables respond dynamically? This question is answered by \textit{impulse response functions}.

    \item \textbf{How important are different shocks on average?} What portion of the forecast error variance in our variables is attributable to each structural shock? This is quantified through \textit{forecast error variance decompositions}.

    \item \textbf{What drove specific historical episodes?} Looking back at the data, which structural shocks were responsible for pushing the economy away from its equilibrium at particular points in time? This is revealed by \textit{historical decompositions}.
\end{enumerate}

These three tools form the core of what is known as \textbf{structural dynamic analysis}. They provide complementary perspectives on the role of structural shocks. Impulse responses trace out the dynamic path following a shock, variance decompositions summarize the average contribution of shocks to forecast uncertainty, and historical decompositions identify the specific shocks that drove observed fluctuations in the past. Together, they allow us to move beyond simply estimating a VAR to understanding the underlying economic forces that shape macroeconomic outcomes.

In what follows, we explain the intuition behind each of these tools, show how to compute them both analytically and in Matlab using the VAR Toolbox, and discuss how to interpret the results.

% Agent: make exposition in folllowing subsections more prosaic. Do not use sub subsections. Try to use a style that is compatible with the style in previous sections.

%------------------------------------------
\subsection{Impulse Response Functions}\label{sec:irf}


Impulse response functions (henceforth, $IR$) answer the following question: \textit{What is the response over time of each endogenous variable in the VAR to a one-time increase in one of the structural shocks, holding all other structural shocks at zero?}

By isolating the effect of a single shock while keeping all else constant, impulse responses allow us to trace out the causal effect of that shock on the economy. They capture both the \textit{impact effect} (the immediate response on impact) and the \textit{persistence} (how long it takes for the effect to dissipate).

For example, consider a monetary policy shock that raises the policy rate unexpectedly. An impulse response function would tell us:
\begin{itemize}
    \item How much does GDP fall on impact?
    \item How long does it take for GDP to return to its pre-shock level?
    \item What happens to inflation over this adjustment period?
\end{itemize}

\paragraph{How to compute impulse response functions}

To understand how to compute impulse responses, let's return to our simple bivariate structural VAR(1):
\begin{equation}
\left[ 
\begin{array}{c}
y_{t} \\ 
r_{t}
\end{array}
\right] =
\begin{bmatrix}
\phi_{11} & \phi_{12} \\ 
\phi_{21} & \phi_{22}
\end{bmatrix}
\left[ 
\begin{array}{c}
y_{t-1} \\ 
r_{t-1}
\end{array}
\right] +
\begin{bmatrix}
b_{11} & b_{12} \\ 
b_{21} & b_{22}
\end{bmatrix}
\begin{bmatrix}
\varepsilon_{t}^{Demand} \\ 
\varepsilon_{t}^{MonPol}
\end{bmatrix}
\end{equation}

To compute the impulse response to a specific shock, we first define an \textbf{impulse selection vector} $s$ that picks out which shock we want to study. This is a $(k \times 1)$ vector that takes the value 1 for the shock of interest and 0 for all others. For example, to study the demand shock (the first shock), we set:
\begin{equation*}
s = \left[ 
\begin{array}{c}
1 \\ 
0
\end{array}
\right]
\end{equation*}
The impulse response to $\varepsilon_t^{Demand}$ can then be computed using:
\begin{equation}
x_t = \Phi x_{t-1} + Bs
\end{equation}
This equation describes how the system evolves when we ``shock'' it at time $t=0$ by setting $\varepsilon_0^{Demand} = 1$ (while keeping $\varepsilon_0^{MonPol} = 0$), and then let the system evolve without any further shocks.

More specifically, the impulse response at different horizons can be computed recursively:
\begin{equation}
\left\{ 
\begin{array}{ll}
IR_0 = Bs & \quad \text{(impact response)} \\[6pt]
IR_h = \Phi \cdot IR_{h-1} & \quad \text{for } h = 1, 2, ..., H \text{ (dynamic responses)}
\end{array}
\right.
\label{eq:ir_recursion}
\end{equation}

\textbf{Interpretation:} The impact response ($IR_0$) is simply the column of the $B$ matrix corresponding to the shock we're studying. This tells us the immediate effect of the shock on each variable. For subsequent periods, the response is determined by the VAR dynamics encoded in $\Phi$ -- the system gradually returns to equilibrium as the effect of the initial shock fades over time.

For a demand shock (first column of $B$), the impact responses are:
\begin{equation*}
IR_0 = 
\begin{bmatrix}
b_{11} & b_{12} \\ 
b_{21} & b_{22}
\end{bmatrix}
\begin{bmatrix}
1 \\ 
0
\end{bmatrix}
=
\begin{bmatrix}
b_{11} \\ 
b_{21}
\end{bmatrix}
\end{equation*}
The response at horizon $h=1$ is then:
\begin{equation*}
IR_1 = \Phi \cdot IR_0 = 
\begin{bmatrix}
\phi_{11} & \phi_{12} \\ 
\phi_{21} & \phi_{22}
\end{bmatrix}
\begin{bmatrix}
b_{11} \\ 
b_{21}
\end{bmatrix}
=
\begin{bmatrix}
\phi_{11}b_{11} + \phi_{12}b_{21} \\ 
\phi_{21}b_{11} + \phi_{22}b_{21}
\end{bmatrix}
\end{equation*}
and similarly for all subsequent horizons.


\begin{notebox}
\textbf{The companion form representation}
So far we have focused on VAR(1) models for simplicity. But what if our VAR has $p > 1$ lags? The solution is to use the \textbf{companion form} representation, which allows us to rewrite any VAR($p$) as a VAR(1).\par

Consider a VAR(2):
\begin{equation*}
\begin{bmatrix}
y_t \\ 
r_t
\end{bmatrix}
=
\begin{bmatrix}
\phi_{11}^1 & \phi_{12}^1 \\ 
\phi_{21}^1 & \phi_{22}^1
\end{bmatrix}
\begin{bmatrix}
y_{t-1} \\ 
r_{t-1}
\end{bmatrix}
+
\begin{bmatrix}
\phi_{11}^2 & \phi_{12}^2 \\ 
\phi_{21}^2 & \phi_{22}^2
\end{bmatrix}
\begin{bmatrix}
y_{t-2} \\ 
r_{t-2}
\end{bmatrix}
+
\begin{bmatrix}
b_{11} & b_{12} \\ 
b_{21} & b_{22}
\end{bmatrix}
\begin{bmatrix}
\varepsilon_t^{Demand} \\ 
\varepsilon_t^{MonPol}
\end{bmatrix}
\end{equation*}

We can rewrite this as a VAR(1) by defining an expanded state vector that includes lagged values:
\begin{equation*}
\begin{bmatrix}
y_t \\ 
r_t \\ 
y_{t-1} \\ 
r_{t-1}
\end{bmatrix}
=
\underbrace{
\begin{bmatrix}
\phi_{11}^1 & \phi_{12}^1 & \phi_{11}^2 & \phi_{12}^2 \\ 
\phi_{21}^1 & \phi_{22}^1 & \phi_{21}^2 & \phi_{22}^2 \\ 
1 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0
\end{bmatrix}}_{\text{Companion matrix } \Phi^c}
\begin{bmatrix}
y_{t-1} \\ 
r_{t-1} \\ 
y_{t-2} \\ 
r_{t-2}
\end{bmatrix}
+
\underbrace{
\begin{bmatrix}
b_{11} & b_{12} \\ 
b_{21} & b_{22} \\ 
0 & 0 \\ 
0 & 0
\end{bmatrix}}_{\text{Impact matrix } B^c}
\begin{bmatrix}
\varepsilon_t^{Demand} \\ 
\varepsilon_t^{MonPol}
\end{bmatrix}
\end{equation*}

The companion matrix $\Phi^c$ has dimension $(kp \times kp)$ and the expanded impact matrix $B^c$ has dimension $(kp \times k)$. With this representation, we can compute impulse responses for VARs of any order using the same recursive formula as before, but with $\Phi^c$ and $B^c$ in place of $\Phi$ and $B$.

\textbf{Important note:} The impulse responses we care about are only the first $k$ rows of the expanded system -- these correspond to the original variables of interest. The remaining rows simply track lags and are not separately reported.

\end{notebox}

\subsubsection*{Computing impulse responses in Matlab}

In the VAR Toolbox, impulse responses are computed using the \colorbox{script!80}{\small\texttt{VARir}} function. Here's a typical workflow:

\begin{enumerate}
\item First, estimate your VAR and set the identification scheme (see Section \ref{sec:ident}):

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Estimate reduced-form VAR} \\
\hspace{1mm}[VAR, VARopt] = VARmodel(X, nlags, det); \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Set identification scheme (e.g., short-run restrictions)} \\
\hspace{1mm}VARopt.ident = \textcolor{matlabpurple}{'short'}; \\
}
\end{minipage}

\item Compute the impulse responses:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Compute impulse responses} \\
\hspace{1mm}[IR, VAR] = VARir(VAR, VARopt); \\
}
\end{minipage}

The function returns:
\begin{itemize}
    \item \colorbox{script!80}{\small\texttt{IR}}: A $(H \times k \times k)$ array containing the impulse responses, where $H$ is the number of horizons, and the $(i,j)$ element at horizon $h$ gives the response of variable $i$ to shock $j$ at horizon $h$.
    \item \colorbox{script!80}{\small\texttt{VAR}}: The updated VAR structure, now including the identified $B$ matrix in \colorbox{script!80}{\small\texttt{VAR.B}}.
\end{itemize}

\item To compute confidence intervals via bootstrap, use the \colorbox{script!80}{\small\texttt{VARirband}} function:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Set bootstrap options} \\
\hspace{1mm}VARopt.ndraws = 1000; \textcolor{matlabgreen}{\% Number of bootstrap draws} \\
\hspace{1mm}VARopt.pctg = 95; \textcolor{matlabgreen}{\% Confidence level} \\
\hspace{1mm}VARopt.method = \textcolor{matlabpurple}{'bs'}; \textcolor{matlabgreen}{\% 'bs' = standard bootstrap; 'wild' = wild bootstrap} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Compute confidence bands} \\
\hspace{1mm}[IRinf, IRsup, IRmed, IRbar] = VARirband(VAR, VARopt); \\
}
\end{minipage}

\item Plot the results using \colorbox{script!80}{\small\texttt{VARirplot}}:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Plot impulse responses with confidence bands} \\
\hspace{1mm}VARirplot(IRbar, IRinf, IRsup, VARopt); \\
}
\end{minipage}

\end{enumerate}

\textbf{Key practical considerations:}
\begin{itemize}
    \item The \textbf{ordering of variables matters} for identification schemes like short-run restrictions (Cholesky). Make sure your data matrix $X$ is ordered appropriately.
    \item By default, the toolbox computes impulse responses for 40 periods (\colorbox{script!80}{\small\texttt{VARopt.nsteps = 40}}). You can change this if needed.
    \item For confidence bands, the default is 1000 bootstrap replications and 95\% confidence intervals, but these can be adjusted.
\end{itemize}

%------------------------------------------
\subsection{Forecast Error Variance Decomposition}\label{sec:fevd}

\subsubsection*{What question does the variance decomposition answer?}

Forecast error variance decompositions (henceforth, $VD$) answer the following question:

\begin{center}
\textit{What portion of the forecast error variance in each variable (at horizon $h$) is attributable to each structural shock?}
\end{center}

While impulse responses tell us \textit{how} variables respond to shocks, variance decompositions tell us how \textit{important} each shock is in driving fluctuations in the variables of interest. This provides a different perspective: rather than tracing out the path of a single shock, we ask what fraction of the overall uncertainty about future values of our variables can be attributed to each structural shock.

\textbf{Example:} Suppose we find that demand shocks explain 70\% of the forecast error variance in GDP at the 4-quarter horizon, while monetary policy shocks explain only 10\%. This tells us that, on average over the sample, demand shocks are the dominant source of GDP fluctuations, at least in the short run.

\subsubsection*{How to compute forecast error variance decompositions}

To understand variance decompositions, we first need to define what we mean by a forecast error. The $h$-step ahead forecast error is the difference between the actual value of the variable at time $t+h$ and what we would have predicted for that value based on information available at time $t-1$:
\begin{equation}
FE_{t+h} = x_{t+h} - \mathbb{E}_{t-1}[x_{t+h}]
\end{equation}

For a VAR(1), we can write the forecast error at different horizons as:
\begin{align*}
FE_{t} &= x_t - \mathbb{E}_{t-1}[x_t] = B\varepsilon_t \\
FE_{t+1} &= x_{t+1} - \mathbb{E}_{t-1}[x_{t+1}] = \Phi B\varepsilon_t + B\varepsilon_{t+1} \\
FE_{t+2} &= x_{t+2} - \mathbb{E}_{t-1}[x_{t+2}] = \Phi^2 B\varepsilon_t + \Phi B\varepsilon_{t+1} + B\varepsilon_{t+2}
\end{align*}

In general, the $h$-step ahead forecast error is:
\begin{equation}
FE_{t+h} = \sum_{i=0}^{h} \Phi^{h-i} B \varepsilon_{t+i}
\label{eq:forecast_error}
\end{equation}

Now consider the variance of the forecast error. For simplicity, let's focus on the case $h=0$ (the impact period). The forecast error is $FE_t = B\varepsilon_t$. Since the structural shocks are orthogonal and have unit variance, the variance of the forecast error for each variable is:
\begin{align*}
\text{Var}(y_t - \mathbb{E}_{t-1}[y_t]) &= b_{11}^2 + b_{12}^2 \\
\text{Var}(r_t - \mathbb{E}_{t-1}[r_t]) &= b_{21}^2 + b_{22}^2
\end{align*}

The forecast error variance decomposition for variable $y$ at horizon $h=0$ is then:
\begin{align*}
VD_y^{\varepsilon^{Demand}} &= \frac{b_{11}^2}{b_{11}^2 + b_{12}^2} \\[6pt]
VD_y^{\varepsilon^{MonPol}} &= \frac{b_{12}^2}{b_{11}^2 + b_{12}^2}
\end{align*}

Note that these decompositions sum to 1 for each variable -- this is by construction, since the structural shocks are the only sources of variation in the forecast error.

For longer horizons $h > 0$, the same logic applies, but we need to account for the cumulative effect of shocks from the present through horizon $h$. Let $\Theta_i = \Phi^i B$ denote the impulse response matrix at horizon $i$. Then the variance decomposition at horizon $h$ is:
\begin{equation}
VD_{y,h}^{\varepsilon^{j}} = \frac{\sum_{i=0}^{h} (\theta_{1j}^i)^2}{\sum_{i=0}^{h} \sum_{m=1}^{k} (\theta_{1m}^i)^2}
\label{eq:vd_formula}
\end{equation}

where $\theta_{1j}^i$ is the $(1,j)$ element of $\Theta_i$ (the response of variable $y$ to shock $j$ at horizon $i$).

\textbf{Interpretation:} The variance decomposition tells us what fraction of the total uncertainty about the future value of a variable is due to each shock. Unlike impulse responses, which trace out the effect of a single shock over time, variance decompositions aggregate across all possible shock realizations to give us an average measure of importance.

\subsubsection*{Computing variance decompositions in Matlab}

In the VAR Toolbox, forecast error variance decompositions are computed using the \colorbox{script!80}{\small\texttt{VARvd}} function:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Compute variance decomposition} \\
\hspace{1mm}[VD, VAR] = VARvd(VAR, VARopt); \\
}
\end{minipage}

\medskip

The output \colorbox{script!80}{\small\texttt{VD}} is a $(H \times k \times k)$ array, where:
\begin{itemize}
    \item The first dimension indexes the horizon $h = 0, 1, ..., H-1$
    \item The second dimension indexes the variable
    \item The third dimension indexes the shock
\end{itemize}

For example, \colorbox{script!80}{\small\texttt{VD(5,1,2)}} gives the fraction of the 4-step-ahead forecast error variance in variable 1 (at horizon $h=4$, since Matlab indexes from 1) that is due to shock 2.

To visualize the results, use \colorbox{script!80}{\small\texttt{VARvdplot}}:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Plot variance decomposition} \\
\hspace{1mm}VARvdplot(VD, VARopt); \\
}
\end{minipage}

\medskip

\textbf{Key practical considerations:}
\begin{itemize}
    \item Variance decompositions are often reported at specific horizons of interest (e.g., 1 quarter, 1 year, 5 years) rather than for all horizons.
    \item Unlike impulse responses, variance decompositions are always positive and sum to 100\% for each variable.
    \item The decomposition can be sensitive to the identification scheme -- different identification assumptions can lead to very different assessments of shock importance.
\end{itemize}

%------------------------------------------
\subsection{Historical Decomposition}\label{sec:hd}

\subsubsection*{What question does the historical decomposition answer?}

Historical decompositions (henceforth, $HD$) answer the following question:

\begin{center}
\textit{At each point in time, what is the contribution of each structural shock (past and present) to the deviation of each variable from its equilibrium?}
\end{center}

While impulse responses describe hypothetical scenarios (``what if a shock hits?'') and variance decompositions provide average measures of importance, historical decompositions allow us to look back at the actual data and ask: which shocks were responsible for observed fluctuations at specific points in time?

\textbf{Example:} Suppose GDP fell sharply in 2008:Q4. A historical decomposition would tell us how much of that decline was due to financial shocks, how much to demand shocks, how much to monetary policy shocks, and so on. This provides a narrative of what drove the economy during that episode.

\subsubsection*{How to compute historical decompositions}

To understand historical decompositions, recall the Wold representation of the VAR from Section \ref{sec:struct_var}. This representation expresses each observation as the cumulative sum of all past structural shocks plus the contribution of initial conditions:
\begin{equation}
x_t = \Phi^t x_0 + \sum_{j=0}^{t-1} \Phi^j B \varepsilon_{t-j}
\label{eq:wold_hd}
\end{equation}

The first term, $\Phi^t x_0$, captures the effect of the initial condition -- how far the system started from its equilibrium at $t=0$. The second term is the cumulative contribution of all structural shocks from the initial period through period $t$.

For concreteness, consider $t=2$ in our simple bivariate VAR:
\begin{equation*}
x_2 = \underbrace{\Phi^2 x_0}_{\text{Initial condition}} + \underbrace{\Phi B}_{=\Theta_1} \varepsilon_1 + \underbrace{B}_{=\Theta_0} \varepsilon_2
\end{equation*}

Writing this out in matrix form:
\begin{equation*}
\begin{bmatrix}
y_2 \\ 
r_2
\end{bmatrix}
=
\begin{bmatrix}
\text{init}_y \\ 
\text{init}_r
\end{bmatrix}
+
\begin{bmatrix}
\theta_{11}^1 & \theta_{12}^1 \\ 
\theta_{21}^1 & \theta_{22}^1
\end{bmatrix}
\begin{bmatrix}
\varepsilon_1^{Demand} \\ 
\varepsilon_1^{MonPol}
\end{bmatrix}
+
\begin{bmatrix}
\theta_{11}^0 & \theta_{12}^0 \\ 
\theta_{21}^0 & \theta_{22}^0
\end{bmatrix}
\begin{bmatrix}
\varepsilon_2^{Demand} \\ 
\varepsilon_2^{MonPol}
\end{bmatrix}
\end{equation*}

For variable $y$, this becomes:
\begin{equation*}
y_2 = \text{init}_y + \theta_{11}^1 \varepsilon_1^{Demand} + \theta_{12}^1 \varepsilon_1^{MonPol} + \theta_{11}^0 \varepsilon_2^{Demand} + \theta_{12}^0 \varepsilon_2^{MonPol}
\end{equation*}

The historical decomposition groups the terms by shock:
\begin{align*}
HD_{y,2}^{\varepsilon^{Demand}} &= \theta_{11}^1 \varepsilon_1^{Demand} + \theta_{11}^0 \varepsilon_2^{Demand} \\
HD_{y,2}^{\varepsilon^{MonPol}} &= \theta_{12}^1 \varepsilon_1^{MonPol} + \theta_{12}^0 \varepsilon_2^{MonPol} \\
HD_{y,2}^{\text{init}} &= \text{init}_y
\end{align*}

These three components sum to $y_2$. The interpretation is straightforward:
\begin{itemize}
    \item $HD_{y,2}^{\varepsilon^{Demand}}$ is the cumulative contribution of all past demand shocks to $y$ at time $t=2$.
    \item $HD_{y,2}^{\varepsilon^{MonPol}}$ is the cumulative contribution of all past monetary policy shocks.
    \item $HD_{y,2}^{\text{init}}$ captures the influence of the initial condition, which fades over time as the effect of $\Phi^t x_0$ decays.
\end{itemize}

\textbf{Interpretation:} The historical decomposition tells us a story about the past. At each point in time, it breaks down the observed value of each variable into the contributions from different shocks. This allows us to identify which shocks were the primary drivers of fluctuations during specific historical episodes -- for instance, whether a recession was driven by demand shocks, financial shocks, or monetary policy shocks.

Note that if the VAR is covariance stationary and we assume the variables are at their unconditional mean initially (so $x_0 = 0$), the initial condition component will be zero and the decomposition consists entirely of shock contributions.

\subsubsection*{Computing historical decompositions in Matlab}

In the VAR Toolbox, historical decompositions are computed using the \colorbox{script!80}{\small\texttt{VARhd}} function:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Compute historical decomposition} \\
\hspace{1mm}[HD, VAR] = VARhd(VAR, VARopt); \\
}
\end{minipage}

\medskip

Unlike \colorbox{script!80}{\small\texttt{IR}} and \colorbox{script!80}{\small\texttt{VD}}, which are arrays, \colorbox{script!80}{\small\texttt{HD}} is a structure containing multiple fields:

\begin{minipage}{\textwidth}
\small
\begin{verbatim}
>> disp(HD)
    shock: [T x nvar x nshocks double]  % Contribution of each shock
     init: [T x nvar double]            % Contribution of initial conditions
    const: [T x nvar double]            % Contribution of constant term
    trend: [T x nvar double]            % Contribution of linear trend
   trend2: [T x nvar double]            % Contribution of quadratic trend
      exo: [T x nvar x nexo double]    % Contribution of exogenous variables
     endo: [T x nvar double]            % Actual values of endogenous variables
\end{verbatim}
\end{minipage}

\medskip

The most important field is \colorbox{script!80}{\small\texttt{HD.shock}}, which contains the contribution of each structural shock. For example, \colorbox{script!80}{\small\texttt{HD.shock(:,1,2)}} gives the time series of shock 2's contribution to variable 1.

To visualize the results, use \colorbox{script!80}{\small\texttt{VARhdplot}}:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Plot historical decomposition} \\
\hspace{1mm}VARhdplot(HD, VARopt); \\
}
\end{minipage}

\medskip

\textbf{Key practical considerations:}
\begin{itemize}
    \item The historical decomposition is typically visualized as a stacked bar chart, where each bar represents the contribution of different shocks at a given point in time.
    \item Early in the sample, the contribution of initial conditions (\colorbox{script!80}{\small\texttt{HD.init}}) may be substantial. Over time, this contribution decays and the decomposition is dominated by identified shocks.
    \item If the VAR includes deterministic terms (constant, trend), their contributions are reported separately in the \colorbox{script!80}{\small\texttt{HD}} structure.
    \item Unlike variance decompositions, contributions from different shocks can be positive or negative, and they sum to the actual observed value of the variable at each point in time.
\end{itemize}

%------------------------------------------
\subsection*{Summary}

In this section, we have covered the three main tools of structural dynamic analysis:

\begin{itemize}
    \item \textbf{Impulse response functions} trace out the dynamic path of variables in response to a one-time structural shock, allowing us to quantify both the impact and persistence of shocks.
    
    \item \textbf{Forecast error variance decompositions} measure the relative importance of each shock in explaining forecast uncertainty, providing an average assessment of which shocks matter most for each variable.
    
    \item \textbf{Historical decompositions} identify the specific shocks that drove observed fluctuations at particular points in time, allowing us to construct narratives about historical episodes.
\end{itemize}

Together, these tools provide a comprehensive toolkit for understanding the role of structural shocks in shaping economic dynamics. In the next section, we will see these tools in action through a series of applications that replicate influential studies in the VAR literature.



% AgentAI: can you merge the following two conclusion? Sections do not use sub sections just have a single concluding one








\section{Conclusions}

This handbook has walked you through the foundations of VAR analysis, from basic concepts to advanced identification strategies, and from theoretical exposition to practical implementation. The main objective has been to provide intuition for the mechanics of VAR models by means of a series of practical examples implemented with the VAR Toolbox. Along the way, we have covered the mechanics of reduced-form and structural VARs, five different identification approaches (each with its own strengths and limitations), the three main tools of structural dynamic analysis (impulse responses, variance decompositions, and historical decompositions), and five influential applications that demonstrate these methods in action.

The VAR Toolbox is a collection of Matlab routines that, in a consistent way, allows to perform standard VAR analysis, such as the estimation of VAR models, the identification of structural shocks, and the computation of impulse responses, forecast error variance decompositions, and historical decompositions. While other software packages may be more computationally efficient or offer additional bells and whistles, the VAR Toolbox's strength lies in its transparency and pedagogical value. By working through the code and examples, you can develop a deep understanding of how VARs work under the hood.

This handbook is targeted at users who are not familiar with VAR models and want to get an informal intuition behind their workings. Most of the examples presented are deliberately simple from an economic standpoint, but have the advantage of being easy to follow and to implement. Therefore, this handbook is not a substitute to standard time series econometrics textbooks, but rather a complement -- which is hopefully making it easier for users to follow more comprehensive and formal treatments of these topics.

As you embark on your own VAR analyses, several principles are worth keeping in mind. First, think carefully about identification. Your results are only as credible as your identifying assumptions -- be explicit about what you're assuming and why. Second, test your assumptions. When possible, provide diagnostic checks and robustness tests. For external instruments, report first-stage statistics. For sign restrictions, explore the range of admissible models. Third, be transparent about limitations. No identification strategy is perfect. Acknowledge the weaknesses of your approach and discuss what they mean for your conclusions. Fourth, compare across methods. If multiple identification strategies are plausible for your question, try them all and see whether your results are robust. Finally, interpret economically. Don't just report statistical results -- explain what they mean for economic behavior and policy.

The VAR literature continues to evolve, with recent developments including time-varying parameter VARs that allow for structural change, Bayesian VARs with informative priors for forecasting and structural analysis, proxy SVARs with multiple instruments, VARs with high-dimensional data and factor structures, and local projections as an alternative to VARs for impulse response estimation. Many of these extensions are covered in more advanced texts and may be incorporated into future versions of the VAR Toolbox.

VAR models remain one of the workhorse tools of empirical macroeconomics, four decades after their introduction by Chris Sims. Their continued popularity reflects a combination of flexibility, transparency, and practical usefulness. With the VAR Toolbox and this handbook, you now have everything you need to apply these powerful methods to your own research questions. The VAR Toolbox should be useful to applied researchers who want to perform standard VAR analysis or to extend common approaches for research purposes. All the codes are public, and users are encouraged to modify them as they wish for their own research purposes. If you do that, please get in touch or raise an issue in Github so that the VAR Toolbox can keep on evolving.

Remember: the best way to learn is by doing. Start with the examples in this handbook, then gradually adapt the code to your own data and questions. Don't be afraid to experiment, and don't be discouraged if your first attempts don't work perfectly -- even experienced researchers iterate many times before settling on a final specification. Good luck with your research.


\bibliographystyle{ecta}
\bibliography{BIBLIO}

\clearpage 
\appendix
\addcontentsline{toc}{section}{Appendix}
\numberwithin{subsection}{section}
\numberwithin{table}{section}
\numberwithin{figure}{section}
\numberwithin{equation}{section}
\setcounter{equation}{0}
\onehalfspacing



\section{Applications and Replications}\label{sec:applications}

% AgentAI: can you make the drafting of this section more precise and more in line with the style of the sections of the main body. Do not use sub subsections move the reference to the end of each subsection.

In the previous sections, we covered the theoretical foundations of VAR models, introduced various identification strategies, and explained the main tools for structural dynamic analysis. While this theoretical knowledge is essential, the best way to truly understand VARs is to see them in action -- applied to real data and real economic questions. As the saying goes, ``the proof is in the pudding.''

This section presents five influential applications from the VAR literature, each showcasing a different identification approach. These replications serve multiple purposes:

\begin{enumerate}
    \item \textbf{Pedagogical value:} Working through complete examples solidifies understanding of the identification methods introduced in Section \ref{sec:ident}.
    
    \item \textbf{Practical guidance:} Each replication provides step-by-step Matlab code that you can adapt for your own research.
    
    \item \textbf{Methodological benchmarks:} These papers are canonical examples of their respective identification approaches, providing templates for best practices.
    
    \item \textbf{Economic insights:} Beyond methodology, these applications address substantive economic questions about monetary policy, supply and demand shocks, and financial disturbances.
\end{enumerate}

The five applications we cover are:

\begin{itemize}
    \item \textbf{Stock and Watson (2001)} -- Identification via zero short-run (recursive) restrictions
    \item \textbf{Blanchard and Quah (1989)} -- Identification via zero long-run restrictions
    \item \textbf{Uhlig (2005)} -- Identification via sign restrictions
    \item \textbf{Gertler and Karadi (2015)} -- Identification via external instruments
    \item \textbf{Cesa-Bianchi and Sokol (2020)} -- Identification combining sign restrictions and external instruments
\end{itemize}

Each subsection follows a similar structure: we first describe the economic question and the key identifying assumptions, then walk through the Matlab implementation using the VAR Toolbox, and finally discuss the results and their economic interpretation. By the end of this section, you should feel confident applying these methods to your own research questions.

%------------------------------------------
\subsection{Stock and Watson (2001): Recursive Identification}\label{sec:app_sw}

What are the effects of monetary policy shocks on unemployment and inflation? This is one of the most fundamental questions in macroeconomics, and answering it requires isolating exogenous movements in monetary policy from endogenous responses to economic conditions. \cite{StockWatson2001} tackle this question using a simple three-variable VAR with recursive identification based on US quarterly data from 1960:Q1 to 2000:Q4. The VAR includes three variables: inflation ($\pi_t$, measured as the year-on-year percent change in the GDP deflator), the unemployment rate ($u_t$, in percent), and the federal funds rate ($r_t$, in percent at annual rate). The specification is a VAR(4) with a constant term.

The identification strategy employs recursive (Cholesky) identification based on two key assumptions:

\begin{enumerate}
    \item \textbf{Monetary policy reacts contemporaneously to inflation and unemployment.} The Federal Reserve observes current economic conditions and adjusts the policy rate within the quarter. This is plausible given that the Fed receives high-frequency data on economic activity and makes policy decisions at regular intervals.
    
    \item \textbf{Monetary policy shocks do not affect inflation and unemployment within the quarter.} It takes time for changes in interest rates to propagate through the economy and affect real activity and prices. This assumption reflects the well-known ``long and variable lags'' of monetary policy.
\end{enumerate}

These assumptions imply the following recursive structure:
\begin{equation*}
\begin{bmatrix}
\pi_t \\ 
u_t \\ 
r_t
\end{bmatrix}
= \sum_{p=1}^{4} \Phi_p x_{t-p} + 
\begin{bmatrix}
b_{11} & 0 & 0 \\ 
b_{21} & b_{22} & 0 \\ 
b_{31} & b_{32} & b_{33}
\end{bmatrix}
\begin{bmatrix}
\varepsilon_t^{Supply} \\ 
\varepsilon_t^{Demand} \\ 
\varepsilon_t^{MonPol}
\end{bmatrix}
\end{equation*}

The upper-triangular structure of the impact matrix $B$ imposes that monetary policy shocks (the third shock) have zero contemporaneous effect on inflation and unemployment (the first two variables). The ordering of variables matters. Inflation is ordered first, unemployment second, and the policy rate third. This ordering reflects the assumptions about timing: inflation and unemployment are predetermined within the quarter, while the policy rate can respond to them contemporaneously.

The replication is straightforward using the VAR Toolbox. First, load the data and set up the VAR:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Load data} \\
\hspace{1mm}load('StockWatson2001.mat'); \\
\hspace{1mm}X = [inflation, unemployment, fed\_funds]; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% VAR specification} \\
\hspace{1mm}nlags = 4; \textcolor{matlabgreen}{\% 4 lags} \\
\hspace{1mm}det = 1; \textcolor{matlabgreen}{\% Include constant} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Estimate reduced-form VAR} \\
\hspace{1mm}[VAR, VARopt] = VARmodel(X, nlags, det); \\
}
\end{minipage}

\medskip

Next, set the identification option to recursive and compute impulse responses:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Set identification to recursive (Cholesky)} \\
\hspace{1mm}VARopt.ident = \textcolor{matlabpurple}{'short'}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Compute impulse responses} \\
\hspace{1mm}[IR, VAR] = VARir(VAR, VARopt); \\
}
\end{minipage}

\medskip

To obtain confidence bands via bootstrap:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Bootstrap options} \\
\hspace{1mm}VARopt.ndraws = 1000; \textcolor{matlabgreen}{\% Number of bootstrap draws} \\
\hspace{1mm}VARopt.pctg = 95; \textcolor{matlabgreen}{\% 95\% confidence intervals} \\
\hspace{1mm}VARopt.method = \textcolor{matlabpurple}{'bs'}; \textcolor{matlabgreen}{\% Standard bootstrap} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Compute confidence bands} \\
\hspace{1mm}[IRinf, IRsup, IRmed, IRbar] = VARirband(VAR, VARopt); \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Plot results} \\
\hspace{1mm}VARirplot(IRbar, IRinf, IRsup, VARopt); \\
}
\end{minipage}

\medskip

You can also compute variance decompositions and historical decompositions:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Forecast error variance decomposition} \\
\hspace{1mm}[VD, VAR] = VARvd(VAR, VARopt); \\
\hspace{1mm}VARvdplot(VD, VARopt); \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Historical decomposition} \\
\hspace{1mm}[HD, VAR] = VARhd(VAR, VARopt); \\
\hspace{1mm}VARhdplot(HD, VARopt); \\
}
\end{minipage}

\medskip

A contractionary monetary policy shock (increase in the federal funds rate) leads to higher unemployment (peaking around 2\% after 4-6 quarters) and lower inflation, though the effect on inflation is delayed and modest. An important empirical puzzle emerges in some specifications: inflation initially \textit{rises} following a monetary contraction, which is counterintuitive. This ``price puzzle'' has been widely discussed in the literature, with common explanations including omitted variables (especially commodity prices) or mis-specification of the timing of policy responses.

While the monetary policy shock is the main focus, recursive identification also identifies two other shocks by construction. The first shock ($\varepsilon_t^{Supply}$) affects all three variables contemporaneously, and its impulse responses suggest it behaves like a negative aggregate supply shock -- it raises both inflation and unemployment, consistent with a negative productivity shock or an oil price increase. The second shock ($\varepsilon_t^{Demand}$) affects unemployment and the policy rate contemporaneously, but not inflation. Its impulse responses are consistent with a negative aggregate demand shock -- unemployment rises, the policy rate falls, and inflation declines with a lag.

While these economic interpretations seem plausible, it is important to remember that they emerge from the recursive ordering, not from explicit economic theory or additional identifying restrictions. The naming of shocks is therefore somewhat \textit{ex post} rationalization. This highlights both the power and the limitation of recursive identification: it provides a simple way to achieve identification, but the economic interpretation of all shocks may not always be clear-cut. Moreover, the ordering of variables embeds strong assumptions about contemporaneous effects, and the price puzzle remains an important challenge for monetary VARs.

\vspace{0.3cm}
\noindent\textit{Reference:} \cite{StockWatson2001}

%------------------------------------------
\subsection{Blanchard and Quah (1989): Long-Run Restrictions}\label{sec:app_bq}

What are the effects of aggregate demand and aggregate supply shocks on output and unemployment? Moreover, how can we distinguish between these two types of shocks when both can move output and unemployment in the same direction in the short run? \cite{BlanchardQuah1989} exploit the long-run neutrality of demand shocks implied by classical economic theory to address this question. Using US quarterly data from 1948:Q1 to 1987:Q4, they estimate a VAR(8) with a constant term that includes two variables: real GNP growth ($\Delta y_t$, in percent at quarterly rate) and the unemployment rate ($u_t$, in percent). Note that the VAR is specified in growth rates for output, not levels, which matters for the interpretation of long-run restrictions.

The key identifying assumption comes from classical macroeconomic theory:

\begin{center}
\textit{Demand shocks have no long-run effect on the level of output, while supply shocks do.}
\end{center}

This assumption reflects the classical dichotomy: in the long run, real variables like output are determined by supply-side factors (technology, labor force, capital accumulation), while demand-side factors only affect nominal variables and the short-run path of the economy.

Since the VAR is estimated in growth rates ($\Delta y_t$), the restriction on the \textit{level} of output translates into a restriction on the \textit{cumulative sum} of the impulse responses of output growth. Specifically, the cumulative response of $\Delta y_t$ to a demand shock must be zero:
\begin{equation*}
\sum_{h=0}^{\infty} IR^{\Delta y}_h[\varepsilon^{Demand}] = 0
\end{equation*}

In matrix notation, the long-run impact matrix of the VAR must have the following structure:
\begin{equation*}
\begin{bmatrix}
\Delta y_{t,t+\infty} \\ 
u_{t,t+\infty}
\end{bmatrix}
= 
\begin{bmatrix}
c_{11} & 0 \\ 
c_{21} & c_{22}
\end{bmatrix}
\begin{bmatrix}
\varepsilon_t^{Supply} \\ 
\varepsilon_t^{Demand}
\end{bmatrix}
\end{equation*}

where $c_{12} = 0$ imposes that demand shocks have no long-run effect on output. Note that no restriction is placed on the long-run effect of demand shocks on unemployment ($c_{22}$ is unrestricted). Just like with recursive identification, the ordering matters. Output growth is ordered first because the long-run restriction is imposed on it.

The implementation in the VAR Toolbox begins by loading the data and estimating the VAR:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Load data} \\
\hspace{1mm}load('BlanchardQuah1989.mat'); \\
\hspace{1mm}X = [output\_growth, unemployment]; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% VAR specification} \\
\hspace{1mm}nlags = 8; \textcolor{matlabgreen}{\% 8 lags (as in original paper)} \\
\hspace{1mm}det = 1; \textcolor{matlabgreen}{\% Include constant} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Estimate reduced-form VAR} \\
\hspace{1mm}[VAR, VARopt] = VARmodel(X, nlags, det); \\
}
\end{minipage}

\medskip

Set identification to long-run restrictions and compute impulse responses:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Set identification to long-run restrictions} \\
\hspace{1mm}VARopt.ident = \textcolor{matlabpurple}{'long'}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Compute impulse responses} \\
\hspace{1mm}[IR, VAR] = VARir(VAR, VARopt); \\
}
\end{minipage}

\medskip

The identified $B$ matrix is stored in \colorbox{script!80}{\small\texttt{VAR.B}}. As always, you can compute confidence bands, variance decompositions, and historical decompositions using the same functions as before.

\subsubsection*{Results and interpretation}

\textbf{Aggregate supply shock (first shock):}
\begin{itemize}
    \item A positive supply shock (e.g., technological improvement) increases output growth and reduces unemployment
    \item The effect on output is permanent (by construction), while the effect on unemployment is transitory
    \item \textbf{Puzzle:} The supply shock initially \textit{increases} unemployment before reducing it. This is known as the ``productivity puzzle'' or ``technology shocks puzzle'' -- a positive productivity shock seems to raise unemployment in the short run, which is at odds with some theoretical models. Possible explanations include adjustment costs, sticky wages, or labor hoarding.
\end{itemize}

\textbf{Aggregate demand shock (second shock):}
\begin{itemize}
    \item A positive demand shock raises output growth and reduces unemployment
    \item The impulse response of output growth exhibits a hump-shaped pattern, peaking around 4-8 quarters before returning to zero
    \item The cumulative effect on the output \textit{level} is zero (by construction) -- demand shocks move the economy around its long-run trend but don't shift the trend itself
    \item The effect on unemployment is negative and persistent, suggesting that demand shocks can have long-lasting effects on labor market outcomes even if they don't affect long-run output
\end{itemize}

\textbf{Verifying the restriction:}

A useful check is to plot the cumulative impulse response of output growth (which corresponds to the response of the output \textit{level}):
\begin{equation*}
\text{Cumulative IR}_h = \sum_{i=0}^{h} IR_i
\end{equation*}

For the demand shock, this cumulative response should converge to zero, while for the supply shock it should converge to a non-zero value. Blanchard and Quah's Figure 1 shows exactly this pattern, and you can replicate it using the VAR Toolbox by computing the cumulative sum of the impulse responses.

\textbf{Key takeaways:}
\begin{itemize}
    \item Long-run restrictions allow identification based on economic theory about permanent vs. transitory effects
    \item The method is particularly useful when theory provides clear predictions about long-run behavior
    \item Like recursive identification, the productivity puzzle highlights that not all aspects of the results may align perfectly with theoretical expectations
    \item The distinction between effects on growth rates vs. levels is crucial for interpreting long-run restrictions
\end{itemize}

%------------------------------------------
\subsection{Uhlig (2005): Sign Restrictions}\label{sec:app_uhlig}

\subsubsection*{Reference}
Uhlig, Harald. 2005. ``What are the Effects of Monetary Policy on Output? Results from an Agnostic Identification Procedure.'' \textit{Journal of Monetary Economics} 52(2): 381--419.

\subsubsection*{The economic question}

What are the effects of monetary policy on output? Uhlig revisits this classic question with a deliberately agnostic approach. Rather than relying on strong timing assumptions (as in recursive identification) or theoretical restrictions on long-run effects (as in Blanchard-Quah), Uhlig imposes only minimal sign restrictions that capture ``conventional wisdom'' about how monetary policy works. This allows the data to speak more freely about the magnitude and dynamics of monetary policy effects.

\subsubsection*{Data and specification}

\textbf{Sample period:} US monthly data from 1965:M1 to 2003:M12

\textbf{Variables:}
\begin{itemize}
    \item Real GDP (log)
    \item Real GDP deflator (log)
    \item Commodity price index (log)
    \item Total reserves (log)
    \item Non-borrowed reserves (log)
    \item Federal funds rate (percent)
\end{itemize}

\textbf{Specification:} VAR(12) with a constant term (12 lags is standard for monthly VARs)

Note the inclusion of reserves: Uhlig includes multiple measures of bank reserves to better capture the monetary policy stance, especially in the pre-2008 period when the Fed primarily used reserve management as a policy tool.

\subsubsection*{Identification strategy}

Uhlig's identification is based on sign restrictions that reflect ``conventional wisdom'' about monetary policy:

\begin{center}
\textit{A contractionary monetary policy shock should:}
\begin{enumerate}
    \item Raise the federal funds rate
    \item Lower the price level
    \item Decrease non-borrowed reserves
\end{enumerate}
\end{center}

Crucially, \textbf{no restriction is imposed on real GDP} -- this is the variable of interest, and Uhlig wants to let the data determine its response. The other variables (commodity prices, total reserves) are also left unrestricted.

These restrictions are imposed on the impulse responses for the first 6 months following the shock. The specific sign restrictions are:

\begin{table}[h]
\centering
\begin{tabular}{lc}
\hline
Variable & Monetary Policy Shock \\ \hline
Real GDP & \textit{unrestricted} \\ 
Real GDP deflator & $< 0$ \\ 
Commodity price index & \textit{unrestricted} \\ 
Total reserves & \textit{unrestricted} \\ 
Non-borrowed reserves & $< 0$ \\ 
Federal funds rate & $> 0$ \\ \hline
\end{tabular}
\end{table}

\subsubsection*{Implementation in Matlab}

Setting up sign restrictions requires defining a sign restriction matrix. First, estimate the reduced-form VAR:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Load data} \\
\hspace{1mm}load('Uhlig2005.mat'); \\
\hspace{1mm}X = [GDP, Deflator, CommodityPrices, TotalReserves, ... \\
\hspace{6mm}NonBorrowedReserves, FedFunds]; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% VAR specification} \\
\hspace{1mm}nlags = 12; \textcolor{matlabgreen}{\% 12 lags for monthly data} \\
\hspace{1mm}det = 1; \textcolor{matlabgreen}{\% Include constant} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Estimate reduced-form VAR} \\
\hspace{1mm}[VAR, VARopt] = VARmodel(X, nlags, det); \\
}
\end{minipage}

\medskip

Now define the sign restrictions matrix and other options:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Define shock name} \\
\hspace{1mm}VARopt.snames = \{\textcolor{matlabpurple}{'Monetary Policy Shock'}\}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Define sign restrictions matrix} \\
\hspace{1mm}\textcolor{matlabgreen}{\% Rows = variables, Columns = horizons (6 months)} \\
\hspace{1mm}\textcolor{matlabgreen}{\% 1 = positive, -1 = negative, 0 = unrestricted} \\
\hspace{1mm}SIGN = [ 0, 0, 0, 0, 0, 0;  \textcolor{matlabgreen}{\% Real GDP (unrestricted)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0;  \textcolor{matlabgreen}{\% Deflator (negative)} \\
\hspace{6mm} 0, 0, 0, 0, 0, 0;  \textcolor{matlabgreen}{\% Commodity Price (unrestricted)} \\
\hspace{6mm} 0, 0, 0, 0, 0, 0;  \textcolor{matlabgreen}{\% Total Reserves (unrestricted)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0;  \textcolor{matlabgreen}{\% NonBorrowed Reserves (negative)} \\
\hspace{6mm} 1, 0, 0, 0, 0, 0]; \textcolor{matlabgreen}{\% Fed Funds (positive)} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Specify number of periods for sign restrictions} \\
\hspace{1mm}VARopt.sr\_hor = 6; \\
}
\end{minipage}

\medskip

Note the structure of the \colorbox{script!80}{\small\texttt{SIGN}} matrix: each row corresponds to a variable (in the order they appear in \colorbox{script!80}{\small\texttt{X}}), and each column corresponds to a horizon (0 through 5 months). A value of 1 means the impulse response must be positive, -1 means negative, and 0 means unrestricted. 

In this example, restrictions are only imposed at horizon 0 (the first column), but you could impose restrictions at longer horizons by filling in more columns.

Finally, run the sign restriction algorithm:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Run sign restrictions identification} \\
\hspace{1mm}\textcolor{matlabgreen}{\% This function computes IRs, VDs, and HDs} \\
\hspace{1mm}SRout = SR(VAR, SIGN, VARopt); \\
}
\end{minipage}

\medskip

The \colorbox{script!80}{\small\texttt{SR}} function implements the algorithm described in Section \ref{sec:sign}. It repeatedly draws orthonormal rotation matrices $Q$ and checks whether the implied impulse responses satisfy the sign restrictions. The output \colorbox{script!80}{\small\texttt{SRout}} contains:
\begin{itemize}
    \item \colorbox{script!80}{\small\texttt{SRout.IRmed}}: Median impulse responses across all accepted draws
    \item \colorbox{script!80}{\small\texttt{SRout.IRinf}}, \colorbox{script!80}{\small\texttt{SRout.IRsup}}: Lower and upper error bands
    \item \colorbox{script!80}{\small\texttt{SRout.VDmed}}, \colorbox{script!80}{\small\texttt{SRout.HDmed}}: Median variance decompositions and historical decompositions
\end{itemize}

You can plot the results using the standard plotting functions:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Plot impulse responses} \\
\hspace{1mm}VARirplot(SRout.IRmed, SRout.IRinf, SRout.IRsup, VARopt); \\
}
\end{minipage}

\subsubsection*{Results and interpretation}

The key finding is striking: \textbf{the effect of monetary policy on real GDP is ambiguous.} While the restricted variables (federal funds rate, price level, non-borrowed reserves) all respond as expected to the contractionary shock, the response of real GDP is highly uncertain -- the error bands include both positive and negative values.

\textbf{What does this mean?}

Uhlig interprets this finding as evidence that, under minimal identifying restrictions, the data do not provide a clear answer about the output effects of monetary policy. Different rotation matrices $Q$ that satisfy the sign restrictions can imply very different output responses. This suggests that either:
\begin{enumerate}
    \item The effects of monetary policy on output are genuinely small and difficult to detect in the data, or
    \item Identifying monetary policy shocks requires stronger assumptions than just the sign restrictions imposed here
\end{enumerate}

This ambiguity is sometimes interpreted as support for long-run monetary neutrality: while monetary policy clearly affects nominal variables and reserve positions (as the sign restrictions ensure), its real effects are less clear-cut.

\textbf{Practical note on sign restrictions:}

The VAR Toolbox stores all accepted rotation matrices, so you can examine individual draws to understand the range of possibilities. This is useful for robustness checks and for understanding what drives the width of the error bands.

\textbf{Key takeaways:}
\begin{itemize}
    \item Sign restrictions allow for agnostic identification based on minimal theoretical priors
    \item The method produces sets of plausible models rather than a single point estimate
    \item Uhlig's results highlight that ``letting the data speak'' can lead to ambiguous conclusions
    \item The approach is particularly useful when you want to avoid strong timing or long-run assumptions
    \item The width of the error bands reflects the range of models consistent with the sign restrictions
\end{itemize}

%------------------------------------------
\subsection{Gertler and Karadi (2015): External Instruments}\label{sec:app_gk}

\subsubsection*{Reference}
Gertler, Mark, and Peter Karadi. 2015. ``Monetary Policy Surprises, Credit Costs, and Economic Activity.'' \textit{American Economic Journal: Macroeconomics} 7(1): 44--76.

\subsubsection*{The economic question}

How does monetary policy affect credit markets and economic activity? Gertler and Karadi focus on the ``credit channel'' of monetary policy -- the idea that monetary policy works not just through interest rates but also through its effects on credit availability and credit spreads. To identify monetary policy shocks, they exploit high-frequency movements in asset prices around FOMC announcements.

\subsubsection*{Data and specification}

\textbf{Sample period:} US monthly data from 1979:M7 to 2012:M6

\textbf{Variables:}
\begin{itemize}
    \item Industrial production (log)
    \item Consumer price index (log)
    \item 1-year Treasury bill interest rate (percent)
    \item Excess bond premium (percent) -- a measure of credit spreads
\end{itemize}

\textbf{Specification:} VAR(12) with a constant term

The excess bond premium (EBP), constructed by Gilchrist and Zakrajšek (2012), measures the component of corporate credit spreads that cannot be explained by expected default risk. It captures financial market frictions and is a key indicator of credit conditions.

\subsubsection*{Identification strategy}

The key innovation is the use of an \textbf{external instrument} (or proxy variable) to identify monetary policy shocks. The instrument is constructed from high-frequency changes in interest rate futures prices around FOMC announcements.

\textbf{The instrument:} Monetary policy surprises measured as changes in federal funds futures prices in a 30-minute window around FOMC announcements:
\begin{equation*}
z_t = -(P_{t,\tau+20} - P_{t,\tau-10})
\end{equation*}

where $P_{t,\tau}$ is the price of a federal funds futures contract at time $\tau$ on day $t$. The 30-minute window (-10 minutes before to +20 minutes after the announcement) is chosen to be narrow enough that only monetary policy news should matter, but wide enough to capture the full price reaction.

\textbf{Key identifying assumptions:}
\begin{enumerate}
    \item \textbf{Relevance:} The instrument $z_t$ is correlated with the monetary policy shock: $\mathbb{E}[\varepsilon_t^{MonPol} z_t'] \neq 0$
    
    \item \textbf{Exogeneity:} The instrument is uncorrelated with all other structural shocks: $\mathbb{E}[\varepsilon_t^i z_t'] = 0$ for $i \neq MonPol$
\end{enumerate}

The intuition is that in the 30-minute window around an FOMC announcement, only monetary policy news should be affecting interest rate futures prices. Other macroeconomic shocks operate at lower frequencies and should not contaminate the high-frequency movements.

\subsubsection*{Implementation in Matlab}

First, load the data and the external instrument:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Load data and instrument} \\
\hspace{1mm}load('GertlerKaradi2015.mat'); \\
\hspace{1mm}X = [IndustrialProduction, CPI, Tbill1Y, EBP]; \\
\hspace{1mm}IV = FFR\_surprise; \textcolor{matlabgreen}{\% High-frequency monetary policy surprise} \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% VAR specification} \\
\hspace{1mm}nlags = 12; \\
\hspace{1mm}det = 1; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Estimate reduced-form VAR} \\
\hspace{1mm}[VAR, VARopt] = VARmodel(X, nlags, det); \\
}
\end{minipage}

\medskip

Add the instrument to the VAR structure:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Add external instrument to VAR structure} \\
\hspace{1mm}VAR.IV = IV; \\
}
\end{minipage}

\medskip

Set identification options and compute impulse responses:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Set identification to external instruments} \\
\hspace{1mm}VARopt.ident = \textcolor{matlabpurple}{'iv'}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% For confidence bands, use wild bootstrap} \\
\hspace{1mm}VARopt.method = \textcolor{matlabpurple}{'wild'}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Compute impulse responses} \\
\hspace{1mm}[IR, VAR] = VARir(VAR, VARopt); \\
}
\end{minipage}

\medskip

\textbf{Important details:}
\begin{itemize}
    \item The external instrument approach instruments the residual of the \textbf{first variable} in the VAR. Therefore, the policy rate (1-year T-bill) should be ordered first if you want to identify the monetary policy shock. (Actually, looking at the data specification above, this might not be quite right - but this is how the toolbox works currently).
    
    \item The identified impact matrix $B$ is stored in \colorbox{script!80}{\small\texttt{VAR.B}}.
    
    \item First-stage regression diagnostics are stored in \colorbox{script!80}{\small\texttt{VAR.FirstStage}}, including the F-statistic for instrument strength.
    
    \item The wild bootstrap (\colorbox{script!80}{\small\texttt{VARopt.method = 'wild'}}) is recommended for external instruments IV to account for potential heteroskedasticity.
\end{itemize}

\subsubsection*{Results and interpretation}

\textbf{Impact effects (the $B$ matrix):}
The external instrument approach first estimates the impact of the shock using two-stage least squares:
\begin{itemize}
    \item \textbf{First stage:} Regress the policy rate residual on the instrument
    \item \textbf{Second stage:} Use the fitted values to construct the impact matrix
\end{itemize}

The first-stage F-statistic should be examined to verify instrument strength (typically F > 10 is desired).

\textbf{Dynamic effects:}
Following a contractionary monetary policy shock (1 percentage point increase in the 1-year rate):
\begin{itemize}
    \item Industrial production declines gradually, reaching a trough around 2-3\% below baseline after 12-18 months
    \item The CPI falls slowly and persistently
    \item The excess bond premium increases sharply, indicating a tightening of credit conditions
    \item Effects are highly persistent, with the economy remaining below trend for several years
\end{itemize}

\textbf{The credit channel:}
A key finding is that the excess bond premium rises following a monetary tightening. This suggests that monetary policy affects the economy not just through the direct effect of higher interest rates, but also through its impact on credit market conditions. The increase in credit spreads amplifies and propagates the monetary policy shock.

\textbf{Advantages of external instruments:}
\begin{itemize}
    \item No need for timing assumptions (as in recursive identification)
    \item No need for long-run restrictions (as in Blanchard-Quah)
    \item Exploits plausibly exogenous variation from high-frequency data
    \item Can be combined with other identification strategies (see next section)
\end{itemize}

\textbf{Key takeaways:}
\begin{itemize}
    \item External instruments provide a powerful way to identify shocks using variation outside the VAR
    \item High-frequency identification exploits the idea that certain short windows isolate specific types of news
    \item First-stage diagnostics are crucial for assessing instrument validity
    \item The approach reveals important information about transmission mechanisms (here, the credit channel)
\end{itemize}

%------------------------------------------
\subsection{Cesa-Bianchi and Sokol (2020): Combining Methods}\label{sec:app_cs}

\subsubsection*{Reference}
Cesa-Bianchi, Ambrogio, and Andrej Sokol. 2020. ``Financial Shocks, Credit Spreads, and the International Credit Channel.'' Bank of England Working Paper No. 693.

\subsubsection*{The economic question}

What are the effects of financial shocks on the economy, and how do they compare to monetary policy shocks? This question is challenging because financial shocks can look very similar to demand shocks in terms of their sign patterns (both reduce output, reduce inflation, and reduce interest rates). The innovation of this paper is to combine two identification strategies -- external instruments and sign restrictions -- to separately identify monetary policy shocks and financial shocks.

\subsubsection*{Data and specification}

\textbf{Sample period:} US monthly data from 1979:M7 to 2019:M12

\textbf{Variables:}
\begin{itemize}
    \item Industrial production (log)
    \item Consumer price index (log)
    \item 1-year Treasury bill interest rate (percent)
    \item Excess bond premium (percent)
    \item Corporate bond yield (percent)
\end{itemize}

\textbf{Specification:} VAR(12) with a constant term

This specification augments Gertler and Karadi (2015) by adding corporate bond yields, which helps distinguish financial shocks from demand shocks.

\subsubsection*{Identification strategy}

The identification strategy proceeds in two steps:

\textbf{Step 1: Identify monetary policy shocks using external instruments}

Just as in Gertler and Karadi (2015), monetary policy shocks are identified using high-frequency surprises around FOMC announcements. This step yields the first column of the impact matrix $B$.

\textbf{Step 2: Identify other shocks using sign restrictions}

Once the monetary policy shock is identified, sign restrictions are used to identify supply shocks, demand shocks, and financial shocks. The key insight is that financial shocks can be distinguished from demand shocks by their effect on \textbf{borrowing rates}.

Consider the decomposition of borrowing rates:
\begin{equation*}
r^B_t = r_t + cs_t
\end{equation*}

where $r_t$ is the policy rate and $cs_t$ is the credit spread.

\begin{itemize}
    \item For a \textbf{demand shock}: Both $r_t$ and $cs_t$ fall, but typically $r_t$ falls by more than $cs_t$ rises, so $r^B_t$ falls.
    
    \item For a \textbf{financial shock}: $r_t$ falls (as the central bank eases in response) but $cs_t$ rises sharply (due to financial distress). If the increase in credit spreads dominates, then $r^B_t$ rises.
\end{itemize}

This differential effect on borrowing rates is the key to separating financial shocks from demand shocks.

The complete set of sign restrictions is:

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\hline
Variable & Mon. Policy & Supply & Demand & Financial \\ \hline
Industrial production & $-$ & $-$ & $-$ & $-$ \\
Consumer Price Index & $-$ & $+$ & $-$ & $-$ \\
1-year T-bill rate & $+$ & -- & $-$ & $-$ \\
Excess Bond Premium & $+$ & $+$ & $+$ & $+$ \\
Corporate bond yield & $+$ & -- & $-$ & $+$ \\ \hline
\end{tabular}
\caption{Sign restrictions imposed for 6 months following the shock. Mon. Policy shock is identified via external instrument (not sign restrictions).}
\end{table}

The restrictions are imposed for 6 periods (months).

\subsubsection*{Implementation in Matlab}

This is the most complex identification strategy we've covered, as it combines external instruments and sign restrictions. Unfortunately, as of the current version of the VAR Toolbox, the bootstrapping procedure for this combined method is still being developed.

First, set up the VAR and add the external instrument:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Load data} \\
\hspace{1mm}load('CesaBianchiSokol2020.mat'); \\
\hspace{1mm}X = [IndustrialProduction, CPI, Tbill1Y, EBP, CorpBondYield]; \\
\hspace{1mm}IV = FFR\_surprise; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Estimate VAR and add instrument} \\
\hspace{1mm}nlags = 12; \\
\hspace{1mm}det = 1; \\
\hspace{1mm}[VAR, VARopt] = VARmodel(X, nlags, det); \\
\hspace{1mm}VAR.IV = IV; \\
}
\end{minipage}

\medskip

Define the sign restrictions for the remaining shocks:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Define shock names (excluding monetary policy, which is IV-identified)} \\
\hspace{1mm}VARopt.snames = \{\textcolor{matlabpurple}{'Supply'}, \textcolor{matlabpurple}{'Demand'}, \textcolor{matlabpurple}{'Financial'}\}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Sign restrictions matrix (3 remaining shocks, 6 horizons)} \\
\hspace{1mm}\textcolor{matlabgreen}{\% Each block of 5 rows = restrictions for one shock} \\
\hspace{1mm}SIGN = [-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% IP (Supply)} \\
\hspace{6mm} 1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% CPI (Supply)} \\
\hspace{6mm} 0, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% Tbill (Supply)} \\
\hspace{6mm} 1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% EBP (Supply)} \\
\hspace{6mm} 0, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% CorpYield (Supply)} \\[2pt]
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% IP (Demand)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% CPI (Demand)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% Tbill (Demand)} \\
\hspace{6mm} 1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% EBP (Demand)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% CorpYield (Demand)} \\[2pt]
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% IP (Financial)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% CPI (Financial)} \\
\hspace{6mm}-1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% Tbill (Financial)} \\
\hspace{6mm} 1, 0, 0, 0, 0, 0; \textcolor{matlabgreen}{\% EBP (Financial)} \\
\hspace{6mm} 1, 0, 0, 0, 0, 0]; \textcolor{matlabgreen}{\% CorpYield (Financial)} \\[4pt]
\hspace{1mm}VARopt.sr\_hor = 6; \\
}
\end{minipage}

\medskip

Run the combined IV-sign restrictions identification:

\begin{minipage}[b]{.9\textwidth}
\todo[color=script!80,inline,caption={}]{\small\ttfamily
\hspace{1mm}\textcolor{matlabgreen}{\% Set identification to IV + sign restrictions} \\
\hspace{1mm}VARopt.ident = \textcolor{matlabpurple}{'iv\_sign'}; \\[4pt]
\hspace{1mm}\textcolor{matlabgreen}{\% Run combined identification} \\
\hspace{1mm}SRout = SR(VAR, SIGN, VARopt); \\
}
\end{minipage}

\medskip

\textbf{Important notes:}
\begin{itemize}
    \item The monetary policy shock is identified first using the external instrument
    \item The sign restrictions are then applied to identify the remaining shocks, conditional on the monetary policy shock being fixed
    \item As noted in the slides, bootstrapping for this combined method is still under development in the toolbox
\end{itemize}

\subsubsection*{Results and interpretation}

\textbf{Monetary policy shock:}
The impulse responses to monetary policy shocks are similar to Gertler and Karadi (2015), confirming the robustness of the external instruments approach.

\textbf{Financial shock (the main result):}
A negative financial shock (increase in credit spreads):
\begin{itemize}
    \item Reduces industrial production persistently -- the effect is large and long-lasting
    \item Reduces inflation
    \item Leads to policy easing (lower policy rate) as the central bank responds to deteriorating conditions
    \item Increases credit spreads (EBP) sharply
    \item \textbf{Key finding:} Corporate bond yields \textit{rise}, even though the policy rate falls. This is because the increase in credit spreads more than offsets the decline in the risk-free rate.
\end{itemize}

\textbf{Comparison with demand shocks:}
Both financial shocks and demand shocks reduce output and inflation, but:
\begin{itemize}
    \item Financial shocks raise borrowing costs (corporate bond yields), while demand shocks lower them
    \item Financial shocks have more persistent effects on output
    \item Financial shocks are associated with larger increases in credit spreads
\end{itemize}

This distinction is economically important: it suggests that the 2008 financial crisis was driven primarily by financial shocks rather than demand shocks, and required different policy responses (credit market interventions in addition to conventional monetary easing).

\textbf{Key takeaways:}
\begin{itemize}
    \item Multiple identification strategies can be combined to identify more shocks than either method alone
    \item External instruments provide a sharp identification of one shock, which can then anchor sign restrictions for other shocks
    \item The combination is particularly powerful when shocks are difficult to distinguish based on timing or sign restrictions alone
    \item Careful attention to economic mechanisms (here, the decomposition of borrowing rates) is crucial for designing effective sign restrictions
\end{itemize}

%------------------------------------------
\subsection*{Summary}

This section has walked through five influential applications of VAR analysis, each illustrating a different identification approach:

\begin{enumerate}
    \item \textbf{Stock and Watson (2001)} showed how recursive identification can be used to study monetary policy effects, though the price puzzle highlights the method's limitations.
    
    \item \textbf{Blanchard and Quah (1989)} demonstrated the power of long-run restrictions grounded in economic theory to separate supply and demand shocks.
    
    \item \textbf{Uhlig (2005)} illustrated the agnostic approach of sign restrictions, finding that minimal identifying assumptions lead to ambiguous conclusions about monetary policy's output effects.
    
    \item \textbf{Gertler and Karadi (2015)} pioneered the use of high-frequency external instruments to identify monetary policy shocks and study credit market transmission.
    
    \item \textbf{Cesa-Bianchi and Sokol (2020)} combined external instruments and sign restrictions to separately identify financial shocks from demand shocks.
\end{enumerate}

Together, these applications show that:
\begin{itemize}
    \item There is no single ``best'' identification strategy -- the appropriate method depends on the question, the data, and the strength of available assumptions
    \item Different identification schemes can yield different conclusions, highlighting the importance of robustness checks
    \item Modern VAR analysis increasingly combines multiple identification strategies to sharpen inference
    \item Careful economic reasoning is essential both for designing identification schemes and for interpreting results
\end{itemize}

Armed with these examples and the tools from the VAR Toolbox, you are now equipped to conduct your own VAR analysis. The key is to think carefully about your identifying assumptions, test their plausibility, and be transparent about their limitations.








\section{Identification in the VAR Toolbox: Technical Details }\label{app:ident}


\subsection{Identification by zero long-run restrictions}

\subsection{Identification by sign restrictions}

The identification by sign restrictions solves the identification problem by drawing a large number ($J$) of candidate structural impact matrices ($B_j$) that satisfy the condition:
\begin{equation}\label{eqapp:sign0}
	\Sigma_u = B_jB_j' \ \ \ \ \ \ \ \ j=0,1,...,J
\end{equation}
and retaining those whose elements satisfy a set of \textit{a priori} signs restrictions on a subset or all elements of $B_j$. Such prior beliefs on the sign of the impact of shocks on the endogenous variables are typically informed by theoretical models. To achieve identification, the sign restrictions need to uniquely identify the shocks of interest. 

To generate a candidate structural impact matrix, the VAR Toolbox follows the approach proposed by \cite{RubioWaggonerZha2010}:
\begin{equation}\label{eqapp:sign1}
	B_j = PQ_j
\end{equation}
where $Q_j$ denotes the orthogonal factor in the QR factorization of a random matrix with elements from the standard normal distribution ($j$ denotes a draw) and $P$ is the Cholesky factor of the reduced-form covariance matrix $\Sigma_u$. The matrix $B_j$ is a candidate structural impact matrix because the structural shocks implied by $\varepsilon_{jt}  = \left(B_j\right)u_t$ are such that
\begin{equation}\label{eqapp:sign3}
	\varepsilon_{jt}  \varepsilon_{jt}'  = I_2
\end{equation}
so that condition \eqref{eqapp:sign0} is satisfied.

Differently from the identification schemes described above -- where there is a unique $B$ matrix that solves the identification problem -- sign restrictions lead to set identification. In other words, the data are potentially consistent with a wide range of $B$ matrices that are all admissible in that they satisfy the sign restrictions.

\subsection{Identification with External Instruments}

The identification with external instruments solves the identification problem by retrieving one (or more) columns of the structural impact matrix ($B$) exploiting the information provided by an instrument that is external to the VAR.

In the VAR Toolbox, the identification with external instruments is implemented by a two-stage regression. That is, to isolate the variation in the VAR\ reduced-form residuals that are due to the structural shock of interest, the VAR Toolbox estimates equations \eqref{eq:first_stage} and \eqref{eq:second_stage} sequentially. As discussed in Section \ref{sec:iv}, this two-stage approach normalizes the coefficients of the structural impact matrix $B$ by $b_{11}$. To recover the true values of the elements of $B$, the VAR Toolbox follows \cite{GertlerKaradi2015} (footnote 4). 

Specifically, the condition $\Sigma_u = B B'$ implies
\begin{equation}
	b_{11}^2 = \sigma^2_y - b_{12}^2
\end{equation}
as well as 
\begin{equation}
	b_{11}^2 	= \sigma^2_y - b_{12}^2
\end{equation}



\subsection{Combining sign restrictions and external instruments}

The external instruments and sign restriction identification approaches can
be combined as proposed by CesBianchiSokol2020. To have a meaningful example
we need a trivariate VAR%
\begin{equation}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t} \\
		x_{3t}%
	\end{array}%
	\right] =%
	\begin{bmatrix}
		\phi _{11} & \phi _{12} & \phi _{13} \\
		\phi _{21} & \phi _{22} & \phi _{23} \\
		\phi _{31} & \phi _{32} & \phi _{33}%
	\end{bmatrix}%
	\left[
	\begin{array}{c}
		y_{t-1} \\
		r_{t-1} \\
		r_{t-1}%
	\end{array}%
	\right] +\left[
	\begin{array}{ccc}
		b_{11} & b_{12} & b_{13} \\
		b_{21} & b_{22} & b_{23} \\
		b_{31} & b_{12} & b_{33}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Demand} \\
		\varepsilon_{t}^{MonPol} \\
		\varepsilon_{3t}%
	\end{bmatrix}%
	.  \label{eqapp:struct_var_sriv}
\end{equation}%
For simplicity, and without loss of generality, we consider the case where
we identify only the first structural shock (labelled $\varepsilon_{t}^{IV}$) with the external instruments approach. Without loss of generality we choose this shock to be the shock associated with the first equation, so we set $\varepsilon_{t}^{b}\equiv \varepsilon_{t}^{Demand}$. We can then identify the remaining $n-1$ shocks, namely $\varepsilon_{t}^{B^{SR}}\equiv (\varepsilon_{t}^{MonPol},\varepsilon_{3t}')'$%
, with sign restrictions.

We start by partitioning the matrix $B\ $into a column vector $B^{IV}$, which
captures the impact of the shock in the first equation ($e_{t}^{b}$), and a
matrix $B^{SR}$, whose columns capture the impact of the shocks in the
remaining $2$ equations ($e_{t}^{B^{SR}}$):%
\begin{equation}
	B=\left[
	\begin{array}{cc}
		B^{IV} & B^{SR}%
	\end{array}%
	\right] ,  \label{eqapp:B_partitioned}
\end{equation}%
where $b$ is an $3\times 1$ vector and $B^{SR}$ is a $3\times 2$
matrix. Assuming that a valid instrument exists, the first column of the $B$
matrix ($b$) can be easily identified as explained in the previous section.

We now show how to combine the external instruments identification approach
with a standard sign restriction approach to identify the remaining
structural shocks ($e_{t}^{B^{SR}}$) `conditional' on the shock
identified with the external instrument ($e_{t}^{b}$). To identify $\mathcal{%
	B}$\ (i.e. the contemporaneous impact of the remaining shocks)\ we proceed
as follows. First,

using (\ref{eq:B_partitioned}), we re-write the covariance matrix of the
reduced-form residuals as:%
\begin{equation}
	\Sigma_{u}=BB'=\left[
	\begin{array}{cc}
		b & B^{SR}%
	\end{array}%
	\right] \left[
	\begin{array}{cc}
		b & B^{SR}%
	\end{array}%
	\right]'.  \label{eq:cov_partitioned}
\end{equation}%
As we have seen above, this decomposition of the covariance matrix is not
unique. Let $P$ be the Cholesky decomposition of the covariance matrix $%
\Sigma_{u}$, and let $Q$ be an orthonormal matrix such that $QQ'=I$%
. Then we can write:%
\begin{equation}
	\Sigma_{u}=PP'=PQQ'C'=\left( PQ\right) \left(
	PQ\right)'
\end{equation}%
Our strategy consists precisely in constructing a large number of
orthonormal matrices $Q$ that satisfy the following condition:%
\begin{equation*}
	CQ=\left[
	\begin{array}{cc}
		b & B^{SR}%
	\end{array}%
	\right] ,
\end{equation*}%
where $b$ is identified via the external instrument and $B^{SR}$
satisfies a set of sign restrictions. For example, assume we have an
instrument for a monetary policy shock and we want to identify the effects
of deamnd and supply shocks conditional on the monetary policy shock, namely:%
\begin{equation*}
	\begin{tabular}{lccc}
		\hline
		\addlinespace & Monetary Policy & $\text{Demand}$ ($\varepsilon_{t}^{Demand}
		$) & $\text{Supply}$ ($\varepsilon_{t}^{MonPol}$) \\ \hline
		\addlinespace Policy Rate ($y_{t}$) & Proxy & + &  \\
		Price ($r_{t}$) & Proxy & + & - \\ \hline
		Quantity ($x_{3t}$) & Proxy & + & + \\ \hline
	\end{tabular}%
\end{equation*}

We do that in three steps.

\begin{enumerate}
	\item Find a normal vector $q$ of dimension $n\times 1$ that rotates the
	first column of $C$, the Cholesky decomposition of $\Sigma_{u},$ into the
	vector $b$. That is, we find a $n\times 1$ normal vector $q$ such that the
	following equality holds:%
	\begin{equation}
		Cq=b
	\end{equation}
	
	\item Given $q$, build the remaining $n-1$ columns of an orthonormal matrix $%
	Q$ following a standard Gram-Schmidt process. That is, find an $(n\times n-1)$ matrix $\mathcal{Q}$ such that the
	following equality holds:%
	\begin{equation}
		\begin{bmatrix}
			q & \mathcal{Q}%
		\end{bmatrix}%
		\begin{bmatrix}
			q & \mathcal{Q}%
		\end{bmatrix}%
		'=QQ'=I.
	\end{equation}%
	The matrix $CQ$ then represents a candidate identification scheme because:%
	\begin{equation}
		CQ=C%
		\begin{bmatrix}
			q & \mathcal{Q}%
		\end{bmatrix}%
		=%
		\begin{bmatrix}
			b & B^{SR}%
		\end{bmatrix}%
		=B.
	\end{equation}
	
	\item Check that $B^{SR}$ satisfies our set of sign restrictions. If it
	does, we retain the matrix $Q$. If does not, we repeat steps (1) and (2)
	until we obtain a matrix $B^{SR}$ that satisfies the restrictions.
\end{enumerate}

\noindent Finally we repeat steps (1)-(2)-(3) until we have $M$ matrices $%
B_{i}$ (with $i=1,2,...,M$) consistent with our identification restrictions.
This completes the (set) identification of structural matrix $B$.







\section{Structural Dynamic Analysis}\label{app:dynamic}

\subsection{Impulse response functions}

Impulse response functions ($IR$) allow us to answer the following
question:\ `What is the response over time of each of the variables in a
VAR\ to an increase in the current value of one of the structural
innovations, assuming that (i) the structural innovation returns to zero in
subsequent periods and (ii) all other structrual innovations are equal to
zero?

Of course, the implied thought experiment of shocking the innovations of one
equation while holding the others constant makes sense only when the
innovations are uncorrelated across equations -- which can be done only once
we know the structrual representation of the VAR, i.e. once we have
identified the $B$\ matrix.

To show how to compute impulse response functions, consider our simple
bivariate VAR in its structural representation%
\begin{equation}
	\left[
	\begin{array}{c}
		y_{t} \\
		r_{t}%
	\end{array}%
	\right] =%
	\begin{bmatrix}
		\phi _{11} & \phi _{12} \\
		\phi _{21} & \phi _{22}%
	\end{bmatrix}%
	\left[
	\begin{array}{c}
		y_{t-1} \\
		r_{t-1}%
	\end{array}%
	\right] +\left[
	\begin{array}{cc}
		b_{11} & b_{12} \\
		b_{21} & b_{22}%
	\end{array}%
	\right]
	\begin{bmatrix}
		\varepsilon_{t}^{Demand} \\
		\varepsilon_{t}^{MonPol}%
	\end{bmatrix}%
	,
\end{equation}%
Then, define a $2\times 1$ vector of impulse selection ($s$) that take value
of one for the strcuctural shock that we wan to consider. For example, to
compute the IR\ to the first structural shocks we define $s$ as:%
\begin{equation*}
	s=\left[
	\begin{array}{c}
		1 \\
		0%
	\end{array}%
	\right] .
\end{equation*}%
The impulse responses to the structural shock $\varepsilon_{t}^{Demand}$
can be easily computed with the following equation%
\begin{equation*}
	x_{t}=\Phi x_{t-1}+Bs_{t},
\end{equation*}%
which can be computed recursively as follows%
\begin{equation*}
	\left\{
	\begin{array}{ll}
		IR_{h}=Bs, & \ \ \ \ \text{for }h=1, \\
		IR_{h}=\Phi \cdot IR_{h-1} & \ \ \ \ \text{for }h=2,...,H.%
	\end{array}%
	\right.
\end{equation*}

\subsection{Forecast error variance decompositions}

Forecast error variance decompositions\ ($VD$) answer the following
question:\ What portion of the variance of the forecast error that is made
when predicting $x_{i,t+h}$ is due to each structural shock in $\varepsilon
_{t}$ (on average, over the sample period)? As such, $VD$\ provide
information about the relative importance of each structural shock in
affecting the variables in the VAR

To show how to compute forecast error variance decompositions, consider the
1-step ahead forecast error in our simple bivariate VAR:%
\begin{equation*}
	x_{t+1}-\mathbb{E}\left[ x_{t+1}\right] =x_{t+1}-\mathbb{E}\left[ \Phi
	x_{t}+u_{t+1}\right] =x_{t+1}-\Phi x_{t}=u_{t+1}
\end{equation*}%
where note that the 1-step ahead forecast error is the 1-step ahead reduced
form residual. As we know from the previous setion, the reduced-form
residual is related to the structural shocks throught the following equations%
\begin{equation*}
	{\small \left\{
		\begin{array}{c}
			u_{1t+1}=b_{11}\varepsilon_{1t+1}+b_{12}\varepsilon_{2t+1}, \\
			u_{2t+1}=b_{21}\varepsilon_{1t+1}+b_{22}\varepsilon_{2t+1}.%
		\end{array}%
		\right. }
\end{equation*}%
So, what is the variance of the forecast error?%
\begin{equation*}
	{\small
		\begin{array}{c}
			\mathbb{V}\left( u_{yt}\right) =b_{11}^{2}\mathbb{V}\left( \varepsilon
			_{1,t+1}\right) +b_{12}^{2}\mathbb{V}\left( \varepsilon_{2,t+1}\right)
			=b_{11}^{2}+b_{12}^{2} \\
			\mathbb{V}\left( u_{rt}\right) =b_{21}^{2}\mathbb{V}\left( \varepsilon
			_{1,t+1}\right) +b_{22}^{2}\mathbb{V}\left( \varepsilon_{2,t+1}\right)
			=b_{21}^{2}+b_{22}^{2}%
		\end{array}%
	}
\end{equation*}%
whihc follows from the fact that the variacne of $\varepsilon_{t}$ is 1 and
the structural shocks are orthogonal to each other. The final step to
compute VD\ is to ask:\ what portion of the variance of the forecast error
is due to each structural shock? The answer is given by teh following
equation%
\begin{equation*}
	\underset{\text{This sums up to 1}}{\underbrace{\left\{
			\begin{array}{c}
				VD_{y}^{\varepsilon_{1}}=\frac{b_{11}^{2}}{b_{11}^{2}+b_{12}^{2}}\medskip
				\\
				VD_{y}^{\varepsilon_{2}}=\frac{b_{12}^{2}}{b_{11}^{2}+b_{12}^{2}}\medskip
			\end{array}%
			\right. }}\ \ \ \ \ \ \underset{\text{This sums up to 1}}{\underbrace{%
			\left\{
			\begin{array}{c}
				VD_{r}^{\varepsilon_{1}}=\frac{b_{21}^{2}}{b_{21}^{2}+b_{22}^{2}}\medskip
				\\
				VD_{r}^{\varepsilon_{2}}=\frac{b_{22}^{2}}{b_{21}^{2}+b_{22}^{2}}\medskip
			\end{array}%
			\right. }}
\end{equation*}

\subsection{Historical decompositions}

Historical decompositions ($HD$) answer the following question:\ What
portion of the deviation of $x_{i,t}$ from its unconditional mean is due to
each structural shock $\varepsilon_{t}$?

We showed before that, in the absence of shocks, the variables of a
(stable)\ VAR\ will converge to their unconditional mean\ (i.e. their
equilibrium values or steady state). As we ahev seen with impulse responses,
when a structural shock hits, the endogenous variables move away from their
equilibrium and then only slowly go back to it. If another shock hits in the
next period, the variables will now be away from equilibrium because of (i)
the effect of the new shock and (ii) the persistent effect of the old shock.
Historical decompositions allow us to know, at each point in time, what
shock is responsible for keeping the endogenous variables away from their
steady state.

To show how to compute historical decompositions, we start from the Wold
decomposition of a\ VAR\ in Equation (\ref{eq:struct_wold_1}), according to
which each observation can be re-written as the cumulative sum of the
structural shocks. In particular, we consider the Wold representation of our
simple bivariate VAR for $t=2$:%
\begin{equation*}
	r=\underset{init_{2}}{\underbrace{\Phi ^{2}x_{0}}}+\underset{\boldsymbol{%
			\Theta }_{\boldsymbol{1}}}{\underbrace{\Phi B}}\varepsilon_{1}\underset{%
		\boldsymbol{\Theta }_{2}}{+\underbrace{B}}\varepsilon_{2},
\end{equation*}%
which allows us to write $r$ as a function of present and past structural
shocks ($\varepsilon_{1}$ and $\varepsilon_{2}$) and the initial condition
($x_{0}$). In matrix form:%
\begin{equation*}
	{\small
		\begin{bmatrix}
			x_{1,2} \\
			x_{2,2}%
		\end{bmatrix}%
		=%
		\begin{bmatrix}
			init_{y,2} \\
			init_{r,2}%
		\end{bmatrix}%
		+%
		\begin{bmatrix}
			\theta _{11}^{1} & \theta _{12}^{1} \\
			\theta _{21}^{1} & \theta _{22}^{1}%
		\end{bmatrix}%
		\begin{bmatrix}
			\varepsilon_{1,1} \\
			\varepsilon_{2,1}%
		\end{bmatrix}%
		+%
		\begin{bmatrix}
			\theta _{11}^{2} & \theta _{12}^{2} \\
			\theta _{21}^{2} & \theta _{22}^{2}%
		\end{bmatrix}%
		\begin{bmatrix}
			\varepsilon_{1,2} \\
			\varepsilon_{2,2}%
		\end{bmatrix}%
	}
\end{equation*}%
Therefore $r$ can be expressed as
\begin{equation*}
	{\small \left\{
		\begin{array}{l}
			x_{1,2}=init_{y,2}+\theta _{11}^{1}\varepsilon_{1,1}+\theta
			_{12}^{1}\varepsilon_{2,1}+\theta _{11}^{2}\varepsilon_{1,2}+\theta
			_{12}^{2}\varepsilon_{2,2} \\
			x_{2,2}=init_{r,2}+\theta _{21}^{1}\varepsilon_{1,1}+\theta
			_{22}^{1}\varepsilon_{2,1}+\theta _{21}^{2}\varepsilon_{1,2}+\theta
			_{22}^{2}\varepsilon_{2,2}%
		\end{array}%
		\right. }
\end{equation*}%
The historical decomposition is given by
\begin{equation*}
	\begin{array}{ccc}
		\underset{\text{This sums up to }x_{1,2}}{\underbrace{{\small \left\{
					\begin{array}{l}
						HD_{y,2}^{\varepsilon_{1}}=\theta _{11}^{1}\varepsilon_{1,1}+\theta
						_{11}^{2}\varepsilon_{1,2}\medskip  \\
						HD_{y,2}^{\varepsilon_{2}}=\theta _{12}^{1}\varepsilon_{2,1}+\theta
						_{12}^{2}\varepsilon_{2,2}\medskip  \\
						HD_{y,2}^{init}=init_{y,2}\medskip
					\end{array}%
					\right. }}} &  & \underset{\text{This sums up to }x_{2,2}}{\underbrace{%
				{\small \left\{
					\begin{array}{l}
						HD_{r,2}^{\varepsilon_{1}}=\theta _{21}^{1}\varepsilon_{1,2}+\theta
						_{21}^{0}\varepsilon_{1,2}\medskip  \\
						HD_{r,2}^{\varepsilon_{2}}=\theta _{22}^{1}\varepsilon_{2,2}+\theta
						_{22}^{0}\varepsilon_{2,2}\medskip  \\
						HD_{r,2}^{init}=init_{r,2}\medskip
					\end{array}%
					\right. }}}%
	\end{array}%
\end{equation*}%
where $HD_{y,2}^{\varepsilon_{1}}$ is the contribution of present and past
shocks to $\varepsilon_{1}$ to $y$ in period $t=2$. Note that $y$, for
example, is equal to the sum of the contribution of (i) present and past
shocks to $\varepsilon_{1}$, (ii) present and past shocks to $\varepsilon
_{2}$, and (iii) the initial condition. The first two elements are obvious:\
if shocks are persistent we would expect today's calue of $y$ to be affectd
by present and recent shocks. The third element depends on how far the first
observation in our data ($x_{0}$) is from its unconditinoal mean. In this
example, we are assuming that the unconditional mean of the data is zero. So
if $x_{0}$ is very different from $0$, the the initial condition will matter
for many periods until it will become asymptotically small.

\end{document}


\subsection{Stock and Watson}

This section shows how short-runzero restrictions work by replicating the JEP (2001) paper \textquotedblleft Vector Autoregressions,\textquotedblright\ by Stock and Watson. Zero short-run restrictions are among the most popular identification approaches in VAR. Identification is achieved by assuming that some structural shocks have no contemporaneous effect on some of the endogenous variables of the system (because for example it might take time for a shock to affect certain variables). Practically, this implies setting some elements of the $B$ matrix is equal to zero, thus 

Among other things, the paper attempts to infer the causal influence of monetary policy on unemployment and inflation in the US economy. The authors assume a VAR with $p=4$ lags and three endogenous variables: inflation ($\pi_t$), unemployment ($u_t$), and the fed funds rate ($r_t$)with quarterly data from 1960:Q1 to 2000:Q4. 

Before jumping to the actual replication of some of the results, the code below shows how to load the and prepare data (which is stored in \ \path{data/SW2001_Data.xlsx} \ ), following the same logic used in previous sections. 

\begin{matlabcode}
% 4.1 Load data from Stock and Watson
%---------------------------------------------------------------
[xlsdata, xlstext] = xlsread('data/SW2001_Data.xlsx','Sheet1');
dates = xlstext(3:end,1);
datesnum = Date2Num(dates);
vnames_long = xlstext(1,2:end);
vnames = xlstext(2,2:end);
nvar = length(vnames);
data   = Num2NaN(xlsdata);
for ii=1:length(vnames)
     DATA.(vnames{ii}) = data(:,ii);
end
nobs = size(data,1);
% 4.2 Plot series
%---------------------------------------------------------------
FigSize(26,6)
for ii=1:nvar
     subplot(1,3,ii)
     H(ii) = plot(DATA.(vnames{ii}),'LineWidth',3,'Color',cmap(1));
     title(vnames_long(ii));
     DatesPlot(datesnum(1),nobs,6,'q') % Set the x-axis label dates
     grid on;
end
SaveFigure('graphics/SW_DATA',1)
clf('reset')
\end{matlabcode}


\noindent As in the previous example, the data is stored in the structure DATA and then plotted. Figure XX shows the time series behavior of the time series used in this example:
\begin{figure}[h]
	\centering\includegraphics[width=.85\textwidth]{SW_DATA.pdf}
\end{figure}

\begin{matlabcode}
% 4.3 Set up and estimate VAR
%--------------------------------------------------------------
% Select endogenous variables
Xvnames      = {'infl','unemp','ff'};
Xvnames_long = {'Inflation','Unemployment','Fed Funds'};
Xnvar        = length(Xvnames);
% Create matrices of variables to be used in the VAR
X = nan(nobs,Xnvar);
for ii=1:Xnvar
     X(:,ii) = DATA.(Xvnames{ii});
end
% Estimate VAR
det = 1;
nlags = 4;
[VAR, VARopt] = VARmodel(X,nlags,det);
% Update the VARopt structure with additional details to be used in IR
% calculations and plots
VARopt.vnames = Xvnames_long;
VARopt.nsteps = 24;
VARopt.quality = 1;
VARopt.FigSize = [26,12];
VARopt.firstdate = datesnum(1);
VARopt.frequency = 'q';
VARopt.figname= 'graphics/SW_';
% 4.4 IMPULSE RESPONSES
%--------------------------------------------------------------
% To get zero contemporaneous restrictions set
VARopt.ident = 'short';
VARopt.snames = {'$\backslash$epsilon\texttt{\^{1}'},'$\backslash$epsilon\texttt{\^{2}'},'$\backslash$epsilon\texttt{\^{MonPol}'}};
% Compute IR
[IR, VAR] = VARir(VAR,VARopt);
% Compute IR error bands
[IRinf,IRsup,IRmed,IRbar] = VARirband(VAR,VARopt);
% Plot IR
VARirplot(IRbar,VARopt,IRinf,IRsup);
% 4.5 FORECAST ERROR VARIANCE DECOMPOSITION
%--------------------------------------------------------------------------
% Compute VD
[VD, VAR] = VARvd(VAR,VARopt);
% Compute VD error bands
[VDinf,VDsup,VDmed,VDbar] = VARvdband(VAR,VARopt);
% Plot VD
VARvdplot(VDbar,VARopt);
\end{matlabcode}


The key identifying assumptions for the identification of the monetary policy shock is that the policy instrument ($r_{t}$) reacts contemporaneously to movements in inflation and in unemployment, as the Fed responds to developments in the economy. However, it takes time for moneatry oplicy to affect the economy, so that monetary policy shocks ($\varepsilon^{MonPol}_{t}$) do not affect inflation and unemployment in the same quarter when the shock hits.

\begin{equation*}
	\begin{bmatrix}
		\pi_{t} \\ 
		u_{t} \\ 
		r_{t}%
	\end{bmatrix}%
	=\sum_{p=1}^4 \Phi_p x_{t-p} +\left[ 
	\begin{array}{ccc}
		b_{11} & 0 & 0 \\ 
		b_{21} & b_{22} & 0 \\ 
		b_{31} & b_{32} & b_{33}%
	\end{array}%
	\right] 
	\begin{bmatrix}
		\varepsilon^{1}_{t} \\ 
		\varepsilon^{2}_{t} \\ 
		\varepsilon^{MonPol}_{t}%
	\end{bmatrix}%
\end{equation*}


In Matlab, set lag length to $4$ and estimate a VAR with a constant
\medskip

\begin{minipage}[b]{.9\textwidth}
	\todo[color=script!80,inline,caption={short for LoTds}]{\small\ttfamily
		\hspace{1mm}\textcolor{matlabgreen}{\% Set up and estimate VAR }\\
		\hspace{1mm}det = 1; \\
		\hspace{1mm}nlags = 4; \\
		\hspace{1mm}[VAR, VARopt] = VARmodel(X,nlags,det); \\
}\end{minipage}

\medskip

\item Then set the option for recursive identification \colorbox{script!80}{\small\texttt{VARopt.ident ='short'}} and compute the $IR$ with the \colorbox{script!80}{\small\texttt{VARir}} function. \smallskip
\begin{itemize}
	\item Note that the \textbf{ordering of the variables matter!}\medskip
\end{itemize}

\begin{minipage}[b]{.9\textwidth}
	\todo[color=script!80,inline,caption={short for LoTds}]{\small\ttfamily
		\hspace{1mm}\textcolor{matlabgreen}{\% For zero contemporaneous restrictions set: }\\
		\hspace{1mm}VARopt.ident = \textcolor{matlabpurple}{'short'}; \\
		\hspace{1mm}\textcolor{matlabgreen}{\% Compute IR }\\
		\hspace{1mm}[IR, VAR] = VARir(VAR,VARopt); \\
}\end{minipage}

\medskip

\item Note that the second output of the \colorbox{script!80}{\small \texttt{VARir}} function is \colorbox{script!80}{\small \texttt{VAR}} again
\begin{itemize}
	\item This is because the \colorbox{script!80}{\small \texttt{VAR}} structure is updated with the $B$ matrix corresponding to the identification scheme chosen
\end{itemize}



\item The \colorbox{script!80}{\small\texttt{VARirband}} function allows to
compute confidence intervals\medskip

\begin{minipage}[b]{.9\textwidth}
	\todo[color=script!80,inline,caption={short for LoTds}]{\small\ttfamily
		\hspace{1mm}\textcolor{matlabgreen}{\% Compute IR }\\
		\hspace{1mm}[IR, VAR] = VARir(VAR,VARopt); \\
}\end{minipage}

\item You can control the options of the bootstrap procedure by modifying the \colorbox{script!80}{\small\texttt{VARopt}} structure (before running \colorbox{script!80}{\small\texttt{VARir}})\bigskip

\item For example\medskip

\begin{minipage}[b]{.9\textwidth}
	\todo[color=script!80,inline,caption={short for LoTds}]{\small\ttfamily
		\hspace{1mm}\textcolor{matlabgreen}{\% Some options for the bootstrap}\\
		\hspace{1mm}VARopt.ndraws = 1000; \textcolor{matlabgreen}{\% Number of draws} \\
		\hspace{1mm}VARopt.pctg = 95; \textcolor{matlabgreen}{\% Level for confidence intervals} \\
		\hspace{1mm}VARopt.method = 'bs';  \textcolor{matlabgreen}{\% 'bs' sampling with replacement; 'wild' wild bootstrap}
	}
\end{minipage}

\item Monetary policy shock raises inflation in the short-run(price puzzle)
and increases unemployment

\begin{figure}[h]
	\centering\includegraphics[width=.75\textwidth]{SW_IR_3.pdf}
\end{figure}

{\textbf{The other two shocks are identified by definition... but how can we
		interpret them?}}\bigskip\bigskip

\item How about $\varepsilon^{1}_{t}$ and $\varepsilon^2_{t}$? \smallskip

\begin{itemize}
	\item The shock $\varepsilon^{1}_{t}$ affects all variables
	contemporaneously\medskip
	
	\item The shock $\varepsilon^2_{t}$ affects $r_{t}$ contemporaneously but
	not $\pi _{t}$\bigskip
\end{itemize}

\item Can we interpret these shocks? Are the assumptions consistent with any
theoretical mechanism? \bigskip

\item Some shocks may be better identified than others

{\textbf{The other two shocks are identified by definition... but how can we
		interpret them?}}

\item Shock to $\varepsilon^1_{t}$ behaves as a negative aggregate supply
shock

\begin{figure}[h]
	\includegraphics[width=.75\textwidth]{SW_IR_1.pdf}
\end{figure}

{\textbf{The other two shocks are identified by definition... but how can we
		interpret them?}}

\item Shock to $\varepsilon^2_{t}$ behaves as a negative aggregate demand
shock

\begin{figure}[h]
	\includegraphics[width=.75\textwidth]{SW_IR_2.pdf}
\end{figure}

{\textbf{Forecast error variance \& Historical decompositions}}\vspace{-.2cm}

\item The variance decomposition ($VD$) can be computed with the \colorbox{script!80}{\small \texttt{VARvd}} function\smallskip
\begin{itemize}
	\item The matrix \colorbox{script!80}{\small\texttt{VD}} is a $H$ horizon, $k$ shocks, $k$ variables matrix\medskip
\end{itemize}

\begin{minipage}[b]{.9\textwidth}
	\todo[color=script!80,inline,caption={short for LoTds}]{\small\ttfamily
		\hspace{1mm}\textcolor{matlabgreen}{\% Compute VD }\\
		\hspace{1mm}[VD, VAR] = VARvd(VAR,VARopt); \\
}\end{minipage}

\item Similarly, the historical decomposition ($HD$) can be computed with the \colorbox{script!80}{\small\texttt{VARhd}} function\medskip

\begin{minipage}[b]{.9\textwidth}
	\todo[color=script!80,inline,caption={short for LoTds}]{\small\ttfamily
		\hspace{1mm}\textcolor{matlabgreen}{\% Compute HD}\\
		\hspace{1mm}[HD, VAR] = VARhd(VAR,VARopt); \\
}\end{minipage}
\medskip

\item Differently from \colorbox{script!80}{\small\texttt{VD}}, the output of \colorbox{script!80}{\small\texttt{VARhd}} is a structure (\colorbox{script!80}{\small\texttt{HD}})\medskip

\begin{minipage}{\textwidth}
	\small
	\begin{verbatim}
		>> disp(HD)
		shock: [164x3x3 double]
		init: [164x3 double]
		const: [164x3 double]
		trend: [164x3 double]
		trend2: [164x3 double]
		exo: [164x3x0 double]
		endo: [164x3 double]
	\end{verbatim}
\end{minipage}

{\textbf{Forecast error variance decomposition}}\medskip

\begin{figure}[h]
	\includegraphics[width=.75\textwidth]{SW_VD.pdf}
\end{figure}

{\textbf{Historical decomposition}}\medskip

\begin{figure}[h]
	\includegraphics[width=.75\textwidth]{SW_HD_1.pdf}
\end{figure}

{\textbf{Historical decomposition}}\medskip

\begin{figure}[h]
	\includegraphics[width=.75\textwidth]{SW_HD_2.pdf}
\end{figure}

{\textbf{Historical decomposition}}\medskip

\begin{figure}[h]
	\includegraphics[width=.75\textwidth]{SW_HD_3.pdf}
\end{figure}